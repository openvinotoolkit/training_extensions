{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Task\n",
    "\n",
    "## Installation and Setup\n",
    "For installation and setting up the repo, please refer to the [Installation Notebook](000_install.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sakcay/projects/training_extensions'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from utils.io import setup_repo\n",
    "\n",
    "# Setup repo and checkout to the branch with the tutorials\n",
    "setup_repo(\n",
    "    git_url=\"https://github.com/openvinotoolkit/training_extensions.git\",\n",
    "    branch='tutorials/cvpr24',\n",
    ")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will setup the repo, change the directory to the root directory of the repo, so we have access to all the files and folders in the repo.\n",
    "\n",
    "## Prepare the Data\n",
    "\n",
    "The first step is to prepare the dataset. If you haven't downloaded the dataset yet, you could download it via the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is already available in data/fruits_and_vegetables\n"
     ]
    }
   ],
   "source": [
    "from notebooks.utils.download import download_dataset\n",
    "\n",
    "download_dataset(\n",
    "    url=(\n",
    "        \"https://github.com/openvinotoolkit/training_extensions/releases/download\"\n",
    "        \"/fruits_and_vegetables_dataset/fruits_and_vegetables.zip\"\n",
    "    ),\n",
    "    extract_to=\"data/fruits_and_vegetables\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data/fruits_and_vegetables\"\n",
    "work_dir = \"./otx-workspace-seg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with OTX Recipes\n",
    "The first step in this task is to train a model using OTX recipes, which are available in the `recipes` folder. The recipes are in the form of `.yaml` files, which can be used to train a model using the `otx` library.\n",
    "\n",
    "These recipes are pre-defined by the OTX, which are validated and tested to work with many different use-cases.\n",
    "\n",
    "Let's see the available recipes for `SEMANTIC_SEGMENTATION` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      <span style=\"font-style: italic\">                                             OTX Recipes                                              </span>       \n",
       "      ┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       \n",
       "      ┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Task                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Model Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Recipe Path                                               </span>┃       \n",
       "      ┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_s    │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_s.yaml                    │       \n",
       "      │ SEMANTIC_SEGMENTATION │ dino_v2        │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/dino_v2.yaml                        │       \n",
       "      │ SEMANTIC_SEGMENTATION │ openvino_model │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/openvino_model.yaml                 │       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_18   │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_18.yaml                   │       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_x    │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_x.yaml                    │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_b      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_b.yaml                      │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_s      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_s.yaml                      │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_t      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_t.yaml                      │       \n",
       "      └───────────────────────┴────────────────┴───────────────────────────────────────────────────────────┘       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "      \u001b[3m                                             OTX Recipes                                              \u001b[0m       \n",
       "      ┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       \n",
       "      ┃\u001b[1;35m \u001b[0m\u001b[1;35mTask                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mModel Name    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mRecipe Path                                              \u001b[0m\u001b[1;35m \u001b[0m┃       \n",
       "      ┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_s    │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_s.yaml                    │       \n",
       "      │ SEMANTIC_SEGMENTATION │ dino_v2        │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/dino_v2.yaml                        │       \n",
       "      │ SEMANTIC_SEGMENTATION │ openvino_model │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/openvino_model.yaml                 │       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_18   │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_18.yaml                   │       \n",
       "      │ SEMANTIC_SEGMENTATION │ litehrnet_x    │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/litehrnet_x.yaml                    │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_b      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_b.yaml                      │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_s      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_s.yaml                      │       \n",
       "      │ SEMANTIC_SEGMENTATION │ segnext_t      │ /home/sakcay/projects/training_extensions/src/otx/recipe/ │       \n",
       "      │                       │                │ semantic_segmentation/segnext_t.yaml                      │       \n",
       "      └───────────────────────┴────────────────┴───────────────────────────────────────────────────────────┘       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from otx.engine.utils.api import list_models\n",
    "\n",
    "available_models = list_models(task=\"SEMANTIC_SEGMENTATION\", print_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output of the above cell, there are 8 recipes available for the `SEMANTIC_SEGMENTATION` task. We can use any of these recipes to train a model. In this example, we will use the `litehrnet_18.yaml` recipe to quickly train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/projects/training_extensions/src/otx/core/data/module.py:62: UserWarning: There are empty annotation items in train set, Of these, only 0.0% are used.\n",
      "  dataset = pre_filtering(dataset, self.config.data_format, self.config.unannotated_items_ratio)\n",
      "<string>:6: UserWarning: Currently, no background label exists for `label_names`. Segmentation requires a background label. To do this, `Background` is added at index 0 of `label_names`.\n",
      "/home/sakcay/projects/training_extensions/src/otx/cli/cli.py:389: UserWarning: Automatically infer label_info from the given dataset. Then, giving it to the OTXModel.__init__() argument. If you don't want this behavior, please use `--disable-infer-num-classes` option.\n",
      "  warn(warning_msg, stacklevel=0)\n",
      "Downloading: \"https://storage.openvinotoolkit.org/repositories/openvino_training_extensions/models/custom_semantic_segmentation/litehrnet18_imagenet1k_rsc.pth\" to /home/sakcay/.cache/torch/hub/checkpoints/litehrnet18_imagenet1k_rsc.pth\n",
      "/home/sakcay/projects/training_extensions/src/otx/algo/utils/mmengine_utils.py:183: UserWarning: The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: increase_modules.0.conv1.weight, increase_modules.0.bn1.weight, increase_modules.0.bn1.bias, increase_modules.0.bn1.running_mean, increase_modules.0.bn1.running_var, increase_modules.0.bn1.num_batches_tracked, increase_modules.0.conv2.weight, increase_modules.0.bn2.weight, increase_modules.0.bn2.bias, increase_modules.0.bn2.running_mean, increase_modules.0.bn2.running_var, increase_modules.0.bn2.num_batches_tracked, increase_modules.0.conv3.weight, increase_modules.0.bn3.weight, increase_modules.0.bn3.bias, increase_modules.0.bn3.running_mean, increase_modules.0.bn3.running_var, increase_modules.0.bn3.num_batches_tracked, increase_modules.0.downsample.conv.weight, increase_modules.0.downsample.bn.weight, increase_modules.0.downsample.bn.bias, increase_modules.0.downsample.bn.running_mean, increase_modules.0.downsample.bn.running_var, increase_modules.0.downsample.bn.num_batches_tracked, increase_modules.1.conv1.weight, increase_modules.1.bn1.weight, increase_modules.1.bn1.bias, increase_modules.1.bn1.running_mean, increase_modules.1.bn1.running_var, increase_modules.1.bn1.num_batches_tracked, increase_modules.1.conv2.weight, increase_modules.1.bn2.weight, increase_modules.1.bn2.bias, increase_modules.1.bn2.running_mean, increase_modules.1.bn2.running_var, increase_modules.1.bn2.num_batches_tracked, increase_modules.1.conv3.weight, increase_modules.1.bn3.weight, increase_modules.1.bn3.bias, increase_modules.1.bn3.running_mean, increase_modules.1.bn3.running_var, increase_modules.1.bn3.num_batches_tracked, increase_modules.1.downsample.conv.weight, increase_modules.1.downsample.bn.weight, increase_modules.1.downsample.bn.bias, increase_modules.1.downsample.bn.running_mean, increase_modules.1.downsample.bn.running_var, increase_modules.1.downsample.bn.num_batches_tracked, increase_modules.2.conv1.weight, increase_modules.2.bn1.weight, increase_modules.2.bn1.bias, increase_modules.2.bn1.running_mean, increase_modules.2.bn1.running_var, increase_modules.2.bn1.num_batches_tracked, increase_modules.2.conv2.weight, increase_modules.2.bn2.weight, increase_modules.2.bn2.bias, increase_modules.2.bn2.running_mean, increase_modules.2.bn2.running_var, increase_modules.2.bn2.num_batches_tracked, increase_modules.2.conv3.weight, increase_modules.2.bn3.weight, increase_modules.2.bn3.bias, increase_modules.2.bn3.running_mean, increase_modules.2.bn3.running_var, increase_modules.2.bn3.num_batches_tracked, increase_modules.2.downsample.conv.weight, increase_modules.2.downsample.bn.weight, increase_modules.2.downsample.bn.bias, increase_modules.2.downsample.bn.running_mean, increase_modules.2.downsample.bn.running_var, increase_modules.2.downsample.bn.num_batches_tracked, increase_modules.3.conv1.weight, increase_modules.3.bn1.weight, increase_modules.3.bn1.bias, increase_modules.3.bn1.running_mean, increase_modules.3.bn1.running_var, increase_modules.3.bn1.num_batches_tracked, increase_modules.3.conv2.weight, increase_modules.3.bn2.weight, increase_modules.3.bn2.bias, increase_modules.3.bn2.running_mean, increase_modules.3.bn2.running_var, increase_modules.3.bn2.num_batches_tracked, increase_modules.3.conv3.weight, increase_modules.3.bn3.weight, increase_modules.3.bn3.bias, increase_modules.3.bn3.running_mean, increase_modules.3.bn3.running_var, increase_modules.3.bn3.num_batches_tracked, increase_modules.3.downsample.conv.weight, increase_modules.3.downsample.bn.weight, increase_modules.3.downsample.bn.bias, increase_modules.3.downsample.bn.running_mean, increase_modules.3.downsample.bn.running_var, increase_modules.3.downsample.bn.num_batches_tracked, downsample_modules.0.conv.weight, downsample_modules.0.bn.weight, downsample_modules.0.bn.bias, downsample_modules.0.bn.running_mean, downsample_modules.0.bn.running_var, downsample_modules.0.bn.num_batches_tracked, downsample_modules.1.conv.weight, downsample_modules.1.bn.weight, downsample_modules.1.bn.bias, downsample_modules.1.bn.running_mean, downsample_modules.1.bn.running_var, downsample_modules.1.bn.num_batches_tracked, downsample_modules.2.conv.weight, downsample_modules.2.bn.weight, downsample_modules.2.bn.bias, downsample_modules.2.bn.running_mean, downsample_modules.2.bn.running_var, downsample_modules.2.bn.num_batches_tracked, final_layer.conv.weight, final_layer.bn.weight, final_layer.bn.bias, final_layer.bn.running_mean, final_layer.bn.running_var, final_layer.bn.num_batches_tracked, head.fc.weight, head.fc.bias\n",
      "\n",
      "  warn(\"\\n\".join(err_msg), stacklevel=1)\n",
      "/home/sakcay/projects/training_extensions/src/otx/engine/engine.py:767: UserWarning: Warning: ['resume', 'run_hpo', 'hpo_config', 'adaptive_bs'] -> not available in Engine constructor. It will be ignored. Use what need in the right places.\n",
      "  warn(msg, stacklevel=1)\n",
      "WARNING:root:Assign new label_info to the model. It is usually not recommended. Please create a new model instance by giving label_info to its initializer such as `OTXModel(label_info=label_info, ...)`.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight - https://storage.openvinotoolkit.org/repositories/openvino_training_extensions/models/custom_semantic_segmentation/litehrnet18_imagenet1k_rsc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/sakcay/projects/training_extensions/otx-workspace-seg/csv/\n",
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /home/sakcay/projects/training_extensions/otx-workspace-seg exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ LiteHRNet18 │  1.1 M │\n",
       "└───┴───────┴─────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ LiteHRNet18 │  1.1 M │\n",
       "└───┴───────┴─────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.1 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.1 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 4                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.1 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.1 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 4                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9bfa27b61a407db3c4274b3b0fbf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "WARNING:root:You are using AdaptiveTrainScheduling hook. This hook will temporarily update Trainer.check_val_every_n_epoch adaptively: 1 => 4\n",
      "WARNING:root:The patience of early stopping will be changed due to the effect of adaptive interval: 10 --> 3.\n",
      "WARNING:root:The frequency of LRscheduler will be changed due to the effect of adaptive interval: 1 --> 4.\n",
      "WARNING:root:The patience of LRscheduler will be changed due to the effect of adaptive interval: 4 --> 1.\n",
      "WARNING:root:Trainer.log_every_n_steps is higher than the number of iterations in a training epoch. To ensure logging at the last batch, temporarily update Trainer.log_every_n_steps: 50 => 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'lr-Adam': tensor(0.0010),\n",
       " 'lr-Adam-momentum': tensor(0.9000),\n",
       " 'lr-Adam-1': tensor(0.0010),\n",
       " 'lr-Adam-1-momentum': tensor(0.9000),\n",
       " 'train/loss_ce_ignore': tensor(0.3017),\n",
       " 'train/loss': tensor(0.3017),\n",
       " 'train/data_time': tensor(0.0063),\n",
       " 'train/iter_time': tensor(0.1498),\n",
       " 'validation/data_time': tensor(0.0057),\n",
       " 'validation/iter_time': tensor(0.0554),\n",
       " 'val/Dice': tensor(0.8242),\n",
       " 'val/mIoU': tensor(0.7747)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from otx.engine import Engine\n",
    "\n",
    "recipe = \"src/otx/recipe/semantic_segmentation/litehrnet_18.yaml\"\n",
    "override_dataset_format = {\"data.config.data_format\": \"datumaro\"}\n",
    "\n",
    "engine = Engine.from_config(config_path=recipe, data_root=data_root, work_dir=work_dir, **override_dataset_format)\n",
    "engine.train(max_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate torch model\n",
    "Now that the training is complete, we could test the performance of the model on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Assign new tile_config to the model. It is usually not recommended. Please create a new model instance by giving tile_config to its initializer such as `OTXModel(..., tile_config=tile_config)`.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight - https://storage.openvinotoolkit.org/repositories/openvino_training_extensions/models/custom_semantic_segmentation/litehrnet18_imagenet1k_rsc.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9972f92323984bec9ce8e98701838d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/sakcay/.pyenv/versions/3.11.9/envs/otx/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/Dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8821600675582886     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/data_time       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.008264228701591492    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/iter_time       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.08276303857564926    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mIoU         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8091709613800049     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/Dice        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8821600675582886    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/data_time      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.008264228701591492   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/iter_time      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08276303857564926   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mIoU        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8091709613800049    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test/data_time': tensor(0.0083),\n",
       " 'test/iter_time': tensor(0.0828),\n",
       " 'test/Dice': tensor(0.8822),\n",
       " 'test/mIoU': tensor(0.8092)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to IR Model\n",
    "After we ensure the model is making the right predictions and are happy with the model to deploy, the next step is to export the model to IR format. This is particularly useful to improve the inference speed on edge devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Assign new tile_config to the model. It is usually not recommended. Please create a new model instance by giving tile_config to its initializer such as `OTXModel(..., tile_config=tile_config)`.\n",
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight - https://storage.openvinotoolkit.org/repositories/openvino_training_extensions/models/custom_semantic_segmentation/litehrnet18_imagenet1k_rsc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakcay/projects/training_extensions/src/otx/algo/segmentation/backbones/litehrnet.py:180: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  min_size = [int(_) for _ in x[-1].size()[-2:]]\n",
      "/home/sakcay/projects/training_extensions/src/otx/algo/segmentation/backbones/litehrnet.py:1245: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if fuse_y.size()[-2:] != y.size()[-2:]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sakcay/projects/training_extensions/otx-workspace-seg/exported_model.xml')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_ir_model_path = engine.export()\n",
    "exported_ir_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate IR Model\n",
    "After exporting the model, we would like to ensure that accuracy has not dropped. Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harimkan/workspace/repo/otx-regression/src/otx/core/utils/build.py:52: UserWarning: Set the default number of OpenVINO inference requests to 8.\n",
      "            You can specify the value in config.\n",
      "  warnings.warn(msg, stacklevel=1)\n",
      "/home/harimkan/workspace/repo/otx-regression/src/otx/engine/engine.py:346: UserWarning: IR model supports inference only on CPU device. The device is changed automatic.\n",
      "  warn(msg, stacklevel=1)\n",
      "WARNING:root:The corresponding keys in config are not used.: ['verbose', 'data_root', 'task', 'seed', 'callback_monitor', 'resume', 'disable_infer_num_classes', 'workspace']\n",
      "/home/harimkan/workspace/repo/otx-regression/src/otx/engine/utils/auto_configurator.py:397: UserWarning: For OpenVINO IR models, Update the following test \n",
      "\t transforms: [{'class_path': 'torchvision.transforms.v2.ToImage'}] \n",
      "\t transform_lib_type: TORCHVISION \n",
      "\t batch_size: 64 \n",
      "\t image_color_channel: RGB \n",
      "And the tiler is disabled.\n",
      "  warn(msg, stacklevel=1)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/harimkan/workspace/repo/otx-regression/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/home/harimkan/workspace/repo/otx-regression/src/otx/core/data/entity/base.py:591: UserWarning: You set stack_images as True, but not all images in the batch has same shape. In this case, we cannot stack images. Some tasks, e.g., detection, can have different image shapes among samples in the batch. However, if it is not your intention, consider setting stack_images as False in the config.\n",
      "  warnings.warn(msg, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/Dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8523644804954529     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mIoU         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8035472631454468     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/Dice        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8523644804954529    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mIoU        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8035472631454468    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test/Dice': tensor(0.8524), 'test/mIoU': tensor(0.8035)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.test(checkpoint=exported_ir_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
