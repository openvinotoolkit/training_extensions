{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "\n",
    "from otx.algorithms.anomaly.adapters.anomalib.data.dataset import (\n",
    "    AnomalyClassificationDataset,\n",
    ")\n",
    "from otx.api.configuration.helper import create as create_hyper_parameters\n",
    "from otx.api.entities.datasets import DatasetEntity\n",
    "from otx.api.entities.inference_parameters import InferenceParameters\n",
    "from otx.api.entities.label_schema import LabelSchemaEntity\n",
    "from otx.api.entities.model import ModelEntity\n",
    "from otx.api.entities.model_template import (\n",
    "    ModelTemplate,\n",
    "    TaskType,\n",
    "    parse_model_template,\n",
    ")\n",
    "from otx.api.entities.optimization_parameters import OptimizationParameters\n",
    "from otx.api.entities.resultset import ResultSetEntity\n",
    "from otx.api.entities.subset import Subset\n",
    "from otx.api.entities.task_environment import TaskEnvironment\n",
    "from otx.api.entities.train_parameters import TrainParameters\n",
    "from otx.api.usecases.adapters.model_adapter import ModelAdapter\n",
    "from otx.api.usecases.tasks.interfaces.export_interface import ExportType\n",
    "from otx.api.usecases.tasks.interfaces.optimization_interface import OptimizationType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = {\n",
    "    \"ann_file\": \"../../data/anomaly/classification/train.json\",\n",
    "    \"data_root\": \"../../data/anomaly/shapes\",\n",
    "}\n",
    "\n",
    "val_subset = {\"ann_file\": \"../../data/anomaly/classification/val.json\", \"data_root\": \"../../data/anomaly/shapes\"}\n",
    "\n",
    "test_subset = {\n",
    "    \"ann_file\": \"../../data/anomaly/classification/test.json\",\n",
    "    \"data_root\": \"../../data/anomaly/shapes\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANOMALY_CLASSIFICATION"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_template_path = \"../../otx/algorithms/anomaly/configs/classification/padim/template.yaml\"\n",
    "\n",
    "model_template = parse_model_template(model_template_path)\n",
    "model_template.task_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabelEntity(1, name=Anomalous, hotkey=, domain=ANOMALY_CLASSIFICATION, color=Color(red=75, green=168, blue=234, alpha=255), is_anomalous=True),\n",
       " LabelEntity(0, name=Normal, hotkey=, domain=ANOMALY_CLASSIFICATION, color=Color(red=71, green=200, blue=25, alpha=255), is_anomalous=False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = AnomalyClassificationDataset(train_subset=train_subset, val_subset=val_subset, test_subset=test_subset)\n",
    "dataset.get_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Task and Task Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_environment(model_template: ModelTemplate, dataset: DatasetEntity) -> TaskEnvironment:\n",
    "    \"\"\"Create task environment.\"\"\"\n",
    "    hyper_parameters = create_hyper_parameters(model_template.hyper_parameters.data)\n",
    "    labels = dataset.get_labels()\n",
    "    label_schema = LabelSchemaEntity.from_labels(labels)\n",
    "\n",
    "    return TaskEnvironment(\n",
    "        model_template=model_template,\n",
    "        model=None,\n",
    "        hyper_parameters=hyper_parameters,\n",
    "        label_schema=label_schema,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task(model_template: ModelTemplate, task_environment: TaskEnvironment, task: str) -> Any:\n",
    "    \"\"\"Create base torch or openvino task.\n",
    "\n",
    "    Args:\n",
    "        task (str): task type. Either base or openvino.\n",
    "\n",
    "    Returns:\n",
    "        Any: Base Torch or OpenVINO Task Class.\n",
    "\n",
    "    Example:\n",
    "        >>> create_task(model_template=model_template, task=\"base\")\n",
    "        <anomaly_classification.torch_task.AnomalyClassificationTask>\n",
    "\n",
    "    \"\"\"\n",
    "    if model_template.entrypoints is not None:\n",
    "        task_path = getattr(model_template.entrypoints, task)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot create {task} task. `model_template.entrypoint` does not have {task}\")\n",
    "\n",
    "    module_name, class_name = task_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)(task_environment=task_environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskEnvironment(model=None, label_schema=LabelSchemaEntity(label_groups=[LabelGroup(id=6331ac071e334d2562411e60, name=from_label_list, group_type=LabelGroupType.EXCLUSIVE, labels=[LabelEntity(0, name=Normal, hotkey=, domain=ANOMALY_CLASSIFICATION, color=Color(red=71, green=200, blue=25, alpha=255), is_anomalous=False), LabelEntity(1, name=Anomalous, hotkey=, domain=ANOMALY_CLASSIFICATION, color=Color(red=75, green=168, blue=234, alpha=255), is_anomalous=True)])]), hyper_params=CONFIGURABLE_PARAMETERS(header='Configuration for Padim', description='Configuration for Padim', visible_in_ui=True, id=ID()))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_environment = create_task_environment(model_template, dataset)\n",
    "task_environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/nncf/torch/dynamic_graph/patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n",
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/defusedxml/__init__.py:30: DeprecationWarning: defusedxml.cElementTree is deprecated, import from defusedxml.ElementTree instead.\n",
      "  from . import cElementTree\n",
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: ote-alpha is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py:111: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  warnings.warn(\n",
      "[INFO] 2022-09-26 15:41:29,445 - otx.algorithms.anomaly.tasks.inference - Initializing the task environment.\n",
      "[INFO] 2022-09-26 15:41:29,712 - otx.algorithms.anomaly.tasks.inference - No trained model in project yet. Created new model with 'PADIM'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<otx.algorithms.anomaly.tasks.train.TrainingTask at 0x7f5622efdf70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_task = create_task(model_template, task_environment, \"base\")\n",
    "torch_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:29,905 - otx.algorithms.anomaly.tasks.train - Training the model.\n",
      "[INFO] 2022-09-26 15:41:29,938 - otx.algorithms.anomaly.tasks.train - Training Configs '{'dataset': {'name': 'mvtec', 'format': 'mvtec', 'path': './datasets/MVTec', 'category': 'bottle', 'task': 'classification', 'image_size': [256, 256], 'train_batch_size': 32, 'test_batch_size': 32, 'num_workers': 8, 'transform_config': {'train': None, 'val': None}, 'create_validation_set': False, 'tiling': {'apply': False, 'tile_size': None, 'stride': None, 'remove_border_count': 0, 'use_random_tiling': False, 'random_tile_count': 16}}, 'model': {'name': 'padim', 'backbone': 'resnet18', 'pre_trained': True, 'layers': ['layer1', 'layer2', 'layer3'], 'normalization_method': 'min_max', 'input_size': [256, 256]}, 'metrics': {'image': ['F1Score', 'AUROC'], 'pixel': ['F1Score', 'AUROC'], 'threshold': {'image_default': 3, 'pixel_default': 3, 'adaptive': True}}, 'visualization': {'show_images': False, 'save_images': True, 'log_images': True, 'image_save_path': None, 'mode': 'full'}, 'project': {'seed': 42, 'path': '/tmp/otx-anomaliblnf_wqin'}, 'logging': {'logger': [], 'log_graph': False}, 'optimization': {'export_mode': None}, 'trainer': {'accelerator': 'auto', 'accumulate_grad_batches': 1, 'amp_backend': 'native', 'auto_lr_find': False, 'auto_scale_batch_size': False, 'auto_select_gpus': False, 'benchmark': False, 'check_val_every_n_epoch': 1, 'default_root_dir': 'results/padim/mvtec/bottle', 'detect_anomaly': False, 'deterministic': False, 'devices': 1, 'enable_checkpointing': True, 'enable_model_summary': True, 'enable_progress_bar': True, 'fast_dev_run': False, 'gpus': None, 'gradient_clip_val': 0, 'ipus': None, 'limit_predict_batches': 1.0, 'limit_test_batches': 1.0, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'log_every_n_steps': 50, 'max_epochs': 1, 'max_steps': -1, 'max_time': None, 'min_epochs': None, 'min_steps': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'num_nodes': 1, 'num_processes': None, 'num_sanity_val_steps': 0, 'overfit_batches': 0.0, 'plugins': None, 'precision': 32, 'profiler': None, 'reload_dataloaders_every_n_epochs': 0, 'replace_sampler_ddp': True, 'sync_batchnorm': False, 'tpu_cores': None, 'track_grad_norm': -1, 'val_check_interval': 1.0}}'\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "[INFO] 2022-09-26 15:41:29,953 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'TRAINING' subset size: Total '20' images. Normal: '20', images. Anomalous: '0' images\n",
      "[INFO] 2022-09-26 15:41:29,955 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'VALIDATION' subset size: Total '12' images. Normal: '4', images. Anomalous: '8' images\n",
      "[INFO] 2022-09-26 15:41:29,958 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'TESTING' subset size: Total '18' images. Normal: '6', images. Anomalous: '12' images\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | image_threshold       | AdaptiveThreshold        | 0     \n",
      "1 | pixel_threshold       | AdaptiveThreshold        | 0     \n",
      "2 | model                 | PadimModel               | 2.8 M \n",
      "3 | normalization_metrics | MinMax                   | 0     \n",
      "4 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "5 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.131    Total estimated model params size (MB)\n",
      "[INFO] 2022-09-26 15:41:31,815 - otx.algorithms.anomaly.adapters.anomalib.data.data - Global annotations: 12\n",
      "[INFO] 2022-09-26 15:41:31,815 - otx.algorithms.anomaly.adapters.anomalib.data.data - Local annotations: 4\n",
      "[INFO] 2022-09-26 15:41:31,816 - otx.algorithms.anomaly.adapters.anomalib.data.data - Dataset does not contain polygon annotations. Not passing masks to anomalib.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9352a5bf8c46d58ef0f7426d43d4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75595fe33d504d78a53796894ada78b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG-HPO] logged_metrics = {'image_F1Score': tensor(0.8889, device='cuda:0'), 'image_AUROC': tensor(0.5625, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:33,703 - otx.algorithms.anomaly.tasks.inference - Saving the model weights.\n",
      "[INFO] 2022-09-26 15:41:33,857 - otx.algorithms.anomaly.tasks.train - Training completed.\n"
     ]
    }
   ],
   "source": [
    "torch_output_model = ModelEntity(train_dataset=dataset, configuration=task_environment.get_model_configuration())\n",
    "torch_task.train(dataset, torch_output_model, TrainParameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Performance of Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(task: Any, output_model: ModelEntity, dataset: DatasetEntity):\n",
    "    ground_truth_validation_dataset = dataset.get_subset(Subset.VALIDATION)\n",
    "    prediction_validation_dataset = task.infer(\n",
    "        dataset=ground_truth_validation_dataset.with_empty_annotations(),\n",
    "        inference_parameters=InferenceParameters(is_evaluation=True),\n",
    "    )\n",
    "\n",
    "    result_set = ResultSetEntity(\n",
    "        model=output_model,\n",
    "        ground_truth_dataset=ground_truth_validation_dataset,\n",
    "        prediction_dataset=prediction_validation_dataset,\n",
    "    )\n",
    "\n",
    "    task.evaluate(result_set)\n",
    "    print(result_set.performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:34,290 - otx.algorithms.anomaly.tasks.inference - Performing inference on the validation set using the base torch model.\n",
      "[INFO] 2022-09-26 15:41:34,303 - otx.algorithms.anomaly.tasks.inference - Inference Configs '{'dataset': {'name': 'mvtec', 'format': 'mvtec', 'path': './datasets/MVTec', 'category': 'bottle', 'task': 'classification', 'image_size': [256, 256], 'train_batch_size': 32, 'test_batch_size': 32, 'num_workers': 8, 'transform_config': {'train': None, 'val': None}, 'create_validation_set': False, 'tiling': {'apply': False, 'tile_size': None, 'stride': None, 'remove_border_count': 0, 'use_random_tiling': False, 'random_tile_count': 16}}, 'model': {'name': 'padim', 'backbone': 'resnet18', 'pre_trained': True, 'layers': ['layer1', 'layer2', 'layer3'], 'normalization_method': 'min_max', 'input_size': [256, 256]}, 'metrics': {'image': ['F1Score', 'AUROC'], 'pixel': ['F1Score', 'AUROC'], 'threshold': {'image_default': 3, 'pixel_default': 3, 'adaptive': True}}, 'visualization': {'show_images': False, 'save_images': True, 'log_images': True, 'image_save_path': None, 'mode': 'full'}, 'project': {'seed': 42, 'path': '/tmp/otx-anomaliblnf_wqin'}, 'logging': {'logger': [], 'log_graph': False}, 'optimization': {'export_mode': None}, 'trainer': {'accelerator': 'auto', 'accumulate_grad_batches': 1, 'amp_backend': 'native', 'auto_lr_find': False, 'auto_scale_batch_size': False, 'auto_select_gpus': False, 'benchmark': False, 'check_val_every_n_epoch': 1, 'default_root_dir': 'results/padim/mvtec/bottle', 'detect_anomaly': False, 'deterministic': False, 'devices': 1, 'enable_checkpointing': True, 'enable_model_summary': True, 'enable_progress_bar': True, 'fast_dev_run': False, 'gpus': None, 'gradient_clip_val': 0, 'ipus': None, 'limit_predict_batches': 1.0, 'limit_test_batches': 1.0, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'log_every_n_steps': 50, 'max_epochs': 1, 'max_steps': -1, 'max_time': None, 'min_epochs': None, 'min_steps': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'num_nodes': 1, 'num_processes': None, 'num_sanity_val_steps': 0, 'overfit_batches': 0.0, 'plugins': None, 'precision': 32, 'profiler': None, 'reload_dataloaders_every_n_epochs': 0, 'replace_sampler_ddp': True, 'sync_batchnorm': False, 'tpu_cores': None, 'track_grad_norm': -1, 'val_check_interval': 1.0}}'\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87bf369d1246b69e1c4abb06a784f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:34,623 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.626 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,624 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Normal', 0.457 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,625 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.803 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,626 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Normal', 0.495 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,627 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.500 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,628 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.529 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,628 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.513 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,629 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.708 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,630 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.799 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,631 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.557 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,632 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.623 min: 0.4732818603515625 max: 49.69810104370117\n",
      "[INFO] 2022-09-26 15:41:34,633 - otx.algorithms.anomaly.adapters.anomalib.callbacks.inference - \n",
      "\tThreshold: 34.800, Assigned Label 'Anomalous', 0.500 min: 0.4732818603515625 max: 49.69810104370117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(score: 0.8333333333333333, dashboard: (4 metric groups))\n"
     ]
    }
   ],
   "source": [
    "print_performance(torch_task, torch_output_model, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:34,866 - otx.algorithms.anomaly.tasks.inference - Exporting the OpenVINO model.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: ote-alpha is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: ote-alpha is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/tmp/otx-anomaliblnf_wqin/onnx_model.onnx\n",
      "\t- Path for generated IR: \t/tmp/otx-anomaliblnf_wqin\n",
      "\t- IR output name: \tonnx_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: ote-alpha is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO runtime found in: \t/home/ashwin/miniconda3/envs/ote/lib/python3.8/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/otx-anomaliblnf_wqin/onnx_model.xml\n",
      "[ SUCCESS ] BIN file: /tmp/otx-anomaliblnf_wqin/onnx_model.bin\n",
      "[ SUCCESS ] Total execution time: 0.65 seconds. \n",
      "[ SUCCESS ] Memory consumed: 559 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n"
     ]
    }
   ],
   "source": [
    "exported_model = ModelEntity(\n",
    "    train_dataset=dataset,\n",
    "    configuration=task_environment.get_model_configuration(),\n",
    ")\n",
    "torch_task.export(ExportType.OPENVINO, exported_model)\n",
    "task_environment.model = exported_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Performance of OpenVINO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:37,496 - otx.algorithms.anomaly.tasks.openvino - Initializing the OpenVINO task.\n",
      "[INFO] 2022-09-26 15:41:37,712 - otx.algorithms.anomaly.tasks.openvino - Start OpenVINO inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(score: 0.8333333333333333, dashboard: (1 metric groups))\n"
     ]
    }
   ],
   "source": [
    "openvino_task = create_task(model_template, task_environment, task=\"openvino\")\n",
    "print_performance(openvino_task, exported_model, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POT Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:41:38,118 - otx.algorithms.anomaly.tasks.openvino - Starting POT optimization.\n",
      "[INFO] 2022-09-26 15:42:02,266 - otx.algorithms.anomaly.tasks.openvino - POT optimization completed\n"
     ]
    }
   ],
   "source": [
    "optimized_model = ModelEntity(\n",
    "    dataset,\n",
    "    configuration=task_environment.get_model_configuration(),\n",
    ")\n",
    "openvino_task.optimize(\n",
    "    optimization_type=OptimizationType.POT,\n",
    "    dataset=dataset.get_subset(Subset.TRAINING),\n",
    "    output_model=optimized_model,\n",
    "    optimization_parameters=OptimizationParameters(),\n",
    ")\n",
    "task_environment.model = optimized_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Performance of POT Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:42:02,479 - otx.algorithms.anomaly.tasks.openvino - Start OpenVINO inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(score: 0.3333333333333332, dashboard: (1 metric groups))\n"
     ]
    }
   ],
   "source": [
    "print_performance(openvino_task, optimized_model, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNCF Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:42:02,928 - otx.algorithms.anomaly.tasks.inference - Initializing the task environment.\n",
      "[INFO] 2022-09-26 15:42:03,281 - otx.algorithms.anomaly.tasks.nncf - Loaded model weights from Task Environment\n",
      "[INFO] 2022-09-26 15:42:03,286 - otx.algorithms.anomaly.tasks.nncf - Optimization the model.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "[INFO] 2022-09-26 15:42:03,298 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'TRAINING' subset size: Total '20' images. Normal: '20', images. Anomalous: '0' images\n",
      "[INFO] 2022-09-26 15:42:03,300 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'VALIDATION' subset size: Total '12' images. Normal: '4', images. Anomalous: '8' images\n",
      "[INFO] 2022-09-26 15:42:03,302 - otx.algorithms.anomaly.adapters.anomalib.data.data - 'TESTING' subset size: Total '18' images. Normal: '6', images. Anomalous: '12' images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Please, provide execution parameters for optimal model initialization\n",
      "INFO:nncf:Please, provide execution parameters for optimal model initialization\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFBatchNorm2d[bn2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFBatchNorm2d[bn2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFBatchNorm2d[bn2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFBatchNorm2d[bn2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFBatchNorm2d[bn2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn1] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFBatchNorm2d[bn1]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2] by PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFBatchNorm2d[bn2]\n",
      "WARNING:nncf:NNCFNetwork(\n",
      "  (nncf_module): PadimModel(\n",
      "    (feature_extractor): FeatureExtractor(\n",
      "      (feature_extractor): FeatureListNet(\n",
      "        (conv1): NNCFConv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): NNCFBatchNorm2d(\n",
      "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): NNCFConv2d(\n",
      "                64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "                (pre_ops): ModuleDict()\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (1): NNCFBatchNorm2d(\n",
      "                128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                (pre_ops): ModuleDict()\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): NNCFConv2d(\n",
      "                128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "                (pre_ops): ModuleDict()\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (1): NNCFBatchNorm2d(\n",
      "                256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                (pre_ops): ModuleDict()\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): NNCFConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn1): NNCFBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (conv2): NNCFConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (bn2): NNCFBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (pre_ops): ModuleDict()\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (act2): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (anomaly_map_generator): AnomalyMapGenerator(\n",
      "      (blur): GaussianBlur2d()\n",
      "    )\n",
      "    (gaussian): MultiVariateGaussian()\n",
      "  )\n",
      ")\n",
      "INFO:nncf:Collecting tensor statistics                 | 1 / 8\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [5.3588, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Processing linked quantizer group:\n",
      " PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/ReLU[act2]/relu__0|OUTPUT\n",
      "PadimModel/interpolate_0|OUTPUT\n",
      "PadimModel/interpolate_1|OUTPUT\n",
      "\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/interpolate_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/interpolate_1\n",
      "INFO:nncf:Set input_low: [-1.9675, ] and input_range: [4.6075, ] for TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [2.8478, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-4.0576, ] and input_range: [7.3965, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [2.2805, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-5.8825, ] and input_range: [8.9217, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [2.6210, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-2.2296, ] and input_range: [5.1054, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [-2.6558, ] and input_range: [5.1748, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [2.0034, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-3.5348, ] and input_range: [6.9652, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [3.0096, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-2.2904, ] and input_range: [8.1214, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [-1.8459, ] and input_range: [2.5496, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFBatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [2.6807, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [-3.8286, ] and input_range: [7.7590, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFBatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [4.4501, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Set input_low: [-7.1963, ] and input_range: [15.8779, ] for TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/matmul_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/matmul_0\n",
      "INFO:nncf:Set input_low: [0.6636, ] and input_range: [17.2248, ] for TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/clamp_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/clamp_0\n",
      "INFO:nncf:Set input_low: [0.7006, ] and input_range: [3.5289, ] for TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/sqrt_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/sqrt_0\n",
      "INFO:nncf:Set input_low: [0.7006, ] and input_range: [3.5281, ] for TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/interpolate_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/interpolate_0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [3.6741, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/ReLU[act1]/relu__0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [4.1880, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [3.3134, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [5.3161, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Set input_low: [0.0000, ] and input_range: [5.2987, ] for TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/ReLU[act2]/relu__0\n",
      "INFO:nncf:Set input_low: [-1.6273, ] and input_range: [5.4086, ] for TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/__sub___0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK PadimModel/AnomalyMapGenerator[anomaly_map_generator]/__sub___0\n",
      "WARNING:nncf:The overflow issue fix will be applied. Now all weight quantizers will effectively use only 7 bits out of 8 bits. This resolves the overflow issue problem on AVX2 and AVX-512 machines. Please take a look at the documentation for a detailed information.\n",
      "INFO:nncf:Set sign: True and scale: [0.7703, 0.3222, 0.1000, 0.1696, 0.1000, 0.4147, 0.5763, 0.1000, 0.7567, 0.1000, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.7993, 0.2508, 0.5557, 0.3661, 0.3500, 0.3276, 0.3736, 0.1836, 0.4557, 0.4273, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.3302, 0.1437, 0.1000, 0.2294, 0.1371, 0.2306, 0.2437, 0.1808, 0.2743, 0.2551, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2256, 0.4316, 0.2377, 0.1686, 0.2971, 0.3104, 0.1985, 0.6491, 0.3446, 0.3593, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2008, 0.3114, 0.1843, 0.1882, 0.1697, 0.3397, 0.2904, 0.2927, 0.2811, 0.2152, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1788, 0.1604, 0.2078, 0.2146, 0.1900, 0.2315, 0.1253, 0.2421, 0.1400, 0.1772, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2259, 0.1478, 0.2487, 0.1070, 0.2211, 0.2044, 0.1799, 0.1703, 0.1824, 0.1680, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.5049, 0.1092, 0.1662, 0.3169, 0.1706, 0.1479, 0.1722, 0.4003, 0.1631, 0.3853, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2546, 0.1527, 0.1660, 0.1682, 0.2018, 0.2016, 0.1172, 0.2657, 0.2204, 0.2895, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1359, 0.1000, 0.1542, 0.1781, 0.1437, 0.1296, 0.1312, 0.1282, 0.1148, 0.1587, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1744, 0.2802, 0.1251, 0.1749, 0.1115, 0.1328, 0.1240, 0.1190, 0.1315, 0.1113, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1279, 0.1107, 0.1000, 0.1867, 0.1346, 0.1987, 0.1606, 0.1013, 0.1537, 0.1228, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1160, 0.1058, 0.1000, 0.1571, 0.1000, 0.1369, 0.1015, 0.1000, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1137, 0.2067, 0.1065, 0.2062, 0.1000, 0.1023, 0.2301, 0.1488, 0.1000, 0.1293, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1343, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS PadimModel/FeatureExtractor[feature_extractor]/FeatureListNet[feature_extractor]/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:BatchNorm statistics adaptation                 | 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | image_threshold       | AdaptiveThreshold        | 0     \n",
      "1 | pixel_threshold       | AdaptiveThreshold        | 0     \n",
      "2 | model                 | NNCFNetwork              | 2.8 M \n",
      "3 | normalization_metrics | MinMax                   | 0     \n",
      "4 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "5 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "57        Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.141    Total estimated model params size (MB)\n",
      "[INFO] 2022-09-26 15:42:06,189 - otx.algorithms.anomaly.adapters.anomalib.data.data - Global annotations: 12\n",
      "[INFO] 2022-09-26 15:42:06,190 - otx.algorithms.anomaly.adapters.anomalib.data.data - Local annotations: 4\n",
      "[INFO] 2022-09-26 15:42:06,190 - otx.algorithms.anomaly.adapters.anomalib.data.data - Dataset does not contain polygon annotations. Not passing masks to anomalib.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea96d3f37c1f4c05a479a2e11e67b00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b0f315a27245379606484d0b233f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:42:08,233 - otx.algorithms.anomaly.tasks.inference - Saving the model weights.\n",
      "[INFO] 2022-09-26 15:42:08,534 - otx.algorithms.anomaly.tasks.nncf - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model from weights.\n",
    "init_model = ModelEntity(\n",
    "    dataset,\n",
    "    configuration=task_environment.get_model_configuration(),\n",
    "    model_adapters={\"weights.pth\": ModelAdapter(torch_output_model.get_data(\"weights.pth\"))},\n",
    ")\n",
    "\n",
    "task_environment.model = init_model\n",
    "\n",
    "nncf_task = create_task(model_template, task_environment, task=\"nncf\")\n",
    "optimized_model = ModelEntity(\n",
    "    dataset,\n",
    "    configuration=task_environment.get_model_configuration(),\n",
    ")\n",
    "nncf_task.optimize(OptimizationType.NNCF, dataset, optimized_model)\n",
    "task_environment.model = optimized_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Performance on NNCF Optimized Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-09-26 15:42:08,765 - otx.algorithms.anomaly.tasks.openvino - Start OpenVINO inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(score: 0.3333333333333332, dashboard: (1 metric groups))\n"
     ]
    }
   ],
   "source": [
    "print_performance(openvino_task, exported_model, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"./results\"\n",
    "if os.path.exists(results_dir):\n",
    "    shutil.rmtree(results_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ote')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f2f4bdd842cfd538d6761530a2501b68629878cccb8c1dc4b8385cd44d74110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
