

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Semantic Segmentation &#8212; OpenVINO™ Training Extensions 1.6.0dev documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'guide/explanation/algorithms/segmentation/semantic_segmentation';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Instance Segmentation" href="instance_segmentation.html" />
    <link rel="prev" title="Segmentation" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/otx-logo.png" class="logo__image only-light" alt="OpenVINO™ Training Extensions 1.6.0dev documentation - Home"/>
    <script>document.write(`<img src="../../../../_static/otx-logo.png" class="logo__image only-dark" alt="OpenVINO™ Training Extensions 1.6.0dev documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/cli_commands.html">OpenVINO™ Training Extensions CLI commands</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/base/index.html">Base Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tutorials/base/how_to_train/index.html">How to train, validate, export and optimize the model</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/classification.html">Classification  model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/detection.html">Object Detection model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/instance_segmentation.html">Instance Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/semantic_segmentation.html">Semantic Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/anomaly_detection.html">Anomaly Detection Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/action_classification.html">Action Classification model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/action_detection.html">Action Detection model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/demo.html">How to run the demonstration mode with OpenVINO™ Training Extensions CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/deploy.html">How to deploy the model and use demo in exportable code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/explain.html">How to explain the model behavior</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/advanced/index.html">Advanced Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/semi_sl.html">Use Semi-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/self_sl.html">Use Self-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/backbones.html">Backbone Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/api_tutorial.html">Utilize OpenVINO™ Training Extensions APIs in your project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/hpo_tutorial.html">Simple HPO Tutorial</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Explanation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Algorithms</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../classification/index.html">Classification</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../classification/multi_class_classification.html">Multi-class Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../classification/multi_label_classification.html">Multi-label Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../classification/hierarhical_classification.html">Hierarchical Classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../object_detection/index.html">Object Detection</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../object_detection/object_detection.html">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Segmentation</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Semantic Segmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="instance_segmentation.html">Instance Segmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../anomaly/index.html">Anomaly Detection</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../action/index.html">Action Recognition</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../action/action_classification.html">Action Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../action/action_detection.html">Action Detection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../visual_prompting/index.html">Visual Prompting</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../visual_prompting/fine_tuning.html">Visual Prompting (Fine-tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../visual_prompting/zero_shot.html">Visual Prompting (Zero-shot learning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../additional_features/index.html">Additional Features</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/models_optimization.html">Models Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/hpo.html">Hyperparameters Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/auto_configuration.html">Auto-configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/adaptive_training.html">Adaptive Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/xai.html">Explainable AI (XAI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/noisy_label_detection.html">Noisy Label Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/fast_data_loading.html">Fast Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/tiling.html">Improve Small Object Detection with Image Tiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/config_input_size.html">Configurable Input Size</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../reference/index.html">API reference</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.html">otx</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.html">otx.algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.action.html">otx.algorithms.action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.anomaly.html">otx.algorithms.anomaly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.classification.html">otx.algorithms.classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.common.html">otx.algorithms.common</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.detection.html">otx.algorithms.detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.segmentation.html">otx.algorithms.segmentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.visual_prompting.html">otx.algorithms.visual_prompting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.api.html">otx.api</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.configuration.html">otx.api.configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.entities.html">otx.api.entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.serialization.html">otx.api.serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.usecases.html">otx.api.usecases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.utils.html">otx.api.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.html">otx.cli</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.builder.html">otx.cli.builder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.manager.html">otx.cli.manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.registry.html">otx.cli.registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.tools.html">otx.cli.tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.utils.html">otx.cli.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.core.html">otx.core</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.data.html">otx.core.data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.file.html">otx.core.file</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.ov.html">otx.core.ov</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.patcher.html">otx.core.patcher</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../reference/_autosummary/otx.hpo.html">otx.hpo</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.html">otx.recipes</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.stages.html">otx.recipes.stages</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.html">otx.utils</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.logger.html">otx.utils.logger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.utils.html">otx.utils.utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../release_notes/index.html">Releases</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">





<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Guide</a></li>
    
    
    <li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Segmentation</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Semantic...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="semantic-segmentation">
<h1>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this heading">#</a></h1>
<p>Semantic segmentation is a computer vision task in which an algorithm assigns a label or class to each pixel in an image.
For example, semantic segmentation can be used to identify the boundaries of different objects in an image, such as cars, buildings, and trees.
The output of semantic segmentation is typically an image where each pixel is colored with a different color or label depending on its class.</p>
<a class="reference internal image-reference" href="../../../../_images/semantic_seg_example.png" id="semantic-segmentation-image-example"><img alt="image uploaded from this `source &lt;https://arxiv.org/abs/1912.03183&gt;`_" id="semantic-segmentation-image-example" src="../../../../_images/semantic_seg_example.png" style="width: 600px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>We solve this task by utilizing <a class="reference external" href="https://arxiv.org/pdf/1411.4038.pdf">FCN Head</a> with implementation from <a class="reference external" href="https://mmsegmentation.readthedocs.io/en/latest/_modules/mmseg/models/decode_heads/fcn_head.html">MMSegmentation</a> on the multi-level image features obtained by the feature extractor backbone (<a class="reference external" href="https://arxiv.org/abs/2104.06403">Lite-HRNet</a>).
For the supervised training we use the following algorithms components:</p>
<ul id="semantic-segmentation-supervised-pipeline">
<li><p><code class="docutils literal notranslate"><span class="pre">Augmentations</span></code>: Besides basic augmentations like random flip, random rotate and random crop, we use mixing images technique with different <a class="reference external" href="https://mmsegmentation.readthedocs.io/en/latest/api.html#mmseg.datasets.pipelines.PhotoMetricDistortion">photometric distortions</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>: We use <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> optimizer with weight decay set to zero and gradient clipping with maximum quadratic norm equals to 40.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Learning</span> <span class="pre">rate</span> <span class="pre">schedule</span></code>: For scheduling training process we use <strong>ReduceLROnPlateau</strong> with linear learning rate warmup for 100 iterations. This method monitors a target metric (in our case we use metric on the validation set) and if no improvement is seen for a <code class="docutils literal notranslate"><span class="pre">patience</span></code> number of epochs, the learning rate is reduced.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Loss</span> <span class="pre">function</span></code>: We use standard <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">Cross Entropy Loss</a>  to train a model.</p></li>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">Additional</span> <span class="pre">training</span> <span class="pre">techniques</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Early</span> <span class="pre">stopping</span></code>: To add adaptability to the training pipeline and prevent overfitting. You can use early stopping like the below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train {TEMPLATE} ... \
            params \
            --learning_parameters.enable_early_stopping=True
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<section id="dataset-format">
<h2>Dataset Format<a class="headerlink" href="#dataset-format" title="Permalink to this heading">#</a></h2>
<p>For the dataset handling inside OpenVINO™ Training Extensions, we use <a class="reference external" href="https://github.com/openvinotoolkit/datumaro">Dataset Management Framework (Datumaro)</a>.</p>
<p>At this end we support <a class="reference external" href="https://github.com/openvinotoolkit/datumaro/blob/develop/docs/source/docs/data-formats/formats/common_semantic_segmentation.md">Common Semantic Segmentation</a> data format.
If you organized supported dataset format, starting training will be very simple. We just need to pass a path to the root folder and desired model template to start training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train  &lt;model_template&gt; --train-data-roots &lt;path_to_data_root&gt; \
                                        --val-data-roots &lt;path_to_data_root&gt;
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to some internal limitations, the dataset should always consist of a “background” label. If your dataset doesn’t have a background label, rename the first label to “background” in the <code class="docutils literal notranslate"><span class="pre">meta.json</span></code> file.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, metrics with models trained with our OTX dataset adapter can differ from popular benchmarks. To avoid this and train the model on exactly the same segmentation masks as intended by the authors, please, set the parameter <code class="docutils literal notranslate"><span class="pre">use_otx_adapter</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div>
</section>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this heading">#</a></h2>
<p id="semantic-segmentation-models">We support the following ready-to-use model templates:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ocr_lite_hrnet_s_mod2/template.yaml">Custom_Semantic_Segmentation_Lite-HRNet-s-mod2_OCR</a></p></td>
<td><p>Lite-HRNet-s-mod2</p></td>
<td><p>1.44</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ocr_lite_hrnet_18_mod2/template.yaml">Custom_Semantic_Segmentation_Lite-HRNet-18-mod2_OCR</a></p></td>
<td><p>Lite-HRNet-18-mod2</p></td>
<td><p>2.82</p></td>
<td><p>4.3</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ocr_lite_hrnet_x_mod3/template.yaml">Custom_Semantic_Segmentation_Lite-HRNet-x-mod3_OCR</a></p></td>
<td><p>Lite-HRNet-x-mod3</p></td>
<td><p>9.20</p></td>
<td><p>5.7</p></td>
</tr>
</tbody>
</table>
<p>All of these models are members of the same <a class="reference external" href="https://arxiv.org/abs/2104.06403">Lite-HRNet</a> backbones family. They differ in the trade-off between accuracy and inference/training speed. <code class="docutils literal notranslate"><span class="pre">Lite-HRNet-x-mod3</span></code> is the template with heavy-size architecture for accurate predictions but it requires long training.
Whereas the <code class="docutils literal notranslate"><span class="pre">Lite-HRNet-s-mod2</span></code> is the lightweight architecture for fast inference and training. It is the best choice for the scenario of a limited amount of data. The <code class="docutils literal notranslate"><span class="pre">Lite-HRNet-18-mod2</span></code> model is the middle-sized architecture for the balance between fast inference and training time.</p>
<p>Besides this, we added new templates in experimental phase. For now, they support only supervised incremental training type. To run training with new templates we should use direct path to <code class="docutils literal notranslate"><span class="pre">template_experimental.yaml</span></code>.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ham_segnext_t/template_experimental.yaml">Custom_Semantic_Segmentation_SegNext_T</a></p></td>
<td><p>SegNext-t</p></td>
<td><p>6.07</p></td>
<td><p>4.23</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ham_segnext_s/template_experimental.yaml">Custom_Semantic_Segmentation_SegNext_S</a></p></td>
<td><p>SegNext-s</p></td>
<td><p>15.35</p></td>
<td><p>13.9</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/segmentation/configs/ham_segnext_b/template_experimental.yaml">Custom_Semantic_Segmentation_SegNext_B</a></p></td>
<td><p>SegNext-b</p></td>
<td><p>32.08</p></td>
<td><p>27.56</p></td>
</tr>
</tbody>
</table>
<p>New templates use <a class="reference external" href="https://arxiv.org/abs/2209.08575">SegNext</a> model which can achieve superior perfomance while preserving fast inference and fast training.</p>
<p>In the table below the <a class="reference external" href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">Dice score</a> on some academic datasets using our <a class="reference internal" href="#semantic-segmentation-supervised-pipeline"><span class="std std-ref">supervised pipeline</span></a> is presented. We use 512x512 image crop resolution, for other hyperparameters, please, refer to the related template. We trained each model with single Nvidia GeForce RTX3090.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p><a class="reference external" href="https://xuebinqin.github.io/dis/index.html">DIS5K</a></p></th>
<th class="head"><p><a class="reference external" href="https://www.cityscapes-dataset.com/">Cityscapes</a></p></th>
<th class="head"><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal-VOC 2012</a></p></th>
<th class="head"><p><a class="reference external" href="https://www.cvlibs.net/datasets/kitti/index.php">KITTI full</a></p></th>
<th class="head"><p>Mean</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Lite-HRNet-s-mod2</p></td>
<td><p>79.95</p></td>
<td><p>62.38</p></td>
<td><p>58.26</p></td>
<td><p>36.06</p></td>
<td><p>59.16</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-18-mod2</p></td>
<td><p>81.12</p></td>
<td><p>65.04</p></td>
<td><p>63.48</p></td>
<td><p>39.14</p></td>
<td><p>62.20</p></td>
</tr>
<tr class="row-even"><td><p>Lite-HRNet-x-mod3</p></td>
<td><p>79.98</p></td>
<td><p>59.97</p></td>
<td><p>61.9</p></td>
<td><p>41.55</p></td>
<td><p>60.85</p></td>
</tr>
<tr class="row-odd"><td><p>SegNext-t</p></td>
<td><p>85.05</p></td>
<td><p>70.67</p></td>
<td><p>80.73</p></td>
<td><p>51.25</p></td>
<td><p>68.99</p></td>
</tr>
<tr class="row-even"><td><p>SegNext-s</p></td>
<td><p>85.62</p></td>
<td><p>70.91</p></td>
<td><p>82.31</p></td>
<td><p>52.94</p></td>
<td><p>69.82</p></td>
</tr>
<tr class="row-odd"><td><p>SegNext-b</p></td>
<td><p>87.92</p></td>
<td><p>76.94</p></td>
<td><p>85.01</p></td>
<td><p>55.49</p></td>
<td><p>73.45</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please, refer to our <a class="reference internal" href="../../../tutorials/base/how_to_train/semantic_segmentation.html"><span class="doc">dedicated tutorial</span></a> for more information on how to train, validate and optimize the semantic segmentation model.</p>
</div>
</section>
<section id="semi-supervised-learning">
<h2>Semi-supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this heading">#</a></h2>
<p>We employ the <a class="reference external" href="https://arxiv.org/abs/1703.01780">Mean Teacher framework</a> to tackle the problem of <a class="reference internal" href="../index.html#semi-sl-explanation"><span class="std std-ref">Semi-supervised learning</span></a> in semantic segmentation.
This framework leverages two models during training: a “student” model, which serves as the primary model being trained, and a “teacher” model, which acts as a guiding reference for the student model.</p>
<p>During training, the student model is updated using both ground truth annotations (for labeled data) and pseudo-labels (for unlabeled data).
These pseudo-labels are generated by the teacher model’s predictions. Notably, the teacher model’s parameters are updated based on the moving average of the student model’s parameters.
This means that backward loss propagation is not utilized for updating the teacher model. Once training is complete, only the student model is used for making predictions in the semantic segmentation task.</p>
<p>The Mean Teacher framework utilizes the same core algorithm components as the <a class="reference internal" href="#semantic-segmentation-supervised-pipeline"><span class="std std-ref">supervised pipeline</span></a> for semantic segmentation.
However, there are some key differences in the augmentation pipelines used for labeled and unlabeled data.
Basic augmentations such as random flip, random rotate, and random crop are employed for the teacher model’s input.
On the other hand, stronger augmentations like color distortion, RGB to gray conversion, and <a class="reference external" href="https://arxiv.org/abs/1708.04552">CutOut</a> are applied to the student model.
This discrepancy helps improve generalization and prevents unnecessary overfitting on the pseudo-labels generated by the teacher model.
Additionally, pixels with high entropy, which are deemed unreliable by the teacher model, are filtered out using a schedule that depends on the training iterations.</p>
<p>For new experimental templates (SegNext family) we also adopted the prototype view approach which is based on two research works: <a class="reference external" href="https://arxiv.org/abs/2203.15102">Rethinking Semantic Segmentation: A Prototype View</a> by Tianfei Zhou et al. and <a class="reference external" href="https://arxiv.org/abs/2210.04388">Semi-supervised Semantic Segmentation with Prototype-based Consistency Regularization</a> by Hai-Ming Xu et al.
We implemented a prototype network and incorporated it into the base Mean Teacher framework. We set weights for losses empirically after extensive experiments on the datasets presented below.</p>
<p>The table below presents the <a class="reference external" href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">Dice score</a> achieved by our templates on various datasets.
We provide these scores for comparison purposes, alongside the supervised baseline trained solely on labeled data.
We use 512x512 image resolution, for other hyperparameters, please, refer to the related templates. When training the new SegNext templates, we disabled early stopping and trained them for the full number of epochs. We trained each model with a single Nvidia GeForce RTX3090.
For <a class="reference external" href="https://www.cityscapes-dataset.com/">Cityscapes</a> and <a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">Pascal-VOC</a> we use splits with 1/16 ratio of labeled to unlabeled data like <a class="reference external" href="https://github.com/charlesCXK/TorchSemiSeg">here</a>.
For other datasets, we prepared different numbers of classes and used the random split of the train data to obtain labeled and unlabeled sets.</p>
<ul class="simple">
<li><p><strong>VOC_12</strong>: 2 classes (person, car) were selected, 12 labeled images, 500 unlabeled and 150 images for validation</p></li>
<li><p><strong>KITTI_54</strong>: 3 classes (vehicle, human, construction) were selected, 54 labeled images, 200 unlabeled and 50 images for validation</p></li>
<li><p><strong>City_4</strong>: 4 classes (fence, vegetation, car, truck) were selected, 53 labeled images, 800 unlabeled and 500 images for validation</p></li>
<li><p><strong>DIS5K 1/4</strong>: 1 class (objects), 242 labeled images, 728 unlabeled and 281 images for validation</p></li>
</ul>
<p>Other classes for these datasets are marked as background labels.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>Cityscapes</p></th>
<th class="head"><p>Pascal-VOC</p></th>
<th class="head"><p>DIS5K</p></th>
<th class="head"><p>VOC_12</p></th>
<th class="head"><p>KITTI_54</p></th>
<th class="head"><p>City_4</p></th>
<th class="head"><p>Mean mDice</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Lite-HRNet-s-mod2</p></td>
<td><p>40.80</p></td>
<td><p>43.05</p></td>
<td><p>81.00</p></td>
<td><p>60.12</p></td>
<td><p>61.83</p></td>
<td><p>66.72</p></td>
<td><p>58.92</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-18-mod2</p></td>
<td><p>42.71</p></td>
<td><p>44.42</p></td>
<td><p>81.18</p></td>
<td><p>63.24</p></td>
<td><p>61.4</p></td>
<td><p>67.97</p></td>
<td><p>60.15</p></td>
</tr>
<tr class="row-even"><td><p>Lite-HRNet-x-mod3</p></td>
<td><p>49.20</p></td>
<td><p>43.87</p></td>
<td><p>81.48</p></td>
<td><p>63.96</p></td>
<td><p>59.76</p></td>
<td><p>68.08</p></td>
<td><p>61.06</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-s-mod2-SSL</p></td>
<td><p>45.05</p></td>
<td><p>44.01</p></td>
<td><p>81.46</p></td>
<td><p>64.78</p></td>
<td><p>61.90</p></td>
<td><p>67.03</p></td>
<td><p>60.71</p></td>
</tr>
<tr class="row-even"><td><p>Lite-HRNet-18-mod2-SSL</p></td>
<td><p>48.65</p></td>
<td><p>46.24</p></td>
<td><p>81.52</p></td>
<td><p>65.64</p></td>
<td><p>65.25</p></td>
<td><p>68.11</p></td>
<td><p>62.57</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-x-mod3-SSL</p></td>
<td><p>50.00</p></td>
<td><p>46.10</p></td>
<td><p>82.00</p></td>
<td><p>66.10</p></td>
<td><p>66.50</p></td>
<td><p>68.41</p></td>
<td><p>63.19</p></td>
</tr>
<tr class="row-even"><td><p>SegNext-t</p></td>
<td><p>55.93</p></td>
<td><p>73.82</p></td>
<td><p>86.87</p></td>
<td><p>68.00</p></td>
<td><p>62.35</p></td>
<td><p>68.30</p></td>
<td><p>69.21</p></td>
</tr>
<tr class="row-odd"><td><p>SegNext-s</p></td>
<td><p>63.75</p></td>
<td><p>77.24</p></td>
<td><p>87.88</p></td>
<td><p>76.30</p></td>
<td><p>66.45</p></td>
<td><p>69.34</p></td>
<td><p>73.49</p></td>
</tr>
<tr class="row-even"><td><p>SegNext-b</p></td>
<td><p>66.39</p></td>
<td><p>80.52</p></td>
<td><p>89.62</p></td>
<td><p>78.65</p></td>
<td><p>70.45</p></td>
<td><p>69.68</p></td>
<td><p>75.89</p></td>
</tr>
<tr class="row-odd"><td><p>SegNext-t-SSL</p></td>
<td><p>60.2</p></td>
<td><p>77.44</p></td>
<td><p>87.60</p></td>
<td><p>70.72</p></td>
<td><p>67.43</p></td>
<td><p>69.21</p></td>
<td><p>72.10</p></td>
</tr>
<tr class="row-even"><td><p>SegNext-s-SSL</p></td>
<td><p>68.06</p></td>
<td><p>80.55</p></td>
<td><p>88.72</p></td>
<td><p>77.00</p></td>
<td><p>68.70</p></td>
<td><p>69.73</p></td>
<td><p>75.46</p></td>
</tr>
<tr class="row-odd"><td><p>SegNext-b-SSL</p></td>
<td><p>71.80</p></td>
<td><p>82.43</p></td>
<td><p>90.32</p></td>
<td><p>80.68</p></td>
<td><p>73.73</p></td>
<td><p>70.02</p></td>
<td><p>78.16</p></td>
</tr>
</tbody>
</table>
</section>
<section id="self-supervised-learning">
<h2>Self-supervised Learning<a class="headerlink" href="#self-supervised-learning" title="Permalink to this heading">#</a></h2>
<p id="selfsl-semantic-segmentation">Self-supervised learning can be one of the solutions if the user has a small data set, but label information is not yet available.
General self-supervised Learning in academia is commonly used to obtain well-pretrained weights from a source dataset without label information.
However, in real-world industries, it is difficult to apply because of small datasets, limited resources, or training in minutes.</p>
<p>For these cases, OpenVINO™ Training Extensions provides improved self-supervised learning recipes that can be applied to the above harsh environments.
OpenVINO™ Training Extensions allows to perform a pre-training phase on any images to further use obtained weights on the target dataset.
We adapted <a class="reference external" href="https://arxiv.org/abs/2103.10957">DetCon</a> as our self-supervised method.
It takes some time to use these self-supervised learning recipes, but you can expect improved performance, especially in small-data regimes.</p>
<p>The below table shows how much performance (mDice) self-supervised methods improved compared with baseline performance on the subsets of Pascal VOC 2012 with three classes (person, car, bicycle).
To get the below performance, we had two steps:</p>
<ul class="simple">
<li><p>Train the models using only images containing at less one class of the three classes without label information to get pretrained weights for a few epochs.</p></li>
<li><p>Fine-tune the models with pretrained weights using subset datasets and get performance.</p></li>
</ul>
<p>We additionally obtained baseline performance from supervised learning using subset datasets for comparison.
Each subset dataset has 8, 16, and 24 images, respectively.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>#8</p></th>
<th class="head"></th>
<th class="head"><p>#16</p></th>
<th class="head"></th>
<th class="head"><p>#24</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>Self-SL</p></td>
<td><p>SL</p></td>
<td><p>Self-SL</p></td>
<td><p>SL</p></td>
<td><p>Self-SL</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-s-mod2</p></td>
<td><p>48.30</p></td>
<td><p>53.55</p></td>
<td><p>57.08</p></td>
<td><p>58.96</p></td>
<td><p>62.40</p></td>
<td><p>63.46</p></td>
</tr>
<tr class="row-even"><td><p>Lite-HRNet-18-mod2</p></td>
<td><p>53.47</p></td>
<td><p>49.20</p></td>
<td><p>56.69</p></td>
<td><p>58.72</p></td>
<td><p>62.81</p></td>
<td><p>63.63</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-x-mod3</p></td>
<td><p>50.23</p></td>
<td><p>50.93</p></td>
<td><p>60.09</p></td>
<td><p>61.61</p></td>
<td><p>62.66</p></td>
<td><p>64.87</p></td>
</tr>
</tbody>
</table>
<p>Unlike other tasks, two things are considered to use self-supervised learning:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--train-data-roots</span></code> must be set to a directory only containing images, not ground truths.
DetCon uses pseudo masks created in <code class="docutils literal notranslate"><span class="pre">detcon_mask</span></code> directory for training. If they are not created yet, they will be created first.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--val-data-roots</span></code> is not needed.</p></li>
</ul>
<p>To enable self-supervised training, the command below can be executed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train Lite-HRNet-18-mod2 \
            --train-data-roots path/to/images \
</pre></div>
</div>
<p>After self-supervised training, pretrained weights can be use for supervised (incremental) learning like the below command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train Lite-HRNet-18-mod2 \
            --train-data-roots path/to/train/subset \
            --val-data-roots path/to/validation/subset \
            --load-weights={PATH/PRETRAINED/WEIGHTS}
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SL stands for Supervised Learning.</p>
</div>
</section>
<section id="supervised-contrastive-learning">
<h2>Supervised Contrastive Learning<a class="headerlink" href="#supervised-contrastive-learning" title="Permalink to this heading">#</a></h2>
<p>To enhance the performance of the algorithm in case when we have a small number of data, <a class="reference external" href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning (SupCon)</a> can be used.</p>
<p>More specifically, we train a model with two heads: segmentation head with Cross Entropy Loss and contrastive head with <a class="reference external" href="https://arxiv.org/abs/2103.10957">DetCon loss</a>.
As of using this advanced approach, we can expect improved performance and reduced training time rather than supervised learning.
The below table shows how much performance (mDice) SupCon improved compared with baseline performance on the subsets of Pascal VOC 2012 with three classes (person, car, bicycle).
Each subset dataset has 8, 16, and 24 images, respectively.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>#8</p></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"><p>#16</p></th>
<th class="head"></th>
<th class="head"></th>
<th class="head" colspan="3"><p>#24   |</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>SupCon</p></td>
<td><p>TR</p></td>
<td><p>SL</p></td>
<td><p>SupCon</p></td>
<td><p>TR</p></td>
<td colspan="3"><p>SL    | SupCon</p></td>
<td><p>TR</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-s-mod2</p></td>
<td><p>52.30</p></td>
<td><p>54.24</p></td>
<td><p>0.83x</p></td>
<td><p>59.58</p></td>
<td><p>61.44</p></td>
<td><p>0.93x</p></td>
<td><p>62.86</p></td>
<td colspan="2"><p>64.30</p></td>
<td><p>1.03x</p></td>
</tr>
<tr class="row-even"><td><p>Lite-HRNet-18-mod2</p></td>
<td><p>53.00</p></td>
<td><p>56.16</p></td>
<td><p>0.71x</p></td>
<td><p>61.44</p></td>
<td><p>60.08</p></td>
<td><p>0.91x</p></td>
<td><p>64.26</p></td>
<td colspan="2"><p>64.82</p></td>
<td><p>0.91x</p></td>
</tr>
<tr class="row-odd"><td><p>Lite-HRNet-x-mod3</p></td>
<td><p>53.71</p></td>
<td><p>58.67</p></td>
<td><p>0.83x</p></td>
<td><p>58.43</p></td>
<td><p>61.52</p></td>
<td><p>0.73x</p></td>
<td><p>64.72</p></td>
<td colspan="2"><p>65.83</p></td>
<td><p>0.73x</p></td>
</tr>
</tbody>
</table>
<p>The SupCon training can be launched by adding additional option to template parameters like the below.
It can be launched only with supervised (incremental) training type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train Lite-HRNet-18-mod2 \
            --train-data-roots path/to/train/subset \
            --val-data-roots path/to/validation/subset \
            params \
            --learning_parameters.enable_supcon=True
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SL : Supervised Learning / TR : Training Time Ratio of SupCon compared with supervised learning</p>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Segmentation</p>
      </div>
    </a>
    <a class="right-next"
       href="instance_segmentation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Instance Segmentation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-format">Dataset Format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models">Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semi-supervised-learning">Semi-supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-supervised-learning">Self-supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-contrastive-learning">Supervised Contrastive Learning</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../../../../_sources/guide/explanation/algorithms/segmentation/semantic_segmentation.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2023, OpenVINO™ Training Extensions Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.2.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>