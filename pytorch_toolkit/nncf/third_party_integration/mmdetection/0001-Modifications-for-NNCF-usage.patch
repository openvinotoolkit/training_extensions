From a715090be60100dc2557a00a0a87e739da238e4e Mon Sep 17 00:00:00 2001
From: Ivan Lazarevich <ivan.lazarevich@intel.com>
Date: Tue, 10 Dec 2019 20:14:52 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 configs/pascal_voc/ssd300_voc_int8.py           | 141 ++++++++++++++++++++++++
 configs/retinanet_r50_fpn_1x_int8.py            | 136 +++++++++++++++++++++++
 mmdet/apis/train.py                             |  65 +++++++++--
 mmdet/core/__init__.py                          |   1 +
 mmdet/core/nncf/__init__.py                     |   7 ++
 mmdet/core/nncf/hooks.py                        |  17 +++
 mmdet/core/nncf/utils.py                        |  92 ++++++++++++++++
 mmdet/models/anchor_heads/anchor_head.py        |   2 +
 mmdet/models/anchor_heads/fcos_head.py          |   2 +
 mmdet/models/anchor_heads/ga_rpn_head.py        |   2 +
 mmdet/models/anchor_heads/guided_anchor_head.py |   2 +
 mmdet/models/anchor_heads/rpn_head.py           |   2 +
 mmdet/models/anchor_heads/ssd_head.py           |   2 +
 mmdet/models/detectors/base.py                  |   7 ++
 mmdet/models/detectors/single_stage.py          |   4 +
 tools/train.py                                  |  10 +-
 16 files changed, 483 insertions(+), 9 deletions(-)
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..f87e4f0
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,141 @@
+# model settings
+input_size = 300
+model = dict(
+    type='SingleStageDetector',
+    pretrained='open-mmlab://vgg16_caffe',
+    backbone=dict(
+        type='SSDVGG',
+        input_size=input_size,
+        depth=16,
+        with_last_pool=False,
+        ceil_mode=True,
+        out_indices=(3, 4),
+        out_feature_indices=(22, 34),
+        l2_norm_scale=20),
+    neck=None,
+    bbox_head=dict(
+        type='SSDHead',
+        input_size=input_size,
+        in_channels=(512, 1024, 512, 256, 256, 256),
+        num_classes=21,
+        anchor_strides=(8, 16, 32, 64, 100, 300),
+        basesize_ratio_range=(0.2, 0.9),
+        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),
+        target_means=(.0, .0, .0, .0),
+        target_stds=(0.1, 0.1, 0.2, 0.2)))
+cudnn_benchmark = True
+train_cfg = dict(
+    assigner=dict(
+        type='MaxIoUAssigner',
+        pos_iou_thr=0.5,
+        neg_iou_thr=0.5,
+        min_pos_iou=0.,
+        ignore_iof_thr=-1,
+        gt_max_assign_all=False),
+    smoothl1_beta=1.,
+    allowed_border=-1,
+    pos_weight=-1,
+    neg_pos_ratio=3,
+    debug=False)
+test_cfg = dict(
+    nms=dict(type='nms', iou_thr=0.45),
+    min_bbox_size=0,
+    score_thr=0.02,
+    max_per_img=200)
+# model training and testing settings
+# dataset settings
+dataset_type = 'VOCDataset'
+data_root = 'data/VOCdevkit/'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+data = dict(
+    imgs_per_gpu=4,
+    workers_per_gpu=2,
+    train=dict(
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=[
+                data_root + 'VOC2007/ImageSets/Main/trainval.txt',
+                data_root + 'VOC2012/ImageSets/Main/trainval.txt'
+            ],
+            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],
+            img_scale=(300, 300),
+            img_norm_cfg=img_norm_cfg,
+            size_divisor=None,
+            flip_ratio=0.5,
+            with_mask=False,
+            with_crowd=False,
+            with_label=True,
+            test_mode=False,
+            extra_aug=dict(
+                photo_metric_distortion=dict(
+                    brightness_delta=32,
+                    contrast_range=(0.5, 1.5),
+                    saturation_range=(0.5, 1.5),
+                    hue_delta=18),
+                expand=dict(
+                    mean=img_norm_cfg['mean'],
+                    to_rgb=img_norm_cfg['to_rgb'],
+                    ratio_range=(1, 4)),
+                random_crop=dict(
+                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),
+            resize_keep_ratio=False)),
+    val=dict(
+        type=dataset_type,
+        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
+        img_prefix=data_root + 'VOC2007/',
+        img_scale=(300, 300),
+        img_norm_cfg=img_norm_cfg,
+        size_divisor=None,
+        flip_ratio=0,
+        with_mask=False,
+        with_label=False,
+        test_mode=True,
+        resize_keep_ratio=False),
+    test=dict(
+        type=dataset_type,
+        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
+        img_prefix=data_root + 'VOC2007/',
+        img_scale=(300, 300),
+        img_norm_cfg=img_norm_cfg,
+        size_divisor=None,
+        flip_ratio=0,
+        with_mask=False,
+        with_label=False,
+        test_mode=True,
+        resize_keep_ratio=False))
+# optimizer
+optimizer = dict(type='SGD', lr=1e-3, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 3,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 24
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc'
+load_from = None
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+ignored_scopes = ["SingleStageDetector/SSDHead[bbox_head]/ModuleList[reg_convs]",
+                  "SingleStageDetector/SSDHead[bbox_head]/ModuleList[cls_convs]"]
+nncf_config = dict(compression=[dict(algorithm="quantization",
+                                     ignored_scopes=ignored_scopes)],
+                   log_dir=work_dir,
+                   input_sample_size=(1, 3, input_size, input_size))
diff --git a/configs/retinanet_r50_fpn_1x_int8.py b/configs/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..5be1188
--- /dev/null
+++ b/configs/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,136 @@
+# model settings
+model = dict(
+    type='RetinaNet',
+    pretrained='torchvision://resnet50',
+    backbone=dict(
+        type='ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        style='pytorch'),
+    neck=dict(
+        type='FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        start_level=1,
+        add_extra_convs=True,
+        num_outs=5),
+    bbox_head=dict(
+        type='RetinaHead',
+        num_classes=81,
+        in_channels=256,
+        stacked_convs=4,
+        feat_channels=256,
+        octave_base_scale=4,
+        scales_per_octave=3,
+        anchor_ratios=[0.5, 1.0, 2.0],
+        anchor_strides=[8, 16, 32, 64, 128],
+        target_means=[.0, .0, .0, .0],
+        target_stds=[1.0, 1.0, 1.0, 1.0],
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))
+# training and testing settings
+train_cfg = dict(
+    assigner=dict(
+        type='MaxIoUAssigner',
+        pos_iou_thr=0.5,
+        neg_iou_thr=0.4,
+        min_pos_iou=0,
+        ignore_iof_thr=-1),
+    allowed_border=-1,
+    pos_weight=-1,
+    debug=False)
+test_cfg = dict(
+    nms_pre=1000,
+    min_bbox_size=0,
+    score_thr=0.05,
+    nms=dict(type='nms', iou_thr=0.5),
+    max_per_img=100)
+# dataset settings
+dataset_type = 'CocoDataset'
+data_root = '/data/coco_mmdet/'
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+data = dict(
+    imgs_per_gpu=1,
+    workers_per_gpu=2,
+    train=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_train2017.json',
+        img_prefix=data_root + 'train2017/',
+        img_scale=(1333, 800),
+        img_norm_cfg=img_norm_cfg,
+        size_divisor=32,
+        flip_ratio=0.5,
+        with_mask=False,
+        with_crowd=False,
+        with_label=True),
+    val=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        img_scale=(1333, 800),
+        img_norm_cfg=img_norm_cfg,
+        size_divisor=32,
+        flip_ratio=0,
+        with_mask=False,
+        with_crowd=False,
+        with_label=True),
+    test=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        img_scale=(1333, 800),
+        img_norm_cfg=img_norm_cfg,
+        size_divisor=32,
+        flip_ratio=0,
+        with_mask=False,
+        with_crowd=False,
+        with_label=False,
+        test_mode=True))
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 3,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 12
+device_ids = range(8)
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x'
+load_from = None
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+ignored_scopes = ["RetinaNet/RetinaHead[bbox_head]/Conv2d[retina_reg]",
+                  "RetinaNet/RetinaHead[bbox_head]/Conv2d[retina_cls]", ]
+sparsity_params = dict(schedule="multistep",
+                       sparsity_levels=[0.5, 0.7],
+                       steps=[15])
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm="quantization",
+                                     initializer=dict(num_init_steps=10),
+                                     ignored_scopes=ignored_scopes)],
+                   log_dir=work_dir,
+                   input_sample_size=(1, 3, input_size, input_size))
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index 35ad34c..0e707cc 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -8,9 +8,16 @@ from mmcv.runner import DistSamplerSeedHook, Runner, obj_from_dict
 
 from mmdet import datasets
 from mmdet.core import (CocoDistEvalmAPHook, CocoDistEvalRecallHook,
-                        DistEvalmAPHook, DistOptimizerHook, Fp16OptimizerHook)
+                        DistEvalmAPHook, DistOptimizerHook,
+                        Fp16OptimizerHook, CompressionHook)
 from mmdet.datasets import DATASETS, build_dataloader
 from mmdet.models import RPN
+
+from nncf.utils import get_all_modules
+from mmdet.core.nncf import load_checkpoint
+from mmdet.core.nncf import wrap_nncf_model
+from mmdet.core.nncf import nncf_algo_initialize
+
 from .env import get_root_logger
 
 
@@ -34,10 +41,14 @@ def parse_losses(losses):
     return loss, log_vars
 
 
-def batch_processor(model, data, train_mode):
+def batch_processor(model, data, compression_algo, train_mode):
     losses = model(**data)
     loss, log_vars = parse_losses(losses)
 
+    if compression_algo is not None:
+        compression_loss = compression_algo.loss()
+        loss += compression_loss
+
     outputs = dict(
         loss=loss, log_vars=log_vars, num_samples=len(data['img'].data))
 
@@ -142,9 +153,16 @@ def _dist_train(model, dataset, cfg, validate=False):
             ds, cfg.data.imgs_per_gpu, cfg.data.workers_per_gpu, dist=True)
         for ds in dataset
     ]
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_algo = wrap_nncf_model(model, cfg)
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_algo = None
     # put model on gpus
     model = MMDistributedDataParallel(model.cuda())
-
+    if cfg.ENABLE_COMPRESSION:
+        compression_algo.distributed()
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = Runner(model, batch_processor, optimizer, cfg.work_dir,
@@ -162,6 +180,10 @@ def _dist_train(model, dataset, cfg, validate=False):
     runner.register_training_hooks(cfg.lr_config, optimizer_config,
                                    cfg.checkpoint_config, cfg.log_config)
     runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        nncf_hook = runner.build_hook({'compression_algo': compression_algo},
+                                          CompressionHook)
+        runner.register_hook(nncf_hook)
     # register eval hooks
     if validate:
         val_dataset_cfg = cfg.data.val
@@ -182,8 +204,17 @@ def _dist_train(model, dataset, cfg, validate=False):
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
     elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+        try:
+            load_checkpoint(model=runner.model,
+                            filename=cfg.load_from)
+        except:
+            runner.load_checkpoint(cfg.load_from)
+
+    if compression_algo is not None:
+        nncf_algo_initialize(compression_algo, data_loaders)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_algo=compression_algo)
 
 
 def _non_dist_train(model, dataset, cfg, validate=False):
@@ -197,6 +228,12 @@ def _non_dist_train(model, dataset, cfg, validate=False):
             cfg.gpus,
             dist=False) for ds in dataset
     ]
+    # nncf model wrapping
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_algo = wrap_nncf_model(model, cfg)
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_algo = None
     # put model on gpus
     model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()
 
@@ -213,9 +250,21 @@ def _non_dist_train(model, dataset, cfg, validate=False):
         optimizer_config = cfg.optimizer_config
     runner.register_training_hooks(cfg.lr_config, optimizer_config,
                                    cfg.checkpoint_config, cfg.log_config)
-
+    if cfg.ENABLE_COMPRESSION:
+        nncf_hook = runner.build_hook({'compression_algo': compression_algo},
+                                      CompressionHook)
+        runner.register_hook(nncf_hook)
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
     elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+        try:
+            load_checkpoint(model=runner.model,
+                            filename=cfg.load_from)
+        except:
+            runner.load_checkpoint(cfg.load_from)
+
+    if compression_algo is not None:
+        nncf_algo_initialize(compression_algo, data_loaders)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_algo=compression_algo)
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index f8eb6cb..14f2c89 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -5,3 +5,4 @@ from .fp16 import *  # noqa: F401, F403
 from .mask import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..cd304c1
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,7 @@
+from .hooks import CompressionHook
+from .utils import get_all_modules, wrap_nncf_model
+from .utils import load_checkpoint, nncf_algo_initialize
+
+__all__ = ['CompressionHook', 'get_all_modules',
+           'wrap_nncf_model', 'load_checkpoint',
+           'nncf_algo_initialize']
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..b5b6c5d
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,17 @@
+from mmcv.runner.hooks.hook import Hook
+from nncf.utils import print_statistics as print_nncf_statistics
+
+
+class CompressionHook(Hook):
+
+    def __init__(self, compression_algo=None):
+        self.compression_algo = compression_algo
+
+    def after_train_iter(self, runner):
+        self.compression_algo.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_algo.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(print_nncf_statistics(self.compression_algo.statistics()))
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..b1e400e
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,92 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+
+from nncf.utils import get_all_modules
+from nncf.config import Config as NNCFConfig
+from nncf.helpers import load_state, create_compressed_model
+
+
+def wrap_nncf_model(model, cfg):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig(cfg.nncf_config)
+    input_size = nncf_config.get("input_sample_size",
+                                 (1, 3, cfg.input_size, cfg.input_size))
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([torch.randn(input_size).to(device), ],)
+        input_kwargs = dict(img_meta=[],
+                            return_loss=False,
+                            dummy_forward=True)
+        model.forward(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+
+    compression_algo, model = create_compressed_model(model, nncf_config,
+                                                      dummy_forward_fn=dummy_forward)
+    print(*get_all_modules(model).keys(), sep='\n')
+
+    return model, compression_algo
+
+
+def load_checkpoint(model,
+                    filename,
+                    map_location=None,
+                    strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError(
+            'No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader:
+
+    def __init__(self, data_loader, kwargs, device):
+        self.data_loader = data_loader
+        self.kwargs = kwargs
+        self.device = device
+        self.num_workers = data_loader.num_workers
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        # redefined InitializeDataLoader because
+        # of DataContainer format in mmdet
+        input = self.data_loader.__iter__().__next__()
+        input_tensor = input['img']._data[0].to(self.device)
+        return ([input_tensor, ], self.kwargs)
+
+
+def nncf_algo_initialize(compression_algo, data_loaders):
+    mmdet_kwargs = dict(img_meta=[],
+                        return_loss=False,
+                        dummy_forward=True)
+    device = next(compression_algo.model.parameters()).device
+    wrapped_loader = MMInitializeDataLoader(data_loader=data_loaders[0],
+                                            kwargs=mmdet_kwargs,
+                                            device=device)
+
+    compression_algo.initialize(wrapped_loader)
diff --git a/mmdet/models/anchor_heads/anchor_head.py b/mmdet/models/anchor_heads/anchor_head.py
index b3fb5b4..aac992d 100644
--- a/mmdet/models/anchor_heads/anchor_head.py
+++ b/mmdet/models/anchor_heads/anchor_head.py
@@ -4,6 +4,7 @@ import numpy as np
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import (AnchorGenerator, anchor_target, delta2bbox, force_fp32,
                         multi_apply, multiclass_nms)
@@ -150,6 +151,7 @@ class AnchorHead(nn.Module):
             avg_factor=num_total_samples)
         return loss_cls, loss_bbox
 
+    @no_nncf_trace()
     @force_fp32(apply_to=('cls_scores', 'bbox_preds'))
     def loss(self,
              cls_scores,
diff --git a/mmdet/models/anchor_heads/fcos_head.py b/mmdet/models/anchor_heads/fcos_head.py
index c01e4ea..65b633d 100644
--- a/mmdet/models/anchor_heads/fcos_head.py
+++ b/mmdet/models/anchor_heads/fcos_head.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import distance2bbox, force_fp32, multi_apply, multiclass_nms
 from ..builder import build_loss
@@ -113,6 +114,7 @@ class FCOSHead(nn.Module):
         bbox_pred = scale(self.fcos_reg(reg_feat)).float().exp()
         return cls_score, bbox_pred, centerness
 
+    @no_nncf_trace()
     @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses'))
     def loss(self,
              cls_scores,
diff --git a/mmdet/models/anchor_heads/ga_rpn_head.py b/mmdet/models/anchor_heads/ga_rpn_head.py
index 11512ff..1a9f550 100644
--- a/mmdet/models/anchor_heads/ga_rpn_head.py
+++ b/mmdet/models/anchor_heads/ga_rpn_head.py
@@ -2,6 +2,7 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import delta2bbox
 from mmdet.ops import nms
@@ -32,6 +33,7 @@ class GARPNHead(GuidedAnchorHead):
          loc_pred) = super(GARPNHead, self).forward_single(x)
         return cls_score, bbox_pred, shape_pred, loc_pred
 
+    @no_nncf_trace()
     def loss(self,
              cls_scores,
              bbox_preds,
diff --git a/mmdet/models/anchor_heads/guided_anchor_head.py b/mmdet/models/anchor_heads/guided_anchor_head.py
index d2d71d0..4e7c328 100644
--- a/mmdet/models/anchor_heads/guided_anchor_head.py
+++ b/mmdet/models/anchor_heads/guided_anchor_head.py
@@ -4,6 +4,7 @@ import numpy as np
 import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import (AnchorGenerator, anchor_inside_flags, anchor_target,
                         delta2bbox, force_fp32, ga_loc_target, ga_shape_target,
@@ -389,6 +390,7 @@ class GuidedAnchorHead(AnchorHead):
             avg_factor=loc_avg_factor)
         return loss_loc
 
+    @no_nncf_trace()
     @force_fp32(
         apply_to=('cls_scores', 'bbox_preds', 'shape_preds', 'loc_preds'))
     def loss(self,
diff --git a/mmdet/models/anchor_heads/rpn_head.py b/mmdet/models/anchor_heads/rpn_head.py
index 50f1cc5..17e35e4 100644
--- a/mmdet/models/anchor_heads/rpn_head.py
+++ b/mmdet/models/anchor_heads/rpn_head.py
@@ -2,6 +2,7 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import normal_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import delta2bbox
 from mmdet.ops import nms
@@ -34,6 +35,7 @@ class RPNHead(AnchorHead):
         rpn_bbox_pred = self.rpn_reg(x)
         return rpn_cls_score, rpn_bbox_pred
 
+    @no_nncf_trace()
     def loss(self,
              cls_scores,
              bbox_preds,
diff --git a/mmdet/models/anchor_heads/ssd_head.py b/mmdet/models/anchor_heads/ssd_head.py
index 600dd4a..c0630ac 100644
--- a/mmdet/models/anchor_heads/ssd_head.py
+++ b/mmdet/models/anchor_heads/ssd_head.py
@@ -3,6 +3,7 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmcv.cnn import xavier_init
+from nncf.dynamic_graph.context import no_nncf_trace
 
 from mmdet.core import AnchorGenerator, anchor_target, multi_apply
 from ..losses import smooth_l1_loss
@@ -132,6 +133,7 @@ class SSDHead(AnchorHead):
             avg_factor=num_total_samples)
         return loss_cls[None], loss_bbox
 
+    @no_nncf_trace()
     def loss(self,
              cls_scores,
              bbox_preds,
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index 038dd10..286ffb6 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -48,6 +48,10 @@ class BaseDetector(nn.Module):
         pass
 
     @abstractmethod
+    def dummy_forward(self, img, **kwargs):
+        pass
+
+    @abstractmethod
     def simple_test(self, img, img_meta, **kwargs):
         pass
 
@@ -66,6 +70,9 @@ class BaseDetector(nn.Module):
                 raise TypeError('{} must be a list, but got {}'.format(
                     name, type(var)))
 
+        if kwargs['dummy_forward']:
+            return self.dummy_forward(imgs[0], **kwargs)
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(
diff --git a/mmdet/models/detectors/single_stage.py b/mmdet/models/detectors/single_stage.py
index 9587392..331644f 100644
--- a/mmdet/models/detectors/single_stage.py
+++ b/mmdet/models/detectors/single_stage.py
@@ -71,5 +71,9 @@ class SingleStageDetector(BaseDetector):
         ]
         return bbox_results[0]
 
+    def dummy_forward(self, img, **kwargs):
+        x = self.extract_feat(img)
+        outs = self.bbox_head(x)
+
     def aug_test(self, imgs, img_metas, rescale=False):
         raise NotImplementedError
diff --git a/tools/train.py b/tools/train.py
index 0854163..0f0086c 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -3,6 +3,10 @@ import argparse
 import os
 
 import torch
+
+from nncf.dynamic_graph import patch_torch_operators
+patch_torch_operators()
+
 from mmcv import Config
 
 from mmdet import __version__
@@ -74,7 +78,11 @@ def main():
     # init logger before other steps
     logger = get_root_logger(cfg.log_level)
     logger.info('Distributed training: {}'.format(distributed))
-
+    if "nncf_config" in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
     # set random seeds
     if args.seed is not None:
         logger.info('Set random seed to {}'.format(args.seed))
-- 
2.7.4

