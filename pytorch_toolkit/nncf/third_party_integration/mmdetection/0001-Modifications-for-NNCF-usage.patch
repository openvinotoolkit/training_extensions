From 31f04a0902ca066b6e340ead50f9fd8e48dc23d6 Mon Sep 17 00:00:00 2001
From: Ivan Lazarevich <ivan.lazarevich@intel.com>
Date: Tue, 28 Apr 2020 17:47:34 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 configs/pascal_voc/ssd300_voc_int8.py | 146 ++++++++++++++++++++++++++++++++++
 configs/retinanet_r50_fpn_1x_int8.py  | 139 ++++++++++++++++++++++++++++++++
 mmdet/apis/train.py                   |  66 +++++++++++++--
 mmdet/core/__init__.py                |   1 +
 mmdet/core/nncf/__init__.py           |  11 +++
 mmdet/core/nncf/hooks.py              |  25 ++++++
 mmdet/core/nncf/utils.py              |  83 +++++++++++++++++++
 mmdet/models/detectors/base.py        |   9 ++-
 tools/train.py                        |   7 ++
 9 files changed, 480 insertions(+), 7 deletions(-)
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..08a0827
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,146 @@
+# model settings
+input_size = 300
+model = dict(
+    type='SingleStageDetector',
+    pretrained='open-mmlab://vgg16_caffe',
+    # pretrained='torchvision://vgg11',
+    backbone=dict(
+        type='SSDVGG',
+        input_size=input_size,
+        depth=16,
+        with_last_pool=False,
+        ceil_mode=True,
+        out_indices=(3, 4),
+        out_feature_indices=(22, 34),
+        l2_norm_scale=20),
+    neck=None,
+    bbox_head=dict(
+        type='SSDHead',
+        input_size=input_size,
+        in_channels=(512, 1024, 512, 256, 256, 256),
+        num_classes=21,
+        anchor_strides=(8, 16, 32, 64, 100, 300),
+        basesize_ratio_range=(0.2, 0.9),
+        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),
+        target_means=(.0, .0, .0, .0),
+        target_stds=(0.1, 0.1, 0.2, 0.2)))
+# model training and testing settings
+cudnn_benchmark = True
+train_cfg = dict(
+    assigner=dict(
+        type='MaxIoUAssigner',
+        pos_iou_thr=0.5,
+        neg_iou_thr=0.5,
+        min_pos_iou=0.,
+        ignore_iof_thr=-1,
+        gt_max_assign_all=False),
+    smoothl1_beta=1.,
+    allowed_border=-1,
+    pos_weight=-1,
+    neg_pos_ratio=3,
+    debug=False)
+test_cfg = dict(
+    nms=dict(type='nms', iou_thr=0.45),
+    min_bbox_size=0,
+    score_thr=0.02,
+    max_per_img=200)
+# dataset settings
+dataset_type = 'VOCDataset'
+data_root = 'data/VOCdevkit/'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile', to_float32=True),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(
+        type='PhotoMetricDistortion',
+        brightness_delta=32,
+        contrast_range=(0.5, 1.5),
+        saturation_range=(0.5, 1.5),
+        hue_delta=18),
+    dict(
+        type='Expand',
+        mean=img_norm_cfg['mean'],
+        to_rgb=img_norm_cfg['to_rgb'],
+        ratio_range=(1, 4)),
+    dict(
+        type='MinIoURandomCrop',
+        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
+        min_crop_size=0.3),
+    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(300, 300),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=False),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    imgs_per_gpu=4,
+    workers_per_gpu=4,
+    train=dict(
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=[
+                data_root + 'VOC2007/ImageSets/Main/trainval.txt',
+                data_root + 'VOC2012/ImageSets/Main/trainval.txt'
+            ],
+            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],
+            pipeline=train_pipeline)),
+    val=dict(
+        type=dataset_type,
+        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
+        img_prefix=data_root + 'VOC2007/',
+        pipeline=test_pipeline),
+    test=dict(
+        type=dataset_type,
+        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
+        img_prefix=data_root + 'VOC2007/',
+        pipeline=test_pipeline))
+evaluation = dict(interval=1, metric='mAP')
+# optimizer
+optimizer = dict(type='SGD', lr=1e-4, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 5
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth
+load_from = './checkpoints/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10,
+                                                                 type='threesigma')))],
+                   log_dir=work_dir)
diff --git a/configs/retinanet_r50_fpn_1x_int8.py b/configs/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..6d28bd4
--- /dev/null
+++ b/configs/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,139 @@
+# model settings
+model = dict(
+    type='RetinaNet',
+    pretrained='torchvision://resnet50',
+    backbone=dict(
+        type='ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        norm_cfg=dict(type='BN', requires_grad=True),
+        style='pytorch'),
+    neck=dict(
+        type='FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        start_level=1,
+        add_extra_convs=True,
+        num_outs=5),
+    bbox_head=dict(
+        type='RetinaHead',
+        num_classes=81,
+        in_channels=256,
+        stacked_convs=4,
+        feat_channels=256,
+        octave_base_scale=4,
+        scales_per_octave=3,
+        anchor_ratios=[0.5, 1.0, 2.0],
+        anchor_strides=[8, 16, 32, 64, 128],
+        target_means=[.0, .0, .0, .0],
+        target_stds=[1.0, 1.0, 1.0, 1.0],
+        loss_cls=dict(
+            type='FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))
+# training and testing settings
+train_cfg = dict(
+    assigner=dict(
+        type='MaxIoUAssigner',
+        pos_iou_thr=0.5,
+        neg_iou_thr=0.4,
+        min_pos_iou=0,
+        ignore_iof_thr=-1),
+    allowed_border=-1,
+    pos_weight=-1,
+    debug=False)
+test_cfg = dict(
+    nms_pre=1000,
+    min_bbox_size=0,
+    score_thr=0.05,
+    nms=dict(type='nms', iou_thr=0.5),
+    max_per_img=100)
+# dataset settings
+dataset_type = 'CocoDataset'
+data_root = 'data/coco/'
+img_norm_cfg = dict(
+    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    imgs_per_gpu=2,
+    workers_per_gpu=2,
+    train=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_train2017.json',
+        img_prefix=data_root + 'train2017/',
+        pipeline=train_pipeline),
+    val=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        pipeline=test_pipeline),
+    test=dict(
+        type=dataset_type,
+        ann_file=data_root + 'annotations/instances_val2017.json',
+        img_prefix=data_root + 'val2017/',
+        pipeline=test_pipeline))
+evaluation = dict(interval=1, metric='bbox')
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 3
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/models/retinanet_r50_fpn_2x_20190616-75574209.pth
+load_from = './checkpoints/retinanet_r50_fpn_2x_20190616-75574209.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+input_size = 800
+nncf_config = dict(compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_steps=10,
+                                                                 type='threesigma')))],
+                   log_dir=work_dir)
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index 8359f30..e244feb 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -8,10 +8,15 @@ from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import DistSamplerSeedHook, Runner
 
 from mmdet.core import (DistEvalHook, DistOptimizerHook, EvalHook,
-                        Fp16OptimizerHook, build_optimizer)
+                        Fp16OptimizerHook, CompressionHook, build_optimizer)
 from mmdet.datasets import build_dataloader, build_dataset
 from mmdet.utils import get_root_logger
 
+from nncf.utils import get_all_modules
+from mmdet.core.nncf import load_checkpoint
+from mmdet.core.nncf import wrap_nncf_model
+from mmdet.core.nncf import nncf_algo_initialize
+
 
 def set_random_seed(seed, deterministic=False):
     """Set random seed.
@@ -56,7 +61,7 @@ def parse_losses(losses):
     return loss, log_vars
 
 
-def batch_processor(model, data, train_mode):
+def batch_processor(model, data, compression_ctrl, train_mode):
     """Process a data batch.
 
     This method is required as an argument of Runner, which defines how to
@@ -75,6 +80,10 @@ def batch_processor(model, data, train_mode):
     losses = model(**data)
     loss, log_vars = parse_losses(losses)
 
+    if compression_ctrl is not None:
+        compression_loss = compression_ctrl.loss()
+        loss += compression_loss
+
     outputs = dict(
         loss=loss, log_vars=log_vars, num_samples=len(data['img'].data))
 
@@ -128,6 +137,14 @@ def _dist_train(model,
             dist=True,
             seed=cfg.seed) for ds in dataset
     ]
+
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg)
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     # put model on gpus
     find_unused_parameters = cfg.get('find_unused_parameters', False)
     # Sets the `find_unused_parameters` parameter in
@@ -138,6 +155,9 @@ def _dist_train(model,
         broadcast_buffers=False,
         find_unused_parameters=find_unused_parameters)
 
+    if cfg.ENABLE_COMPRESSION:
+        compression_ctrl.distributed()
+
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = Runner(
@@ -162,6 +182,9 @@ def _dist_train(model,
     runner.register_training_hooks(cfg.lr_config, optimizer_config,
                                    cfg.checkpoint_config, cfg.log_config)
     runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     # register eval hooks
     if validate:
         val_dataset = build_dataset(cfg.data.val, dict(test_mode=True))
@@ -177,8 +200,18 @@ def _dist_train(model,
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
     elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+        try:
+            load_checkpoint(model=runner.model,
+                            filename=cfg.load_from)
+        except:
+            runner.load_checkpoint(cfg.load_from)
+
+    if compression_ctrl is not None:
+        nncf_algo_initialize(compression_ctrl, data_loaders,
+                             device=next(model.parameters()).device)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_ctrl=compression_ctrl)
 
 
 def _non_dist_train(model,
@@ -199,6 +232,14 @@ def _non_dist_train(model,
             dist=False,
             seed=cfg.seed) for ds in dataset
     ]
+
+    # nncf model wrapping
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg)
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     # put model on gpus
     model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()
 
@@ -235,8 +276,21 @@ def _non_dist_train(model,
         eval_cfg = cfg.get('evaluation', {})
         runner.register_hook(EvalHook(val_dataloader, **eval_cfg))
 
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
     elif cfg.load_from:
-        runner.load_checkpoint(cfg.load_from)
-    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
+        try:
+            load_checkpoint(model=runner.model,
+                            filename=cfg.load_from)
+        except:
+            runner.load_checkpoint(cfg.load_from)
+
+    if compression_ctrl is not None:
+        nncf_algo_initialize(compression_ctrl, data_loaders,
+                             device=next(model.parameters()).device)
+
+    runner.run(data_loaders, cfg.workflow, cfg.total_epochs,
+               compression_ctrl=compression_ctrl)
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index 675c7b9..6efa709 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -6,3 +6,4 @@ from .mask import *  # noqa: F401, F403
 from .optimizer import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..11ecb87
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,11 @@
+from .hooks import CompressionHook
+from .utils import get_all_modules, wrap_nncf_model
+from .utils import load_checkpoint, nncf_algo_initialize
+
+__all__ = [
+    'CompressionHook',
+    'get_all_modules',
+    'wrap_nncf_model',
+    'load_checkpoint',
+    'nncf_algo_initialize',
+]
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..60d9a82
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,25 @@
+from texttable import Texttable
+from mmcv.runner.hooks.hook import Hook
+
+
+class CompressionHook(Hook):
+    def __init__(self, compression_ctrl=None):
+        self.compression_ctrl = compression_ctrl
+
+    def after_train_iter(self, runner):
+        self.compression_ctrl.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_ctrl.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(print_statistics(self.compression_ctrl.statistics()))
+
+
+def print_statistics(stats):
+    for key, val in stats.items():
+        if isinstance(val, Texttable):
+            logger.info(key)
+            logger.info(val.draw())
+        else:
+            logger.info('{}: {}'.format(key, val))
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..205e63f
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,83 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+
+from nncf.utils import get_all_modules
+from nncf import Config as NNCFConfig
+from nncf import load_state
+from nncf import create_compressed_model
+
+
+def wrap_nncf_model(model, cfg):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig(cfg.nncf_config)
+    input_size = nncf_config.get(
+        'input_sample_size', (1, 3, cfg.input_size, cfg.input_size)
+    )
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([torch.randn(input_size).to(device),],)
+        input_kwargs = dict(return_loss=False, dummy_forward=True)
+        model(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+    compression_ctrl, model = create_compressed_model(
+        model, nncf_config, dummy_forward_fn=dummy_forward
+    )
+    return model, compression_ctrl
+
+
+def load_checkpoint(model, filename, map_location=None, strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError('No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader:
+    def __init__(self, data_loader, kwargs, device):
+        self.data_loader = data_loader
+        self.data_iterator = self.data_loader.__iter__()
+        self.kwargs = kwargs
+        self.device = device
+        self.num_workers = data_loader.num_workers
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        # redefined InitializeDataLoader because
+        # of DataContainer format in mmdet
+        input = self.data_iterator.__next__()
+        input_tensor = input['img']._data[0].to(self.device)
+        return ([input_tensor,], None, self.kwargs)
+
+
+def nncf_algo_initialize(compression_ctrl, data_loaders, device):
+    mmdet_kwargs = dict(return_loss=False, dummy_forward=True)
+    wrapped_loader = MMInitializeDataLoader(
+        data_loader=data_loaders[0], kwargs=mmdet_kwargs, device=device
+    )
+
+    compression_ctrl.initialize(wrapped_loader)
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index 0a71307..628382e 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -109,6 +109,9 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
                 raise TypeError('{} must be a list, but got {}'.format(
                     name, type(var)))
 
+        if kwargs['dummy_forward']:
+            return self.forward_dummy(imgs[0])
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(
@@ -133,8 +136,12 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             assert 'proposals' not in kwargs
             return self.aug_test(imgs, img_metas, **kwargs)
 
+    @abstractmethod
+    def forward_dummy(self, img, **kwargs):
+        pass
+
     @auto_fp16(apply_to=('img', ))
-    def forward(self, img, img_metas, return_loss=True, **kwargs):
+    def forward(self, img, img_metas=[], return_loss=True, **kwargs):
         """
         Calls either forward_train or forward_test depending on whether
         return_loss=True. Note this setting will change the expected inputs.
diff --git a/tools/train.py b/tools/train.py
index 01df1ca..7d36c50 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -7,6 +7,7 @@ import time
 
 import mmcv
 import torch
+
 from mmcv import Config
 from mmcv.runner import init_dist
 
@@ -103,6 +104,12 @@ def main():
     logger.info('Distributed training: {}'.format(distributed))
     logger.info('Config:\n{}'.format(cfg.text))
 
+    if 'nncf_config' in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
+
     # set random seeds
     if args.seed is not None:
         logger.info('Set random seed to {}, deterministic: {}'.format(
-- 
2.7.4

