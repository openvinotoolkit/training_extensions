name: Performance-Speed Benchmark Test

on:
  workflow_dispatch: # run on request (no need for PR)
    inputs:
      model-type:
        type: choice
        description: Model type to run benchmark
        options:
        - default  # speed, balance, accuracy models only
        - all      # default + other models
        default: default
      data-size:
        type: choice
        description: Dataset size to run benchmark
        options:
        - small
        - medium
        - large
        - all
        default: large
      num-repeat:
        description: Overrides default per-data-size number of repeat setting
        default: 0
      num-epoch:
        description: Overrides default per-model number of epoch setting
        default: 0
      eval-upto:
        type: choice
        description: The last operation to evaluate. 'optimize' means all.
        options:
        - train
        - export
        - optimize
        default: optimize

jobs:
  Perf-Speed-Benchmark-Tests:
    name: Perf-Speed-Benchmark-Test-py310-all
    uses: ./.github/workflows/run_tests_in_tox.yml
    with:
      python-version: "3.10"
      toxenv-pyver: "py310"
      toxenv-task: all
      tests-dir: >
        tests/perf/
        -k speed
        --model-type ${{ inputs.model-type }}
        --data-root /home/validation/data/new/
        --data-size ${{ inputs.data-size }}
        --num-repeat ${{ inputs.num-repeat }}
        --num-epoch ${{ inputs.num-epoch }}
        --summary-csv .tox/perf-speed-benchmark-all.csv
      runs-on: "['self-hosted', 'Linux', 'X64', 'dmount']"
      task: all
      timeout-minutes: 8640
      upload-artifact: true
      artifact-prefix: perf-speed-benchmark
