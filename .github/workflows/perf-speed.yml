name: Performance-Speed Benchmark Test

on:
  workflow_dispatch: # run on request (no need for PR)
    inputs:
      model-category:
        type: choice
        description: Model category to run benchmark
        options:
          - default # speed, balance, accuracy models only
          - all # default + other models
        default: default
      data-size:
        type: choice
        description: Dataset size to run benchmark
        options:
          - small
          - medium
          - large
          - all
        default: medium
      num-repeat:
        description: Overrides default per-data-size number of repeat setting
        default: 1
      num-epoch:
        description: Overrides default per-model number of epoch setting
        default: 3
      eval-upto:
        type: choice
        description: The last operation to evaluate. 'optimize' means all.
        options:
          - train
          - export
          - optimize
        default: optimize

jobs:
  Perf-Speed-Benchmark:
    name: Perf-Speed-Benchmark-all-py310
    runs-on: [self-hosted, linux, x64, dmount]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Install Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install tox
        run: python -m pip install tox
      - name: Run Performance Test
        run: >
          tox -vv -e performance-test -- tests/perf
          -k speed
          --model-category ${{ inputs.model-category }}
          --data-root /home/validation/data/v2/
          --data-size ${{ inputs.data-size }}
          --num-repeat ${{ inputs.num-repeat }}
          --num-epoch ${{ inputs.num-epoch }}
          --eval-upto ${{ inputs.eval-upto }}
          --summary-csv .tox/perf-speed-benchmark-all.csv
          --mlflow-tracking-uri ${{ vars.MLFLOW_TRACKING_SERVER_URI }}
          --user-name ${{ vars.USER_NAME }}
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: perf-speed-benchmark-all
          path: .tox/perf-*.csv
        # Use always() to always run this step to publish test results when there are test failures
        if: ${{ always() }}
