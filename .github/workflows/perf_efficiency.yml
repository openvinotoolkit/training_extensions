name: Performance-Efficiency Benchmark

on:
  workflow_dispatch: # run on request (no need for PR)
    inputs:
      model-category:
        type: choice
        description: Model category to run benchmark
        options:
          - default # speed, balance, accuracy models only
          - all # default + other models
        default: default
      data-size:
        type: choice
        description: Dataset size to run benchmark
        options:
          - small
          - medium
          - large
          - all
        default: medium
      num-repeat:
        description: Overrides default per-data-size number of repeat setting
        default: 1
      num-epoch:
        description: Overrides default per-model number of epoch setting
        default: 3
      eval-upto:
        type: choice
        description: The last operation to evaluate. 'optimize' means all.
        options:
          - train
          - export
          - optimize
        default: optimize
      artifact-prefix:
        type: string
        default: perf-efficiency-benchmark
  workflow_call:
    inputs:
      model-category:
        type: string
        description: Model category to run benchmark [default, all]
        default: default
      data-size:
        type: string
        description: Dataset size to run benchmark [small, medium, large, all]
        default: medium
      num-repeat:
        type: number
        description: Overrides default per-data-size number of repeat setting
        default: 1
      num-epoch:
        type: number
        description: Overrides default per-model number of epoch setting
        default: 3
      eval-upto:
        type: string
        description: The last operation to evaluate. 'optimize' means all [train, export, optimize]
        default: optimize
      artifact-prefix:
        type: string
        default: perf-efficiency-benchmark

# Declare default permissions as read only.
permissions: read-all

jobs:
  Perf-Efficiency-Benchmark:
    name: Perf-Efficiency-Benchmark-all-py310
    uses: ./.github/workflows/run_tests_in_tox.yml
    with:
      python-version: "3.10"
      toxenv-pyver: "py310"
      toxenv-task: all
      tests-dir: >
        tests/perf/
        --benchmark-type efficiency
        --model-category ${{ inputs.model-category }}
        --data-root /home/validation/data/new/
        --data-size ${{ inputs.data-size }}
        --num-repeat ${{ inputs.num-repeat }}
        --num-epoch ${{ inputs.num-epoch }}
        --eval-upto ${{ inputs.eval-upto }}
        --summary-csv .tox/perf-efficiency-benchmark-all.csv
      runs-on: "['self-hosted', 'Linux', 'X64', 'dmount']"
      task: all
      timeout-minutes: 8640
      upload-artifact: true
      artifact-prefix: ${{ inputs.artifact-prefix }}
