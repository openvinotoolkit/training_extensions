<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Object Detection &#8212; OpenVINO™ Training Extensions 1.6.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=d626d52b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script src="../../../../_static/documentation_options.js?v=662e1299"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'guide/explanation/algorithms/object_detection/object_detection';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Segmentation" href="../segmentation/index.html" />
    <link rel="prev" title="Object Detection" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../../index.html">

  
  
  
  
  
  
  

  
    <img src="../../../../_static/logos/otx-logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../../_static/logos/otx-logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-toggle="tooltip"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-toggle="tooltip"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Section navigation">
  <p class="bd-links__title" role="heading" aria-level="1">
    Section Navigation
  </p>
  <div class="bd-toc-item navbar-nav">
    <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/cli_commands.html">OpenVINO™ Training Extensions CLI commands</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/base/index.html">Base Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tutorials/base/how_to_train/index.html">How to train, validate, export and optimize the model</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/classification.html">Classification  model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/detection.html">Object Detection model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/instance_segmentation.html">Instance Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/semantic_segmentation.html">Semantic Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/anomaly_detection.html">Anomaly Detection Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/action_classification.html">Action Classification model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/base/how_to_train/action_detection.html">Action Detection model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/demo.html">How to run the demonstration mode with OpenVINO™ Training Extensions CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/deploy.html">How to deploy the model and use demo in exportable code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/base/explain.html">How to explain the model behavior</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/advanced/index.html">Advanced Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/semi_sl.html">Use Semi-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/self_sl.html">Use Self-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/backbones.html">Backbone Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/api_tutorial.html">Utilize OpenVINO™ Training Extensions APIs in your project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/advanced/hpo_tutorial.html">Simple HPO Tutorial</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Explanation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Algorithms</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../classification/index.html">Classification</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../classification/multi_class_classification.html">Multi-class Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../classification/multi_label_classification.html">Multi-label Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../classification/hierarhical_classification.html">Hierarchical Classification</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Object Detection</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../segmentation/index.html">Segmentation</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../segmentation/semantic_segmentation.html">Semantic Segmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../segmentation/instance_segmentation.html">Instance Segmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../anomaly/index.html">Anomaly Detection</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../action/index.html">Action Recognition</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../action/action_classification.html">Action Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../action/action_detection.html">Action Detection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../visual_prompting/index.html">Visual Prompting</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../visual_prompting/fine_tuning.html">Visual Prompting (Fine-tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../visual_prompting/zero_shot.html">Visual Prompting (Zero-shot learning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../additional_features/index.html">Additional Features</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/models_optimization.html">Models Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/hpo.html">Hyperparameters Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/auto_configuration.html">Auto-configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/adaptive_training.html">Adaptive Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/xai.html">Explainable AI (XAI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/noisy_label_detection.html">Noisy Label Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/fast_data_loading.html">Fast Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/tiling.html">Improve Small Object Detection with Image Tiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../additional_features/config_input_size.html">Configurable Input Size</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../reference/index.html">API reference</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.html">otx</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.html">otx.algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.action.html">otx.algorithms.action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.anomaly.html">otx.algorithms.anomaly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.classification.html">otx.algorithms.classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.common.html">otx.algorithms.common</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.detection.html">otx.algorithms.detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.segmentation.html">otx.algorithms.segmentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.visual_prompting.html">otx.algorithms.visual_prompting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.api.html">otx.api</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.configuration.html">otx.api.configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.entities.html">otx.api.entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.serialization.html">otx.api.serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.usecases.html">otx.api.usecases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.utils.html">otx.api.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.html">otx.cli</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.builder.html">otx.cli.builder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.manager.html">otx.cli.manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.registry.html">otx.cli.registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.tools.html">otx.cli.tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.utils.html">otx.cli.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.core.html">otx.core</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.data.html">otx.core.data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.file.html">otx.core.file</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.ov.html">otx.core.ov</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.patcher.html">otx.core.patcher</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../reference/_autosummary/otx.hpo.html">otx.hpo</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.html">otx.recipes</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.stages.html">otx.recipes.stages</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.html">otx.utils</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.logger.html">otx.utils.logger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.utils.html">otx.utils.utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../release_notes/index.html">Releases</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>

  </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  <section id="object-detection">
<h1>Object Detection<a class="headerlink" href="#object-detection" title="Link to this heading">#</a></h1>
<p>Object detection is a computer vision task where it’s needed to locate objects, finding their bounding boxes coordinates together with defining class.
The input is an image, and the output is a pair of coordinates for bouding box corners and a class number for each detected object.</p>
<p>The common approach to building object detection architecture is to take a feature extractor (backbone), that can be inherited from the classification task.
Then goes a head that calculates coordinates and class probabilities based on aggregated information from the image.
Additionally, some architectures use <a class="reference external" href="https://arxiv.org/abs/1612.03144">Feature Pyramid Network (FPN)</a> to transfer and process feature maps from backbone to head and called neck.</p>
<p>For the supervised training we use the following algorithms components:</p>
<ul id="od-supervised-pipeline">
<li><p><code class="docutils literal notranslate"><span class="pre">Augmentations</span></code>: We use random crop and random rotate, simple bright and color distortions and multiscale training for the training pipeline.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>: We use <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> optimizer with the weight decay set to <strong>1e-4</strong> and momentum set to <strong>0.9</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Learning</span> <span class="pre">rate</span> <span class="pre">schedule</span></code>: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a>. This learning rate scheduler proved its efficiency in dataset-agnostic trainings, its logic is to drop LR after some time without improving the target accuracy metric. Also, we update it with <code class="docutils literal notranslate"><span class="pre">iteration_patience</span></code> parameter that ensures that a certain number of training iterations (steps through the dataset) were passed before dropping LR.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Loss</span> <span class="pre">function</span></code>: We use <a class="reference external" href="https://giou.stanford.edu/">Generalized IoU Loss</a>  for localization loss to train the ability of the model to find the coordinates of the objects. For the classification head, we use a standard <a class="reference external" href="https://arxiv.org/abs/1708.02002">FocalLoss</a>.</p></li>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">Additional</span> <span class="pre">training</span> <span class="pre">techniques</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Early</span> <span class="pre">stopping</span></code>: To add adaptability to the training pipeline and prevent overfitting. You can use early stopping like the below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train {TEMPLATE} ... \
            params \
            --learning_parameters.enable_early_stopping=True
</pre></div>
</div>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2211.17170">Anchor clustering for SSD</a>: This model highly relies on predefined anchor boxes hyperparameter that impacts the size of objects, which can be detected. So before training, we collect object statistics within dataset, cluster them and modify anchor boxes sizes to fit the most for objects the model is going to detect.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Backbone</span> <span class="pre">pretraining</span></code>: we pretrained MobileNetV2 backbone on large <a class="reference external" href="https://github.com/Alibaba-MIIL/ImageNet21K">ImageNet21k</a> dataset to improve feature extractor and learn better and faster.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<section id="dataset-format">
<h2>Dataset Format<a class="headerlink" href="#dataset-format" title="Link to this heading">#</a></h2>
<p>At the current point we support <a class="reference external" href="https://cocodataset.org/#format-data">COCO</a> and
<a class="reference external" href="https://openvinotoolkit.github.io/datumaro/stable/docs/data-formats/formats/pascal_voc.html">Pascal-VOC</a> dataset formats.
Learn more about the formats by following the links above. Here is an example of expected format for COCO dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── annotations/
    ├── instances_train.json
    ├── instances_val.json
    └── instances_test.json
├──images/
    (Split is optional)
    ├── train
    ├── val
    └── test
</pre></div>
</div>
<p>If you have your dataset in those formats, then you can simply run using one line of code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx train  &lt;model_template&gt; --train-data-roots &lt;path_to_data_root&gt; \
                                        --val-data-roots &lt;path_to_data_root&gt;
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please, refer to our <a class="reference internal" href="../../../tutorials/base/how_to_train/detection.html"><span class="doc">dedicated tutorial</span></a> for more information how to train, validate and optimize detection models.</p>
</div>
</section>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h2>
<p>We support the following ready-to-use model templates:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_tiny/template.yaml">Custom_Object_Detection_YOLOX</a></p></td>
<td><p>YOLOX-TINY</p></td>
<td><p>6.5</p></td>
<td><p>20.4</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_s/template.yaml">Object_Detection_YOLOX_S</a></p></td>
<td><p>YOLOX_S</p></td>
<td><p>33.51</p></td>
<td><p>46.0</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_l/template.yaml">Object_Detection_YOLOX_L</a></p></td>
<td><p>YOLOX_L</p></td>
<td><p>194.57</p></td>
<td><p>207.0</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_x/template.yaml">Object_Detection_YOLOX_X</a></p></td>
<td><p>YOLOX_X</p></td>
<td><p>352.42</p></td>
<td><p>378.0</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/mobilenetv2_ssd/template.yaml">Custom_Object_Detection_Gen3_SSD</a></p></td>
<td><p>SSD</p></td>
<td><p>9.4</p></td>
<td><p>7.6</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/mobilenetv2_atss/template.yaml">Custom_Object_Detection_Gen3_ATSS</a></p></td>
<td><p>MobileNetV2-ATSS</p></td>
<td><p>20.6</p></td>
<td><p>9.1</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnext101_atss/template.yaml">Object_Detection_ResNeXt101_ATSS</a></p></td>
<td><p>ResNeXt101-ATSS</p></td>
<td><p>434.75</p></td>
<td><p>344.0</p></td>
</tr>
</tbody>
</table>
<p>Above table can be found using the following command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx find --task detection
</pre></div>
</div>
<p><a class="reference external" href="https://arxiv.org/abs/1912.02424">MobileNetV2-ATSS</a> is a good medium-range model that works well and fast in most cases.
<a class="reference external" href="https://arxiv.org/abs/1512.02325">SSD</a> and <a class="reference external" href="https://arxiv.org/abs/2107.08430">YOLOX</a> are light models, that a perfect for the fastest inference on low-power hardware.
YOLOX achieved the same accuracy as SSD, and even outperforms its inference on CPU 1.5 times, but requires 3 times more time for training due to <a class="reference external" href="https://arxiv.org/pdf/2004.10934.pdf">Mosaic augmentation</a>, which is even more than for ATSS.
So if you have resources for a long training, you can pick the YOLOX model.
ATSS still shows good performance among <a class="reference external" href="https://arxiv.org/abs/1708.02002">RetinaNet</a> based models. Therfore, We added ATSS with large scale backbone, ResNeXt101-ATSS. We integrated large ResNeXt101 backbone to our Custom ATSS head, and it shows good transfer learning performance.
In addition, we added a YOLOX variants to support users’ diverse situations.</p>
<p>In addition to these models, we supports experimental models for object detection. These experimental models will be changed to official models within a few releases.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnet50_deformable_detr/template_experimental.yaml">Object_Detection_Deformable_DETR</a></p></td>
<td><p>Deformable_DETR</p></td>
<td><p>165</p></td>
<td><p>157.0</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnet50_dino/template_experimental.yaml">Object_Detection_DINO</a></p></td>
<td><p>DINO</p></td>
<td><p>235</p></td>
<td><p>182.0</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnet50_litedino/template_experimental.yaml">Object_Detection_Lite_DINO</a></p></td>
<td><p>Lite-DINO</p></td>
<td><p>140</p></td>
<td><p>190.0</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://arxiv.org/abs/2010.04159">Deformable_DETR</a> is <a class="reference external" href="https://arxiv.org/abs/2005.12872">DETR</a> based model, and it solves slow convergence problem of DETR. <a class="reference external" href="https://arxiv.org/abs/2203.03605">DINO</a> improves Deformable DETR based methods via denoising anchor boxes. Current SOTA models for object detection are based on DINO.
<a class="reference external" href="https://arxiv.org/abs/2303.07335">Lite-DINO</a> is efficient structure for DINO. It reduces FLOPS of transformer’s encoder which takes the highest computational costs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For using experimental templates, you should specify full path of experimental template. Ex) otx build src/otx/algorithms/detection/configs/detection/resnet50_dino/template_experimental.yaml –task detection</p>
</div>
<p>In addition to these models, we supports experimental models for object detection. These experimental models will be changed to official models within a few releases.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Template ID</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Model size (MB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnet50_deformable_detr/template_experimental.yaml">Custom_Object_Detection_Gen3_Deformable_DETR</a></p></td>
<td><p>Deformable_DETR</p></td>
<td><p>165</p></td>
<td><p>157.0</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnet50_dino/template_experimental.yaml">Custom_Object_Detection_Gen3_DINO</a></p></td>
<td><p>DINO</p></td>
<td><p>235</p></td>
<td><p>182.0</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/openvinotoolkit/training_extensions/blob/develop/src/otx/algorithms/detection/configs/detection/resnext101_atss/template_experimental.yaml">Custom_Object_Detection_Gen3_ResNeXt101_ATSS</a></p></td>
<td><p>ResNeXt101-ATSS</p></td>
<td><p>434.75</p></td>
<td><p>344.0</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://arxiv.org/abs/2010.04159">Deformable_DETR</a> is <a class="reference external" href="https://arxiv.org/abs/2005.12872">DETR</a> based model, and it solves slow convergence problem of DETR. <a class="reference external" href="https://arxiv.org/abs/2203.03605">DINO</a> improves Deformable DETR based methods via denoising anchor boxes. Current SOTA models for object detection are based on DINO.
Although transformer based models show notable performance on various object detection benchmark, CNN based model still show good performance with proper latency.
Therefore, we added a new experimental CNN based method, ResNeXt101-ATSS. ATSS still shows good performance among <a class="reference external" href="https://arxiv.org/abs/1708.02002">RetinaNet</a> based models. We integrated large ResNeXt101 backbone to our Custom ATSS head, and it shows good transfer learning performance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For using experimental templates, you should specify full path of experimental template. Ex) otx build src/otx/algorithms/detection/configs/detection/resnet50_dino/template_experimental.yaml –task detection</p>
</div>
<p>Besides this, we support public backbones from <a class="reference external" href="https://pytorch.org/vision/stable/index.html">torchvision</a>, <a class="reference external" href="https://github.com/osmr/imgclsmob">pytorchcv</a>, <a class="reference external" href="https://github.com/open-mmlab/mmclassification">mmcls</a> and <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo">OpenVino Model Zoo</a>.
Please, refer to the <a class="reference internal" href="../../../tutorials/advanced/backbones.html"><span class="doc">tutorial</span></a> how to customize models and run public backbones.</p>
<p>To see which public backbones are available for the task, the following command can be executed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ otx find --backbone {torchvision, pytorchcv, mmcls, omz.mmcls}
</pre></div>
</div>
<p>In the table below the test mAP on some academic datasets using our <a class="reference internal" href="#od-supervised-pipeline"><span class="std std-ref">supervised pipeline</span></a> is presented.</p>
<p>For <a class="reference external" href="https://cocodataset.org/#home">COCO</a> dataset the accuracy of pretrained weights is shown, and we report official COCO mAP with AP50.
Except for COCO, we report AP50 as performance metric.</p>
<p>5 datasets were selected as transfer learning datasets.
<a class="reference external" href="https://www.bdd100k.com/">BDD100K</a> is the largest dataset among we used. 70000 images are used as train images and 10000 images are used for validation.
<a class="reference external" href="https://public.roboflow.com/object-detection/brackish-underwater">Brackish</a> and <a class="reference external" href="https://public.roboflow.com/object-detection/plantdoc">Plantdoc</a> are datasets of medium size. They have around 10000 images for train and 1500 images for validation.
<a class="reference external" href="https://public.roboflow.com/object-detection/bccd">BCCD</a> and <a class="reference external" href="https://public.roboflow.com/object-detection/chess-full">Chess pieces</a> are datasets of small size. They have around 300 images for train and 100 images for validation.
We used our own templates without any modification.
For hyperparameters, please, refer to the related template.
We trained each model with a single Nvidia GeForce RTX3090.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model name</p></th>
<th class="head"><p>COCO(AP50)</p></th>
<th class="head"><p>BDD100K</p></th>
<th class="head"><p>Brackish</p></th>
<th class="head"><p>Plantdoc</p></th>
<th class="head"><p>BCCD</p></th>
<th class="head"><p>Chess pieces</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>YOLOX-TINY</p></td>
<td><p>31.0 (48.2)</p></td>
<td><p>24.8</p></td>
<td><p>96.3</p></td>
<td><p>51.5</p></td>
<td><p>88.5</p></td>
<td><p>99.2</p></td>
</tr>
<tr class="row-odd"><td><p>SSD</p></td>
<td><p>13.5</p></td>
<td><p>28.2</p></td>
<td><p>96.5</p></td>
<td><p>52.9</p></td>
<td><p>91.1</p></td>
<td><p>99.1</p></td>
</tr>
<tr class="row-even"><td><p>MobileNetV2-ATSS</p></td>
<td><p>32.5 (49.5)</p></td>
<td><p>40.2</p></td>
<td><p>99.1</p></td>
<td><p>63.4</p></td>
<td><p>93.4</p></td>
<td><p>99.1</p></td>
</tr>
<tr class="row-odd"><td><p>ResNeXt101-ATSS</p></td>
<td><p>45.1 (63.8)</p></td>
<td><p>45.5</p></td>
<td><p>99.3</p></td>
<td><p>69.3</p></td>
<td><p>93.1</p></td>
<td><p>99.1</p></td>
</tr>
<tr class="row-even"><td><p>ResNet50-Deformable-DETR</p></td>
<td><p>44.3 (63.2)</p></td>
<td><p>44.8</p></td>
<td><p>97.7</p></td>
<td><p>60.7</p></td>
<td><p>93.4</p></td>
<td><p>99.2</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet50-DINO</p></td>
<td><p>49.0 (66.4)</p></td>
<td><p>47.2</p></td>
<td><p>99.5</p></td>
<td><p>62.9</p></td>
<td><p>93.5</p></td>
<td><p>99.1</p></td>
</tr>
<tr class="row-even"><td><p>ResNet50-Lite-DINO</p></td>
<td><p>48.1 (64.4)</p></td>
<td><p>47.0</p></td>
<td><p>99.0</p></td>
<td><p>62.5</p></td>
<td><p>93.6</p></td>
<td><p>99.4</p></td>
</tr>
<tr class="row-odd"><td><p>YOLOX-S</p></td>
<td><p>40.3 (59.1)</p></td>
<td><p>37.1</p></td>
<td><p>93.6</p></td>
<td><p>54.8</p></td>
<td><p>92.7</p></td>
<td><p>98.8</p></td>
</tr>
<tr class="row-even"><td><p>YOLOX-L</p></td>
<td><p>49.4 (67.1)</p></td>
<td><p>44.5</p></td>
<td><p>94.6</p></td>
<td><p>55.8</p></td>
<td><p>91.8</p></td>
<td><p>99.0</p></td>
</tr>
<tr class="row-odd"><td><p>YOLOX-X</p></td>
<td><p>50.9 (68.4)</p></td>
<td><p>44.2</p></td>
<td><p>96.3</p></td>
<td><p>56.2</p></td>
<td><p>91.5</p></td>
<td><p>98.9</p></td>
</tr>
</tbody>
</table>
</section>
<section id="semi-supervised-learning">
<h2>Semi-supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Link to this heading">#</a></h2>
<p>For Semi-SL task solving we use the <a class="reference external" href="https://arxiv.org/abs/2102.09480">Unbiased Teacher model</a>, which is a specific implementation of Semi-SL for object detection. The unbiased teacher detaches the student model and the teacher model to prevent the teacher from being polluted by noisy pseudo-labels. In the early stage, the teacher model is trained by supervised loss. This stage is called a burn-in stage. After the burn-in, the student model is trained using both pseudo-labeled data from the teacher model and labeled data. And the teacher model is updated using
EMA.</p>
<p>In Semi-SL, the pseudo-labeling process is combined with a consistency loss that ensures that the predictions of the model are consistent across augmented versions of the same data. This helps to reduce the impact of noisy or incorrect labels that may arise from the pseudo-labeling process. Additionally, our algorithm uses a combination of strong data augmentations and a specific optimizer called Sharpness-Aware Minimization (SAM) to further improve the accuracy of the model.</p>
<p>Overall, OpenVINO™ Training Extensions utilizes powerful techniques for improving the performance of Semi-SL algorithm with limited labeled data. They can be particularly useful in domains where labeled data is expensive or difficult to obtain, and can help to reduce the time and cost associated with collecting labeled data.</p>
<ul class="simple" id="od-semi-supervised-pipeline">
<li><p><code class="docutils literal notranslate"><span class="pre">Pseudo-labeling</span></code>: A specific implementation of Semi-SL that combines the use of pseudo-labeling with a consistency loss, strong data augmentations, and a specific optimizer called Sharpness-Aware Minimization (SAM) to improve the performance of the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Weak</span> <span class="pre">&amp;</span> <span class="pre">Strong</span> <span class="pre">augmentation</span></code>: For teacher model weak augmentations(random flip) are applied to input image. For the student model strong augmentations(colorjtter, grayscale, goussian blur, random erasing) are applied.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Additional</span> <span class="pre">training</span> <span class="pre">techniques</span></code>: Other than that, we use several solutions that apply to supervised learning (No bias Decay, Augmentations, Early stopping, LR conditioning.).</p></li>
</ul>
<p>Please, refer to the <a class="reference internal" href="../../../tutorials/advanced/semi_sl.html"><span class="doc">tutorial</span></a> how to train semi supervised learning.</p>
<p>In the table below the mAP on toy data sample from <a class="reference external" href="https://cocodataset.org/#home">COCO</a> dataset using our pipeline is presented.</p>
<p>We sample 400 images that contain one of [person, car, bus] for labeled train images. And 4000 images for unlabeled images. For validation 100 images are selected from val2017.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head" colspan="2"><p>Sampled COCO dataset</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>SL</p></td>
<td><p>Semi-SL</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNetV2-ATSS</p></td>
<td><div class="line-block">
<div class="line">Person: 69.70</div>
<div class="line">Car:    65.00</div>
<div class="line">Bus:    42.96</div>
<div class="line">Mean:   59.20</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Person: 69.44</div>
<div class="line">Car:    65.84</div>
<div class="line">Bus:    50.7</div>
<div class="line">Mean:   61.98</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>SSD</p></td>
<td><div class="line-block">
<div class="line">Person: 39.24</div>
<div class="line">Car:    19.24</div>
<div class="line">Bus:    21.34</div>
<div class="line">Mean:   26.60</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Person: 38.52</div>
<div class="line">Car:    28.02</div>
<div class="line">Bus:    26.28</div>
<div class="line">Mean:   30.96</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>YOLOX</p></td>
<td><div class="line-block">
<div class="line">Person: 65.64</div>
<div class="line">Car:    64.44</div>
<div class="line">Bus:    60.68</div>
<div class="line">Mean:   63.6</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Person: 69.00</div>
<div class="line">Car:   65.66</div>
<div class="line">Bus:   65.12</div>
<div class="line">Mean:  66.58</div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
</section>


            </article>
            
            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="index.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Object Detection</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="../segmentation/index.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Segmentation</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-format">
   Dataset Format
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#models">
   Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semi-supervised-learning">
   Semi-supervised Learning
  </a>
 </li>
</ul>

</nav>
</div>

<div class="toc-item">
  
<div id="searchbox"></div>
</div>

<div class="toc-item">
  
</div>

<div class="toc-item">
  
<div class="tocsection sourcelink">
    <a href="../../../../_sources/guide/explanation/algorithms/object_detection/object_detection.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
</div>

</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2024, OpenVINO™ Training Extensions Contributors.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="theme-version">
    Built with the
    <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">
        PyData Sphinx Theme
    </a>
    0.12.0.
</p>
  </div>
  
  <div class="footer-item">
    
<p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 7.2.6.<br>
</p>

  </div>
  
</div>
  </footer>
  </body>
</html>