model:
  class_path: otx.algo.classification.timm_model.TimmModelForMulticlassCls
  init_args:
    label_info: 1000
    backbone: mobilenetv4_conv_small.e2400_r224_in1k

    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.002
        weight_decay: 0.01
    # optimizer:
    #   class_path: torch.optim.SGD
    #   init_args:
    #     lr: 0.002
    #     momentum: 0.9
    #     weight_decay: 0.0001

    scheduler:
      class_path: otx.core.schedulers.LinearWarmupSchedulerCallable
      init_args:
        num_warmup_steps: 10
        main_scheduler_callable:
          class_path: lightning.pytorch.cli.ReduceLROnPlateau
          init_args:
            mode: max
            factor: 0.5
            patience: 1
            monitor: val/accuracy

engine:
  task: MULTI_CLASS_CLS
  device: auto

callback_monitor: val/accuracy

data: ../../../_base_/data/classification.yaml
overrides:
  # reset:
  #   - data.train_subset.transforms

  max_epochs: 90
  callbacks:
    - class_path: otx.algo.callbacks.adaptive_early_stopping.EarlyStoppingWithWarmup
      init_args:
        patience: 3
  # data:
  #   train_subset:
  #     transforms:
  #       - class_path: otx.core.data.transform_libs.torchvision.RandomResizedCrop
  #         init_args:
  #           scale: $(input_size)
  #           interpolation: bicubic
  #       - class_path: otx.core.data.transform_libs.torchvision.RandomFlip
  #         init_args:
  #           prob: 0.5
  #           is_numpy_to_tvtensor: true
  #       - class_path: torchvision.transforms.v2.ColorJitter
  #         init_args:
  #           brightness:
  #             - 0.6
  #             - 1.4
  #           contrast:
  #             - 0.6
  #             - 1.4
  #           saturation:
  #             - 0.6
  #             - 1.4
  #       - class_path: torchvision.transforms.v2.ToDtype
  #         init_args:
  #           dtype: ${as_torch_dtype:torch.float32}
  #           scale: false
  #       - class_path: torchvision.transforms.v2.Normalize
  #         init_args:
  #           mean: [123.675, 116.28, 103.53]
  #           std: [58.395, 57.12, 57.375]
  # - class_path: torchvision.transforms.v2.Normalize
  #   init_args:
  #     mean: [0.485, 0.456, 0.406]
  #     std: [0.229, 0.224, 0.225]
