task: MULTI_CLASS_CLS
input_size: 224
mem_cache_size: 1GB
mem_cache_img_max_size:
  - 500
  - 500
image_color_channel: RGB
stack_images: True
data_format: imagenet_with_subset_dirs
unannotated_items_ratio: 0.0
train_subset:
  subset_name: train
  transform_lib_type: TORCHVISION
  batch_size: 16
  num_workers: 2
  to_tv_image: False
  transforms:
    - class_path: otx.core.data.transform_libs.torchvision.Resize
      init_args:
        scale: $(input_size)
    - class_path: otx.core.data.transform_libs.torchvision.RandomFlip
      init_args:
        prob: 0.5
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: False
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean: [123.675, 116.28, 103.53]
        std: [58.395, 57.12, 57.375]
  sampler:
    class_path: otx.algo.samplers.balanced_sampler.BalancedSampler

val_subset:
  subset_name: val
  transform_lib_type: TORCHVISION
  batch_size: 64
  num_workers: 2
  to_tv_image: False
  transforms:
    - class_path: otx.core.data.transform_libs.torchvision.Resize
      init_args:
        scale: $(input_size)
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: False
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean: [123.675, 116.28, 103.53]
        std: [58.395, 57.12, 57.375]
  sampler:
    class_path: torch.utils.data.RandomSampler

test_subset:
  subset_name: test
  transform_lib_type: TORCHVISION
  batch_size: 64
  num_workers: 2
  to_tv_image: False
  transforms:
    - class_path: otx.core.data.transform_libs.torchvision.Resize
      init_args:
        scale: $(input_size)
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: False
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean: [123.675, 116.28, 103.53]
        std: [58.395, 57.12, 57.375]
  sampler:
    class_path: torch.utils.data.RandomSampler

unlabeled_subset:
  data_format: image_dir
  batch_size: 48
  subset_name: unlabeled
  to_tv_image: False
  transforms:
    weak_transforms:
      - class_path: otx.core.data.transform_libs.torchvision.Resize
        init_args:
          scale: $(input_size)
      - class_path: otx.core.data.transform_libs.torchvision.RandomFlip
        init_args:
          prob: 0.5
          is_numpy_to_tvtensor: true
      - class_path: torchvision.transforms.v2.ToDtype
        init_args:
          dtype: ${as_torch_dtype:torch.float32}
          scale: false
      - class_path: torchvision.transforms.v2.Normalize
        init_args:
          mean:
            - 123.675
            - 116.28
            - 103.53
          std:
            - 58.395
            - 57.12
            - 57.375

    strong_transforms:
      - class_path: otx.core.data.transform_libs.torchvision.Resize
        init_args:
          scale: $(input_size)
      - class_path: otx.core.data.transform_libs.torchvision.RandomFlip
        init_args:
          prob: 0.5
          is_numpy_to_tvtensor: true
      - class_path: torchvision.transforms.v2.RandAugment
        init_args:
          num_ops: 8
          magnitude: 10
      - class_path: torchvision.transforms.v2.ToDtype
        init_args:
          dtype: ${as_torch_dtype:torch.float32}
          scale: false
      - class_path: torchvision.transforms.v2.Normalize
        init_args:
          mean:
            - 123.675
            - 116.28
            - 103.53
          std:
            - 58.395
            - 57.12
            - 57.375
  transform_lib_type: TORCHVISION
  num_workers: 2
  sampler:
    class_path: torch.utils.data.RandomSampler
    init_args: {}
