task: VISUAL_PROMPTING
mem_cache_size: 1GB
mem_cache_img_max_size: null
image_color_channel: RGB
stack_images: false
data_format: coco_instances
unannotated_items_ratio: 0.0
vpm_config:
  use_bbox: true
  use_point: false

train_subset:
  subset_name: train
  transform_lib_type: TORCHVISION
  to_tv_image: true
  transforms:
  - class_path: otx.core.data.transform_libs.torchvision.ResizetoLongestEdge
    init_args:
      size: 1024
      antialias: true
  - class_path: otx.core.data.transform_libs.torchvision.PadtoSquare
  - class_path: torchvision.transforms.v2.ToDtype
    init_args:
      dtype: ${as_torch_dtype:torch.float32}
  - class_path: torchvision.transforms.v2.Normalize
    init_args:
      mean: [123.675, 116.28, 103.53]
      std: [58.395, 57.12, 57.375]
  batch_size: 2
  num_workers: 4
  sampler:
    class_path: torch.utils.data.RandomSampler

val_subset:
  subset_name: val
  transform_lib_type: TORCHVISION
  to_tv_image: true
  transforms:
  - class_path: otx.core.data.transform_libs.torchvision.ResizetoLongestEdge
    init_args:
      size: 1024
      antialias: true
  - class_path: otx.core.data.transform_libs.torchvision.PadtoSquare
  - class_path: torchvision.transforms.v2.ToDtype
    init_args:
      dtype: ${as_torch_dtype:torch.float32}
  - class_path: torchvision.transforms.v2.Normalize
    init_args:
      mean: [123.675, 116.28, 103.53]
      std: [58.395, 57.12, 57.375]
  batch_size: 1
  num_workers: 4
  sampler:
    class_path: torch.utils.data.RandomSampler

test_subset:
  subset_name: test
  transform_lib_type: TORCHVISION
  to_tv_image: true
  transforms:
  - class_path: otx.core.data.transform_libs.torchvision.ResizetoLongestEdge
    init_args:
      size: 1024
      antialias: true
  - class_path: otx.core.data.transform_libs.torchvision.PadtoSquare
  - class_path: torchvision.transforms.v2.ToDtype
    init_args:
      dtype: ${as_torch_dtype:torch.float32}
  - class_path: torchvision.transforms.v2.Normalize
    init_args:
      mean: [123.675, 116.28, 103.53]
      std: [58.395, 57.12, 57.375]
  batch_size: 1
  num_workers: 4
  sampler:
    class_path: torch.utils.data.RandomSampler
