{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTX API DEMO (MMPretrain Example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization Training API\n",
    "\n",
    "Select a framework & import adapter modules.\n",
    "\n",
    "We'll choose MMpretrain here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Environment:\n",
    "- mmpretrain-1.0.0rc8\n",
    "- mmcv-2.0.1\n",
    "- mmengine-0.7.4\n",
    "- mmdeploy-1.2.0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset & DataLoader\n",
    "1. Prepare a dataset and enter path into Dataset\n",
    "\n",
    "    - Convert to OTX's DatasetEntity and Label Schema by leveraging Datumaro's features through paths (path -> Datumaro -> OTX DatasetEntity & LabelSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain import Dataset\n",
    "sample_dataset = \"../../../../tests/assets/classification_dataset\"\n",
    "dataset = Dataset(\n",
    "    train_data_roots=sample_dataset,\n",
    "    val_data_roots=sample_dataset,\n",
    "    test_data_roots=sample_dataset,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. Build Torch Dataset from MMCV config (filepath or dict) -> torch.utils.data.Dataset\n",
    "\n",
    "    - User can build a dataset from a config file or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-08-22 15:07:42,291 | INFO : Try to create a 0 size memory pool.\n",
      "Dataset type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 25\n",
      "Dataset size: 25\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = dataset.train_dataloader()\n",
    "\n",
    "print(f\"Dataset type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 13\n",
      "Dataset size: 25\n"
     ]
    }
   ],
   "source": [
    "# Customize batch_size & num_workers\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=2,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"DataLoader type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 25\n",
      "Dataset size: 25\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = dataset.val_dataloader()\n",
    "\n",
    "print(f\"DataLoader type: {type(val_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(val_dataloader)}\")\n",
    "print(f\"Dataset size: {len(val_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 25\n",
      "Dataset size: 25\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = dataset.test_dataloader()\n",
    "\n",
    "print(f\"DataLoader type: {type(test_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(test_dataloader)}\")\n",
    "print(f\"Dataset size: {len(test_dataloader.dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "### Model provided by OTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet101-csra_1xb16_voc07-448px', 'resnet101_8xb16_cifar10', 'resnet101_8xb32_in1k', 'resnet152_8xb16_cifar10', 'resnet152_8xb32_in1k', 'resnet18_8xb16_cifar10', 'resnet18_8xb32_in1k', 'resnet34_8xb16_cifar10', 'resnet34_8xb32_in1k', 'resnet50-arcface_inshop', 'resnet50_8xb16_cifar10', 'resnet50_8xb16_cifar100', 'resnet50_8xb256-rsb-a1-600e_in1k', 'resnet50_8xb256-rsb-a2-300e_in1k', 'resnet50_8xb256-rsb-a3-100e_in1k', 'resnet50_8xb32-fp16_in1k', 'resnet50_8xb32_in1k', 'resnet50_8xb8_cub', 'resnet50_barlowtwins-pre_8xb32-linear-coslr-100e_in1k', 'resnet50_byol-pre_8xb512-linear-coslr-90e_in1k', 'resnet50_densecl-pre_8xb32-linear-steplr-100e_in1k', 'resnet50_mocov2-pre_8xb32-linear-steplr-100e_in1k', 'resnet50_mocov3-100e-pre_8xb128-linear-coslr-90e_in1k', 'resnet50_mocov3-300e-pre_8xb128-linear-coslr-90e_in1k', 'resnet50_mocov3-800e-pre_8xb128-linear-coslr-90e_in1k', 'resnet50_simclr-200e-pre_8xb512-linear-coslr-90e_in1k', 'resnet50_simclr-800e-pre_8xb512-linear-coslr-90e_in1k', 'resnet50_simsiam-100e-pre_8xb512-linear-coslr-90e_in1k', 'resnet50_simsiam-200e-pre_8xb512-linear-coslr-90e_in1k', 'resnet50_spark-pre_300e_in1k', 'resnet50_swav-pre_8xb32-linear-coslr-100e_in1k', 'resnetv1c101_8xb32_in1k', 'resnetv1c152_8xb32_in1k', 'resnetv1c50_8xb32_in1k', 'resnetv1d101_8xb32_in1k', 'resnetv1d152_8xb32_in1k', 'resnetv1d50_8xb32_in1k']\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain import list_models\n",
    "# MMPretrain's model\n",
    "model_list = list_models(\"resnet*\")\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['otx_efficientnet_b0', 'otx_mobilenet_v3_large_1']\n"
     ]
    }
   ],
   "source": [
    "# OTX's model\n",
    "model_list = list_models(\"otx*\")\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'mmpretrain.models.classifiers.image.ImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain import get_model\n",
    "# mmpretrain's pre-defined model\n",
    "mmpretrain_model = get_model(\"resnet18_8xb32_in1k\")\n",
    "print(f\"Model type: {type(mmpretrain_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:07:43,235 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 15:07:43,259 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 15:07:43,325 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "Model type: <class 'otx.v2.adapters.torch.mmengine.mmpretrain.modules.models.classifiers.custom_image_classifier.CustomImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "## Build Model from model_name\n",
    "\n",
    "otx_model = get_model(\n",
    "    model=\"otx_efficientnet_b0\",\n",
    "    num_classes=dataset.num_classes\n",
    ")\n",
    "print(f\"Model type: {type(otx_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:07:43,399 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 15:07:43,419 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 15:07:43,484 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "Model type: <class 'otx.v2.adapters.torch.mmengine.mmpretrain.modules.models.classifiers.custom_image_classifier.CustomImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "## Build Model from Config\n",
    "otx_model = get_model(\n",
    "    model=\"../configs/classification/models/otx_efficientnet_b0.yaml\",\n",
    "    num_classes=dataset.num_classes\n",
    ")\n",
    "print(f\"Model type: {type(otx_model)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Users can use each framework's training provided by OTX. (Engine)\n",
    "\n",
    "- The engine requires the necessary models and DataLoaders for each framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/22 15:07:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 359152301\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 359152301\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 15:07:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 15:07:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 15:07:44,592 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 15:07:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "08/22 15:07:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150743\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][13/13]  lr: 1.0000e-02  eta: 0:00:02  time: 0.0295  data_time: 0.0008  memory: 305  loss: 0.8828\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 15:07:46 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150743\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0258  data_time: 0.0007  memory: 305  loss: 2.2504\n",
      "08/22 15:07:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150743\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0256  data_time: 0.0007  memory: 305  loss: 0.9330\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "/tmp/otx-test/best_accuracy_top1_epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain.engine import MMPTEngine\n",
    "\n",
    "# OTX Model Training\n",
    "engine = MMPTEngine(work_dir=\"/tmp/otx-test\",)\n",
    "\n",
    "# Training without validation\n",
    "results = engine.train(\n",
    "    model=otx_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "print(results[\"checkpoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:07:47,317 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][25/25]    accuracy/top1: 44.0000  data_time: 0.0004  time: 0.0062\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 44.0000 accuracy/top1 at 3 epoch is saved to best_accuracy_top1_epoch_3.pth.\n",
      "Val Metric: {'accuracy/top1': 44.0}\n",
      "2023-08-22 15:07:47,748 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [25/25]    accuracy/top1: 44.0000  data_time: 0.0003  time: 0.0061\n",
      "Test Metric: {'accuracy/top1': 44.0}\n"
     ]
    }
   ],
   "source": [
    "val_score = engine.validate(val_dataloader=val_dataloader)\n",
    "print(f\"Val Metric: {val_score}\")\n",
    "\n",
    "test_score = engine.test(test_dataloader=test_dataloader)\n",
    "print(f\"Test Metric: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1936483432\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1936483432\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 15:07:48,116 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - init_weights of CustomImageClassifier has been called more than once.\n",
      "08/22 15:07:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150747\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0259  data_time: 0.0008  memory: 366  loss: 0.4958\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 15:07:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][25/25]    accuracy/top1: 100.0000  data_time: 0.0003  time: 0.0061\n",
      "08/22 15:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 100.0000 accuracy/top1 at 1 epoch is saved to best_accuracy_top1_epoch_1.pth.\n",
      "08/22 15:07:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150747\n",
      "08/22 15:07:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0251  data_time: 0.0007  memory: 366  loss: 0.5317\n",
      "08/22 15:07:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "08/22 15:07:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][25/25]    accuracy/top1: 52.0000  data_time: 0.0003  time: 0.0061\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150747\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0263  data_time: 0.0008  memory: 366  loss: 1.1432\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][25/25]    accuracy/top1: 0.0000  data_time: 0.0003  time: 0.0061\n",
      "/tmp/otx-test/best_accuracy_top1_epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "# Training with validation\n",
    "results = engine.train(\n",
    "    model=otx_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "print(results[\"checkpoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:07:50,525 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][25/25]    accuracy/top1: 0.0000  data_time: 0.0003  time: 0.0061\n",
      "Val Metric: {'accuracy/top1': 0.0}\n",
      "2023-08-22 15:07:50,759 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [25/25]    accuracy/top1: 0.0000  data_time: 0.0003  time: 0.0061\n",
      "Test Metric: {'accuracy/top1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "val_score = engine.validate()\n",
    "print(f\"Val Metric: {val_score}\")\n",
    "\n",
    "test_score = engine.test(test_dataloader=test_dataloader)\n",
    "print(f\"Test Metric: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /tmp/otx-test/best_accuracy_top1_epoch_3.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224)\n",
      "    ori_shape: (24, 24)\n",
      "    num_classes: 2\n",
      "    scale_factor: (9.333333333333334, 9.333333333333334)\n",
      "\n",
      "DATA FIELDS\n",
      "    pred_label: tensor([1], device='cuda:0')\n",
      "    pred_score: tensor([0.3193, 0.6807], device='cuda:0')\n",
      "\n",
      ") at 0x7fe481d879d0>]\n"
     ]
    }
   ],
   "source": [
    "sample = \"../../../../tests/assets/classification_dataset/0/11.jpg\"\n",
    "\n",
    "predict_output = engine.predict(\n",
    "    model=results[\"model\"],\n",
    "    checkpoint=results[\"checkpoint\"],\n",
    "    img=sample\n",
    ")\n",
    "\n",
    "print(predict_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export (with mmdeploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/22 15:07:51 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmengine\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmengine\" is a correct scope, or whether the registry is initialized.\n",
      "08/22 15:07:51 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "08/22 15:07:51 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "2023-08-22 15:07:51,889 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "08/22 15:07:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: /tmp/otx-test/openvino.onnx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/mmpretrain/utils/setup_env.py:34: UserWarning: The current default scope \"mmengine\" is not \"mmpretrain\", `register_all_modules` will force the current default scope to be \"mmpretrain\". If this is not expected, please set `init_default_scope=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/22 15:07:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Args for Model Optimizer: mo --input_model=\"/tmp/otx-test/openvino.onnx\" --output_dir=\"/tmp/otx-test/\" --output=\"output\" --input=\"input\" --input_shape=\"[1, 3, 224, 224]\" --mean_values=\"[123.675, 116.28, 103.53]\" --scale_values=\"[58.395, 57.12, 57.375]\" \n",
      "08/22 15:07:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - [ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/otx-test/openvino.xml\n",
      "[ SUCCESS ] BIN file: /tmp/otx-test/openvino.bin\n",
      "\n",
      "08/22 15:07:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Successfully exported OpenVINO model: /tmp/otx-test/openvino.xml\n",
      "{'outputs': {'bin': '/tmp/otx-test/openvino.bin', 'xml': '/tmp/otx-test/openvino.xml'}}\n"
     ]
    }
   ],
   "source": [
    "export_output = engine.export(\n",
    "    model=results[\"model\"],\n",
    "    checkpoint=results[\"checkpoint\"],\n",
    ")\n",
    "\n",
    "print(export_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-SL Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "[*] Semisupervised training type detected with unlabeled data: ../../../../tests/assets/unlabeled_dataset\n",
      "2023-08-22 15:07:55,775 | INFO : possible max iterations = 13\n"
     ]
    }
   ],
   "source": [
    "# Semi-SL\n",
    "from otx.v2.adapters.torch.mmengine.mmpretrain import Dataset\n",
    "sample_dataset = \"../../../../tests/assets/classification_dataset\"\n",
    "semisl_dataset = Dataset(\n",
    "    train_data_roots=sample_dataset,\n",
    "    val_data_roots=sample_dataset,\n",
    "    test_data_roots=sample_dataset,\n",
    "    unlabeled_data_roots=\"../../../../tests/assets/unlabeled_dataset\"\n",
    ")\n",
    "\n",
    "strong_pipeline = [\n",
    "    dict(type=\"OTXRandAugment\", num_aug=8, magnitude=10),\n",
    "]\n",
    "\n",
    "pipeline = {\n",
    "    \"train\": [\n",
    "        dict(type=\"Resize\", scale=[224, 224]),\n",
    "        dict(type=\"mmpretrain.PackInputs\"),\n",
    "    ],\n",
    "    \"unlabeled\": [\n",
    "        dict(type=\"Resize\", scale=[224, 224]),\n",
    "        dict(type=\"PostAug\", keys=dict(img_strong=strong_pipeline)),\n",
    "        dict(type=\"mmpretrain.PackMultiKeyInputs\", input_key=\"img\", multi_key=[\"img_strong\"]),\n",
    "    ]\n",
    "}\n",
    "\n",
    "semisl_train_dataloader = semisl_dataset.train_dataloader(\n",
    "    pipeline=pipeline,\n",
    "    batch_size=2,\n",
    "    unlabeled_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:07:55,851 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain import get_model\n",
    "semi_sl_model = dict(\n",
    "    model=dict(\n",
    "        type='SemiSLClassifier',\n",
    "        backbone=dict(\n",
    "            type='OTXEfficientNet',\n",
    "            pretrained=True,\n",
    "            version='b0'),\n",
    "        neck=dict(\n",
    "            type='GlobalAveragePooling'),\n",
    "        head=dict(\n",
    "            type='SemiLinearClsHead',\n",
    "            num_classes=10,\n",
    "            in_channels=1280,\n",
    "            loss=dict(\n",
    "                type='CrossEntropyLoss',\n",
    "                loss_weight=1.0),\n",
    "            topk=(1, 5)),\n",
    "        pretrained=None\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "semisl_model = get_model(model=semi_sl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/22 15:07:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 783113650\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 783113650\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 15:07:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 15:07:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 15:07:56,015 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:56 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 15:07:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/semi-sl.\n",
      "08/22 15:07:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150755\n",
      "08/22 15:07:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][13/13]  lr: 1.0000e-02  eta: 0:00:01  time: 0.0612  data_time: 0.0091  memory: 901  loss: 3.6480  unlabeled_loss: 0.0482\n",
      "08/22 15:07:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 15:07:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "08/22 15:07:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150755\n",
      "08/22 15:07:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0613  data_time: 0.0090  memory: 901  loss: 2.3799  unlabeled_loss: 0.0305\n",
      "08/22 15:07:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "08/22 15:07:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150755\n",
      "08/22 15:07:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0608  data_time: 0.0090  memory: 901  loss: 0.8218  unlabeled_loss: 0.1443\n",
      "08/22 15:07:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "/tmp/semi-sl/epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain.engine import MMPTEngine\n",
    "engine = MMPTEngine(work_dir=\"/tmp/semi-sl\")\n",
    "results = engine.train(\n",
    "    model=semisl_model,\n",
    "    train_dataloader=semisl_train_dataloader,\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "print(results[\"checkpoint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTX AutoRunner (Automation Training API)\n",
    "OTX provides a more convenient API called AutoRunner.\n",
    "\n",
    "- It's more convenient for users to use Engine, which provides auto-configuration and the features provided by OTX without having to choose a framework.\n",
    "- Prepare Dataset & DataLoader + Prepare Model + OTX Recipes + Training + ETC.\n",
    "- This will make all of the above steps happen automatically. (Auto: Model Selection & build, Dataset Configuration, Training, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-08-22 15:07:59,035 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 15:07:59,056 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 15:07:59,122 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 15:07:59,124 | WARNING : Currently, OTX does not accept val_dataloader as a dict configuration.\n",
      "2023-08-22 15:07:59,124 | WARNING : Currently, OTX does not accept test_dataloader as a dict configuration.\n",
      "2023-08-22 15:07:59,125 | WARNING : In Engine.config, remove ['framework', 'data', 'optimizer', 'max_epochs', 'max_iters', 'val_interval', 'precision'] that are unavailable to the Runner.\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1162452383\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1162452383\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 15:07:59,282 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 15:07:59 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/OTX-API-test.\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150759\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][13/13]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0286  data_time: 0.0041  memory: 625  loss: 1.0878\n",
      "08/22 15:07:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 15:08:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_150759\n",
      "08/22 15:08:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][13/13]  lr: 5.0000e-03  eta: 0:00:00  time: 0.0271  data_time: 0.0040  memory: 454  loss: 0.7704\n",
      "08/22 15:08:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "/tmp/OTX-API-test/epoch_2.pth\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.api.core import AutoRunner\n",
    "\n",
    "output_dir = \"/tmp/OTX-API-test\"\n",
    "sample_dataset = \"../../../../tests/assets/classification_dataset\"\n",
    "\n",
    "engine = AutoRunner(\n",
    "    work_dir=output_dir,\n",
    "    train_data_roots=sample_dataset,\n",
    ")\n",
    "\n",
    "# Customization training\n",
    "results = engine.train(\n",
    "    batch_size=2,\n",
    "    max_epochs=2\n",
    ")\n",
    "print(results[\"checkpoint\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
