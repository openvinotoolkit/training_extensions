{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTX CLI\n",
    "1. otx [subcommand] [args] --help\n",
    "- Users can see the options available to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-09-08 16:55:41,461 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:41,486 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:41,551 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "\u001b[91mUsage:\u001b[0m \u001b[37motx [options] train\u001b[0m [\u001b[36m-h\u001b[0m] [\u001b[36m-c\u001b[0m \u001b[36mCONFIG\u001b[0m] [\u001b[36m--print_config\u001b[0m \u001b[36m[=flags]\u001b[0m]\n",
      "                           [\u001b[36m-o\u001b[0m \u001b[36mWORK_DIR\u001b[0m] [\u001b[36m--framework\u001b[0m \u001b[36mFRAMEWORK\u001b[0m]\n",
      "                           [\u001b[36m--model\u001b[0m \u001b[36mCONFIG\u001b[0m]\n",
      "                           [\u001b[36m--model.multilabel\u001b[0m \u001b[36mMODEL.MULTILABEL\u001b[0m]\n",
      "                           [\u001b[36m--model.hierarchical\u001b[0m \u001b[36mMODEL.HIERARCHICAL\u001b[0m]\n",
      "                           [\u001b[36m--model.task_adapt\u001b[0m \u001b[36mMODEL.TASK_ADAPT\u001b[0m]\n",
      "                           [\u001b[36m--model.track_loss_dynamics\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--model.head\u001b[0m \u001b[36mMODEL.HEAD\u001b[0m]\n",
      "                           [\u001b[36m--model.backbone\u001b[0m \u001b[36mMODEL.BACKBONE\u001b[0m]\n",
      "                           [\u001b[36m--model.neck\u001b[0m \u001b[36mMODEL.NECK\u001b[0m]\n",
      "                           [\u001b[36m--model.pretrained\u001b[0m \u001b[36mMODEL.PRETRAINED\u001b[0m]\n",
      "                           [\u001b[36m--model.train_cfg\u001b[0m \u001b[36mMODEL.TRAIN_CFG\u001b[0m]\n",
      "                           [\u001b[36m--model.data_preprocessor\u001b[0m \u001b[36mMODEL.DATA_PREPROCESSOR\u001b[0m]\n",
      "                           [\u001b[36m--model.init_cfg\u001b[0m \u001b[36mMODEL.INIT_CFG\u001b[0m]\n",
      "                           [\u001b[36m--model.name\u001b[0m \u001b[36mMODEL.NAME\u001b[0m] [\u001b[36m--data\u001b[0m \u001b[36mCONFIG\u001b[0m]\n",
      "                           [\u001b[36m--data.task\u001b[0m \u001b[36mDATA.TASK\u001b[0m]\n",
      "                           [\u001b[36m--data.train_type\u001b[0m \u001b[36mDATA.TRAIN_TYPE\u001b[0m]\n",
      "                           [\u001b[36m--data.train_data_roots\u001b[0m \u001b[36mDATA.TRAIN_DATA_ROOTS\u001b[0m]\n",
      "                           [\u001b[36m--data.train_ann_files\u001b[0m \u001b[36mDATA.TRAIN_ANN_FILES\u001b[0m]\n",
      "                           [\u001b[36m--data.val_data_roots\u001b[0m \u001b[36mDATA.VAL_DATA_ROOTS\u001b[0m]\n",
      "                           [\u001b[36m--data.val_ann_files\u001b[0m \u001b[36mDATA.VAL_ANN_FILES\u001b[0m]\n",
      "                           [\u001b[36m--data.test_data_roots\u001b[0m \u001b[36mDATA.TEST_DATA_ROOTS\u001b[0m]\n",
      "                           [\u001b[36m--data.test_ann_files\u001b[0m \u001b[36mDATA.TEST_ANN_FILES\u001b[0m]\n",
      "                           [\u001b[36m--data.unlabeled_data_roots\u001b[0m \u001b[36mDATA.UNLABELED_DATA_ROOTS\u001b[0m]\n",
      "                           [\u001b[36m--data.unlabeled_file_list\u001b[0m \u001b[36mDATA.UNLABELED_FILE_LIST\u001b[0m]\n",
      "                           [\u001b[36m--data.data_format\u001b[0m \u001b[36mDATA.DATA_FORMAT\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader\u001b[0m \u001b[36mCONFIG\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.self\u001b[0m \u001b[36mTRAIN_DATALOADER.SELF\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.subset\u001b[0m \u001b[36mTRAIN_DATALOADER.SUBSET\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.pipeline\u001b[0m \u001b[36mTRAIN_DATALOADER.PIPELINE\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.batch_size\u001b[0m \u001b[36mTRAIN_DATALOADER.BATCH_SIZE\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.num_workers\u001b[0m \u001b[36mTRAIN_DATALOADER.NUM_WORKERS\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.distributed\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.config\u001b[0m \u001b[36mTRAIN_DATALOADER.CONFIG\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.shuffle\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.pin_memory\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.drop_last\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.sampler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.sampler\u001b[0m \u001b[36mTRAIN_DATALOADER.SAMPLER\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.persistent_workers\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--train_dataloader.unlabeled_batch_size\u001b[0m \u001b[36mTRAIN_DATALOADER.UNLABELED_BATCH_SIZE\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader\u001b[0m \u001b[36mCONFIG\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.self\u001b[0m \u001b[36mVAL_DATALOADER.SELF\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.subset\u001b[0m \u001b[36mVAL_DATALOADER.SUBSET\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.pipeline\u001b[0m \u001b[36mVAL_DATALOADER.PIPELINE\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.batch_size\u001b[0m \u001b[36mVAL_DATALOADER.BATCH_SIZE\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.num_workers\u001b[0m \u001b[36mVAL_DATALOADER.NUM_WORKERS\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.distributed\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.config\u001b[0m \u001b[36mVAL_DATALOADER.CONFIG\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.shuffle\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.pin_memory\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.drop_last\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.sampler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.sampler\u001b[0m \u001b[36mVAL_DATALOADER.SAMPLER\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.persistent_workers\u001b[0m \u001b[36m{true,false}\u001b[0m]\n",
      "                           [\u001b[36m--val_dataloader.unlabeled_batch_size\u001b[0m \u001b[36mVAL_DATALOADER.UNLABELED_BATCH_SIZE\u001b[0m]\n",
      "                           [\u001b[36m--optimizer.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--optimizer\u001b[0m \u001b[36mOPTIMIZER\u001b[0m] [\u001b[36m--checkpoint\u001b[0m \u001b[36mCHECKPOINT\u001b[0m]\n",
      "                           [\u001b[36m--max_iters\u001b[0m \u001b[36mMAX_ITERS\u001b[0m] [\u001b[36m--max_epochs\u001b[0m \u001b[36mMAX_EPOCHS\u001b[0m]\n",
      "                           [\u001b[36m--distributed\u001b[0m \u001b[36m{true,false,null}\u001b[0m] [\u001b[36m--seed\u001b[0m \u001b[36mSEED\u001b[0m]\n",
      "                           [\u001b[36m--deterministic\u001b[0m \u001b[36m{true,false,null}\u001b[0m]\n",
      "                           [\u001b[36m--precision\u001b[0m \u001b[36mPRECISION\u001b[0m]\n",
      "                           [\u001b[36m--val_interval\u001b[0m \u001b[36mVAL_INTERVAL\u001b[0m]\n",
      "                           [\u001b[36m--val_evaluator.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--val_evaluator\u001b[0m \u001b[36mVAL_EVALUATOR\u001b[0m]\n",
      "                           [\u001b[36m--param_scheduler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--param_scheduler\u001b[0m \u001b[36mPARAM_SCHEDULER\u001b[0m]\n",
      "                           [\u001b[36m--default_hooks\u001b[0m \u001b[36mDEFAULT_HOOKS\u001b[0m]\n",
      "                           [\u001b[36m--custom_hooks.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--custom_hooks\u001b[0m \u001b[36mCUSTOM_HOOKS\u001b[0m]\n",
      "                           [\u001b[36m--visualizer.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m]\n",
      "                           [\u001b[36m--visualizer\u001b[0m \u001b[36mVISUALIZER\u001b[0m]\n",
      "\n",
      "\u001b[39mTraining Functions with the MMEngine Framework.\u001b[0m\n",
      "\n",
      "\u001b[91mDefault Config File Locations:\u001b[0m\n",
      "  \u001b[39m['/home/harimkan/workspace/repo/otx-main/src/otx/v2/configs/classification\u001b[0m\n",
      "  \u001b[39m/otx_mmpretrain_cli.yaml', \u001b[0m\n",
      "  \u001b[39m'/home/harimkan/workspace/repo/otx-main/src/otx/v2/configs/classification/\u001b[0m\n",
      "  \u001b[39mmodels/otx_efficientnet_b0.yaml'], Note: default values below are the ones\u001b[0m\n",
      "  \u001b[39moverridden by the contents of: \u001b[0m\n",
      "  \u001b[39m['/home/harimkan/workspace/repo/otx-main/src/otx/v2/configs/classification\u001b[0m\n",
      "  \u001b[39m/otx_mmpretrain_cli.yaml', \u001b[0m\n",
      "  \u001b[39m'/home/harimkan/workspace/repo/otx-main/src/otx/v2/configs/classification/\u001b[0m\n",
      "  \u001b[39mmodels/otx_efficientnet_b0.yaml']\u001b[0m\n",
      "\n",
      "\u001b[91mOptional Arguments:\u001b[0m\n",
      "  \u001b[36m-h\u001b[0m, \u001b[36m--help\u001b[0m            \u001b[39mshow this help message and exit\u001b[0m\n",
      "  \u001b[36m-c\u001b[0m, \u001b[36m--config\u001b[0m \u001b[36mCONFIG\u001b[0m   \u001b[39mPath to a configuration file in yaml format.\u001b[0m\n",
      "  \u001b[36m--print_config\u001b[0m \u001b[36m[=flags]\u001b[0m\n",
      "                        \u001b[39mPrint the configuration after applying all other\u001b[0m\n",
      "                        \u001b[39marguments and exit. The optional flags customizes the\u001b[0m\n",
      "                        \u001b[39moutput and are one or more keywords separated by\u001b[0m\n",
      "                        \u001b[39mcomma. The supported flags are: comments,\u001b[0m\n",
      "                        \u001b[39mskip_default, skip_null.\u001b[0m\n",
      "  \u001b[36m-o\u001b[0m, \u001b[36m--work_dir\u001b[0m \u001b[36mWORK_DIR\u001b[0m\n",
      "                        \u001b[39mPath to store logs and outputs related to the command.\u001b[0m\n",
      "  \u001b[36m--framework\u001b[0m \u001b[36mFRAMEWORK\u001b[0m\n",
      "                        \u001b[39mSelect Framework: {mmpretrain, anomalib}.\u001b[0m\n",
      "  \u001b[36m--model.name\u001b[0m \u001b[36mMODEL.NAME\u001b[0m\n",
      "                        \u001b[39mEnter the name of model.\u001b[0m\n",
      "\n",
      "\u001b[91mSam-Enabled Imageclassifier:\u001b[0m\n",
      "  \u001b[36m--model\u001b[0m \u001b[36mCONFIG\u001b[0m        \u001b[39mPath to a configuration file.\u001b[0m\n",
      "  \u001b[36m--model.multilabel\u001b[0m \u001b[36mMODEL.MULTILABEL\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--model.hierarchical\u001b[0m \u001b[36mMODEL.HIERARCHICAL\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--model.task_adapt\u001b[0m \u001b[36mMODEL.TASK_ADAPT\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--model.track_loss_dynamics\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--model.head\u001b[0m \u001b[36mMODEL.HEAD\u001b[0m\n",
      "                        \u001b[39mThe head module to do prediction and calculate loss\u001b[0m\n",
      "                        \u001b[39mfrom processed features. See\u001b[0m\n",
      "                        \u001b[39m:mod:`\u001b[0m\u001b[1;39mmmpretrain.models.heads\u001b[0m\u001b[39m`. Notice that if the\u001b[0m\n",
      "                        \u001b[39mhead is not set, almost all methods cannot be used\u001b[0m\n",
      "                        \u001b[39mexcept :meth:`\u001b[0m\u001b[1;39mextract_feat\u001b[0m\u001b[39m`. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--model.backbone\u001b[0m \u001b[36mMODEL.BACKBONE\u001b[0m\n",
      "                        \u001b[39mThe backbone module. See\u001b[0m\n",
      "                        \u001b[39m:mod:`\u001b[0m\u001b[1;39mmmpretrain.models.backbones\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--model.neck\u001b[0m \u001b[36mMODEL.NECK\u001b[0m\n",
      "                        \u001b[39mThe neck module to process features from backbone. See\u001b[0m\n",
      "                        \u001b[39m:mod:`\u001b[0m\u001b[1;39mmmpretrain.models.necks\u001b[0m\u001b[39m`. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--model.pretrained\u001b[0m \u001b[36mMODEL.PRETRAINED\u001b[0m\n",
      "                        \u001b[39mThe pretrained checkpoint path, support local path and\u001b[0m\n",
      "                        \u001b[39mremote path. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--model.train_cfg\u001b[0m \u001b[36mMODEL.TRAIN_CFG\u001b[0m\n",
      "                        \u001b[39mThe training setting. The acceptable fields are:\u001b[0m\u001b[39m \u001b[0m\u001b[39m-\u001b[0m\n",
      "                        \u001b[39maugments (List\u001b[0m\u001b[39m): The batch augmentation methods to\u001b[0m\n",
      "                        \u001b[39muse.\u001b[0m\u001b[39m \u001b[0m\u001b[39mMore details can be found in\u001b[0m\n",
      "                        \u001b[39m:mod:`\u001b[0m\u001b[1;39mmmpretrain.model.utils.augment\u001b[0m\u001b[39m`. - probs (List\u001b[0m\u001b[39m,\u001b[0m\n",
      "                        \u001b[39moptional): The probability of every batch\u001b[0m\u001b[39m \u001b[0m\u001b[39maugmentation\u001b[0m\n",
      "                        \u001b[39mmethods. If None, choose evenly. Defaults to None.\u001b[0m\n",
      "                        \u001b[39mDefaults to None.\u001b[0m\n",
      "  \u001b[36m--model.data_preprocessor\u001b[0m \u001b[36mMODEL.DATA_PREPROCESSOR\u001b[0m\n",
      "                        \u001b[39mThe config for preprocessing input data. If None or no\u001b[0m\n",
      "                        \u001b[39mspecified type, it will use \"ClsDataPreprocessor\" as\u001b[0m\n",
      "                        \u001b[39mtype. See :class:`\u001b[0m\u001b[1;39mClsDataPreprocessor\u001b[0m\u001b[39m` for more\u001b[0m\n",
      "                        \u001b[39mdetails. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--model.init_cfg\u001b[0m \u001b[36mMODEL.INIT_CFG\u001b[0m\n",
      "                        \u001b[39mthe config to control the initialization. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "\n",
      "\u001b[91mMmpretrain'S Dataset Class:\u001b[0m\n",
      "  \u001b[36m--data\u001b[0m \u001b[36mCONFIG\u001b[0m         \u001b[39mPath to a configuration file.\u001b[0m\n",
      "  \u001b[36m--data.task\u001b[0m \u001b[36mDATA.TASK\u001b[0m\n",
      "                        \u001b[39mThe task type of the dataset want to load. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--data.train_type\u001b[0m \u001b[36mDATA.TRAIN_TYPE\u001b[0m\n",
      "                        \u001b[39mThe train type of the dataset want to load. Defaults\u001b[0m\n",
      "                        \u001b[39mto None.\u001b[0m\n",
      "  \u001b[36m--data.train_data_roots\u001b[0m \u001b[36mDATA.TRAIN_DATA_ROOTS\u001b[0m\n",
      "                        \u001b[39mThe root address of the dataset to be used for\u001b[0m\n",
      "                        \u001b[39mtraining. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.train_ann_files\u001b[0m \u001b[36mDATA.TRAIN_ANN_FILES\u001b[0m\n",
      "                        \u001b[39mLocation of the annotation file for the dataset to be\u001b[0m\n",
      "                        \u001b[39mused for training. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.val_data_roots\u001b[0m \u001b[36mDATA.VAL_DATA_ROOTS\u001b[0m\n",
      "                        \u001b[39mThe root address of the dataset to be used for\u001b[0m\n",
      "                        \u001b[39mvalidation. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.val_ann_files\u001b[0m \u001b[36mDATA.VAL_ANN_FILES\u001b[0m\n",
      "                        \u001b[39mLocation of the annotation file for the dataset to be\u001b[0m\n",
      "                        \u001b[39mused for validation. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.test_data_roots\u001b[0m \u001b[36mDATA.TEST_DATA_ROOTS\u001b[0m\n",
      "                        \u001b[39mThe root address of the dataset to be used for\u001b[0m\n",
      "                        \u001b[39mtesting. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.test_ann_files\u001b[0m \u001b[36mDATA.TEST_ANN_FILES\u001b[0m\n",
      "                        \u001b[39mLocation of the annotation file for the dataset to be\u001b[0m\n",
      "                        \u001b[39mused for testing. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.unlabeled_data_roots\u001b[0m \u001b[36mDATA.UNLABELED_DATA_ROOTS\u001b[0m\n",
      "                        \u001b[39mThe root address of the unlabeled dataset to be used\u001b[0m\n",
      "                        \u001b[39mfor training. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.unlabeled_file_list\u001b[0m \u001b[36mDATA.UNLABELED_FILE_LIST\u001b[0m\n",
      "                        \u001b[39mThe file where the list of unlabeled images is\u001b[0m\n",
      "                        \u001b[39mdeclared. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--data.data_format\u001b[0m \u001b[36mDATA.DATA_FORMAT\u001b[0m\n",
      "                        \u001b[39mThe format of the dataset. Defaults to None.\u001b[0m\n",
      "\n",
      "\u001b[91mMmpretrain'S Dataset.Subset_Dataloader:\u001b[0m\n",
      "  \u001b[36m--train_dataloader\u001b[0m \u001b[36mCONFIG\u001b[0m\n",
      "                        \u001b[39mPath to a configuration file.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.self\u001b[0m \u001b[36mTRAIN_DATALOADER.SELF\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--train_dataloader.subset\u001b[0m \u001b[36mTRAIN_DATALOADER.SUBSET\u001b[0m\n",
      "                        \u001b[39mEnter an available subset of that dataset.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.pipeline\u001b[0m, \u001b[36m--train_dataloader.pipeline+\u001b[0m \u001b[36mTRAIN_DATALOADER.PIPELINE\u001b[0m\n",
      "                        \u001b[39mDataset Pipeline. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.batch_size\u001b[0m \u001b[36mTRAIN_DATALOADER.BATCH_SIZE\u001b[0m\n",
      "                        \u001b[39mHow many samples per batch to load. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.num_workers\u001b[0m \u001b[36mTRAIN_DATALOADER.NUM_WORKERS\u001b[0m\n",
      "                        \u001b[39mHow many subprocesses to use for data loading. ``0``\u001b[0m\n",
      "                        \u001b[39mmeans that the data will be loaded in the main\u001b[0m\n",
      "                        \u001b[39mprocess. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.distributed\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mDistributed value for sampler. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.config\u001b[0m \u001b[36mTRAIN_DATALOADER.CONFIG\u001b[0m\n",
      "                        \u001b[39mPath to configuration file or Config. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.shuffle\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mSet to ``True`` to have the data reshuffled at every\u001b[0m\n",
      "                        \u001b[39mepoch. Defaults to True.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.pin_memory\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mIf ``True``, the data loader will copy Tensors into\u001b[0m\n",
      "                        \u001b[39mdevice/CUDA pinned memory before returning them.\u001b[0m\u001b[39m \u001b[0m\u001b[39mIf\u001b[0m\n",
      "                        \u001b[39myour data elements are a custom type, or your\u001b[0m\n",
      "                        \u001b[39m:attr:`\u001b[0m\u001b[1;39mcollate_fn\u001b[0m\u001b[39m` returns a batch that is a custom\u001b[0m\n",
      "                        \u001b[39mtype, see the example below. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.drop_last\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39m_description_. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.sampler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Sampler and\u001b[0m\n",
      "                        \u001b[39mexit.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.sampler\u001b[0m, \u001b[36m--train_dataloader.sampler+\u001b[0m \u001b[36mTRAIN_DATALOADER.SAMPLER\u001b[0m\n",
      "                        \u001b[39mDefines the strategy to draw samples from the dataset.\u001b[0m\n",
      "                        \u001b[39mCan be any ``Iterable`` with ``__len__`` implemented.\u001b[0m\n",
      "                        \u001b[39mIf specified, :attr:`\u001b[0m\u001b[1;39mshuffle\u001b[0m\u001b[39m` must not be specified..\u001b[0m\n",
      "                        \u001b[39mDefaults to None.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.persistent_workers\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mIf ``True``, the data loader will not shutdown the\u001b[0m\n",
      "                        \u001b[39mworker processes after a dataset has been consumed\u001b[0m\n",
      "                        \u001b[39monce. This allows to maintain the workers `\u001b[0m\u001b[1;39mDataset\u001b[0m\u001b[39m`\u001b[0m\n",
      "                        \u001b[39minstances alive. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--train_dataloader.unlabeled_batch_size\u001b[0m \u001b[36mTRAIN_DATALOADER.UNLABELED_BATCH_SIZE\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "\n",
      "\u001b[91mMmpretrain'S Dataset.Subset_Dataloader:\u001b[0m\n",
      "  \u001b[36m--val_dataloader\u001b[0m \u001b[36mCONFIG\u001b[0m\n",
      "                        \u001b[39mPath to a configuration file.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.self\u001b[0m \u001b[36mVAL_DATALOADER.SELF\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--val_dataloader.subset\u001b[0m \u001b[36mVAL_DATALOADER.SUBSET\u001b[0m\n",
      "                        \u001b[39mEnter an available subset of that dataset.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.pipeline\u001b[0m, \u001b[36m--val_dataloader.pipeline+\u001b[0m \u001b[36mVAL_DATALOADER.PIPELINE\u001b[0m\n",
      "                        \u001b[39mDataset Pipeline. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.batch_size\u001b[0m \u001b[36mVAL_DATALOADER.BATCH_SIZE\u001b[0m\n",
      "                        \u001b[39mHow many samples per batch to load. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.num_workers\u001b[0m \u001b[36mVAL_DATALOADER.NUM_WORKERS\u001b[0m\n",
      "                        \u001b[39mHow many subprocesses to use for data loading. ``0``\u001b[0m\n",
      "                        \u001b[39mmeans that the data will be loaded in the main\u001b[0m\n",
      "                        \u001b[39mprocess. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.distributed\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mDistributed value for sampler. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.config\u001b[0m \u001b[36mVAL_DATALOADER.CONFIG\u001b[0m\n",
      "                        \u001b[39mPath to configuration file or Config. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.shuffle\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mSet to ``True`` to have the data reshuffled at every\u001b[0m\n",
      "                        \u001b[39mepoch. Defaults to True.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.pin_memory\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mIf ``True``, the data loader will copy Tensors into\u001b[0m\n",
      "                        \u001b[39mdevice/CUDA pinned memory before returning them.\u001b[0m\u001b[39m \u001b[0m\u001b[39mIf\u001b[0m\n",
      "                        \u001b[39myour data elements are a custom type, or your\u001b[0m\n",
      "                        \u001b[39m:attr:`\u001b[0m\u001b[1;39mcollate_fn\u001b[0m\u001b[39m` returns a batch that is a custom\u001b[0m\n",
      "                        \u001b[39mtype, see the example below. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.drop_last\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39m_description_. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.sampler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Sampler and\u001b[0m\n",
      "                        \u001b[39mexit.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.sampler\u001b[0m, \u001b[36m--val_dataloader.sampler+\u001b[0m \u001b[36mVAL_DATALOADER.SAMPLER\u001b[0m\n",
      "                        \u001b[39mDefines the strategy to draw samples from the dataset.\u001b[0m\n",
      "                        \u001b[39mCan be any ``Iterable`` with ``__len__`` implemented.\u001b[0m\n",
      "                        \u001b[39mIf specified, :attr:`\u001b[0m\u001b[1;39mshuffle\u001b[0m\u001b[39m` must not be specified..\u001b[0m\n",
      "                        \u001b[39mDefaults to None.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.persistent_workers\u001b[0m \u001b[36m{true,false}\u001b[0m\n",
      "                        \u001b[39mIf ``True``, the data loader will not shutdown the\u001b[0m\n",
      "                        \u001b[39mworker processes after a dataset has been consumed\u001b[0m\n",
      "                        \u001b[39monce. This allows to maintain the workers `\u001b[0m\u001b[1;39mDataset\u001b[0m\u001b[39m`\u001b[0m\n",
      "                        \u001b[39minstances alive. Defaults to False.\u001b[0m\n",
      "  \u001b[36m--val_dataloader.unlabeled_batch_size\u001b[0m \u001b[36mVAL_DATALOADER.UNLABELED_BATCH_SIZE\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "\n",
      "\u001b[91mTraining Functions With The Mmengine Framework:\u001b[0m\n",
      "  \u001b[36m--optimizer.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Optimizer and\u001b[0m\n",
      "                        \u001b[39mexit.\u001b[0m\n",
      "  \u001b[36m--optimizer\u001b[0m \u001b[36mOPTIMIZER\u001b[0m\n",
      "                        \u001b[39m_description_. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--checkpoint\u001b[0m \u001b[36mCHECKPOINT\u001b[0m\n",
      "                        \u001b[39mModel checkpoint path. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--max_iters\u001b[0m \u001b[36mMAX_ITERS\u001b[0m\n",
      "                        \u001b[39mSpecifies the maximum iters of training. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--max_epochs\u001b[0m \u001b[36mMAX_EPOCHS\u001b[0m\n",
      "                        \u001b[39mSpecifies the maximum epoch of training. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--distributed\u001b[0m \u001b[36m{true,false,null}\u001b[0m\n",
      "                        \u001b[39mWhether to use the distributed setting. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--seed\u001b[0m \u001b[36mSEED\u001b[0m           \u001b[39mThe seed to use for training. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--deterministic\u001b[0m \u001b[36m{true,false,null}\u001b[0m\n",
      "                        \u001b[39mThe deterministic to use for training. Defaults to\u001b[0m\n",
      "                        \u001b[39mNone.\u001b[0m\n",
      "  \u001b[36m--precision\u001b[0m \u001b[36mPRECISION\u001b[0m\n",
      "                        \u001b[39mThe precision to use for training. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--val_interval\u001b[0m \u001b[36mVAL_INTERVAL\u001b[0m\n",
      "                        \u001b[39mSpecifies the validation Interval. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--val_evaluator.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Evaluator and\u001b[0m\n",
      "                        \u001b[39mexit.\u001b[0m\n",
      "  \u001b[36m--val_evaluator\u001b[0m, \u001b[36m--val_evaluator+\u001b[0m \u001b[36mVAL_EVALUATOR\u001b[0m\n",
      "                        \u001b[39mA evaluator object used for computing metrics for\u001b[0m\n",
      "                        \u001b[39mvalidation. It can be a dict or a list of dict to\u001b[0m\n",
      "                        \u001b[39mbuild a evaluator. If specified,\u001b[0m\n",
      "                        \u001b[39m:attr:`\u001b[0m\u001b[1;39mval_dataloader\u001b[0m\u001b[39m` should also be specified.\u001b[0m\n",
      "                        \u001b[39mDefaults to None.\u001b[0m\n",
      "  \u001b[36m--param_scheduler.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of\u001b[0m\n",
      "                        \u001b[39m_ParamScheduler and exit.\u001b[0m\n",
      "  \u001b[36m--param_scheduler\u001b[0m, \u001b[36m--param_scheduler+\u001b[0m \u001b[36mPARAM_SCHEDULER\u001b[0m\n",
      "                        \u001b[39m_EMPTY_HELP_\u001b[0m\n",
      "  \u001b[36m--default_hooks\u001b[0m \u001b[36mDEFAULT_HOOKS\u001b[0m\n",
      "                        \u001b[39mHooks to execute default actions like updating model\u001b[0m\n",
      "                        \u001b[39mparameters and saving checkpoints. Default hooks are\u001b[0m\n",
      "                        \u001b[39m``OptimizerHook``, ``IterTimerHook``, ``LoggerHook``,\u001b[0m\n",
      "                        \u001b[39m``ParamSchedulerHook`` and ``CheckpointHook``.\u001b[0m\n",
      "                        \u001b[39mDefaults to None. See :meth:`\u001b[0m\u001b[1;39mregister_default_hooks\u001b[0m\u001b[39m`\u001b[0m\n",
      "                        \u001b[39mfor more details.\u001b[0m\n",
      "  \u001b[36m--custom_hooks.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Hook and exit.\u001b[0m\n",
      "  \u001b[36m--custom_hooks\u001b[0m, \u001b[36m--custom_hooks+\u001b[0m \u001b[36mCUSTOM_HOOKS\u001b[0m\n",
      "                        \u001b[39mHooks to execute custom actions like visualizing\u001b[0m\n",
      "                        \u001b[39mimages processed by pipeline. Defaults to None.\u001b[0m\n",
      "  \u001b[36m--visualizer.help\u001b[0m \u001b[36mCLASS_PATH_OR_NAME\u001b[0m\n",
      "                        \u001b[39mShow the help for the given subclass of Visualizer and\u001b[0m\n",
      "                        \u001b[39mexit.\u001b[0m\n",
      "  \u001b[36m--visualizer\u001b[0m \u001b[36mVISUALIZER\u001b[0m\n",
      "                        \u001b[39mA Visualizer object or a dict build Visualizer object.\u001b[0m\n",
      "                        \u001b[39mDefaults to None. If not specified, default config\u001b[0m\n",
      "                        \u001b[39mwill be used.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. otx [subcommand] [args] --print_config\n",
    "- This allows the user to see the configuration used by the command before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-09-08 16:55:44,826 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:44,850 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:44,916 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "work_dir: null\n",
      "framework: mmpretrain\n",
      "model:\n",
      "  multilabel: false\n",
      "  hierarchical: false\n",
      "  task_adapt: null\n",
      "  track_loss_dynamics: false\n",
      "  head:\n",
      "    in_channels: -1\n",
      "    loss:\n",
      "      loss_weight: 1.0\n",
      "      type: CrossEntropyLoss\n",
      "    num_classes: 1000\n",
      "    topk:\n",
      "    - 1\n",
      "    - 5\n",
      "    type: CustomLinearClsHead\n",
      "  backbone:\n",
      "    pretrained: true\n",
      "    type: OTXEfficientNet\n",
      "    version: b0\n",
      "  neck:\n",
      "    type: GlobalAveragePooling\n",
      "  pretrained: null\n",
      "  train_cfg: null\n",
      "  data_preprocessor:\n",
      "    mean:\n",
      "    - 123.675\n",
      "    - 116.28\n",
      "    - 103.53\n",
      "    std:\n",
      "    - 58.395\n",
      "    - 57.12\n",
      "    - 57.375\n",
      "    to_rgb: true\n",
      "  init_cfg: null\n",
      "  name: otx_efficientnet_b0\n",
      "  type: CustomImageClassifier\n",
      "data:\n",
      "  task: Classification\n",
      "  train_type: Incremental\n",
      "  train_data_roots: ../../../../tests/assets/classification_dataset_class_incremental\n",
      "  train_ann_files: null\n",
      "  val_data_roots: null\n",
      "  val_ann_files: null\n",
      "  test_data_roots: null\n",
      "  test_ann_files: null\n",
      "  unlabeled_data_roots: null\n",
      "  unlabeled_file_list: null\n",
      "  data_format: null\n",
      "train_dataloader:\n",
      "  self: null\n",
      "  subset: train\n",
      "  pipeline: null\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  distributed: false\n",
      "  config: null\n",
      "  shuffle: true\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "  sampler: null\n",
      "  persistent_workers: false\n",
      "  unlabeled_batch_size: null\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - direction: horizontal\n",
      "      prob: 0.5\n",
      "      type: RandomFlip\n",
      "    - config_str: augmix-m5-w3-d1\n",
      "      type: AugMixAugment\n",
      "    - angle:\n",
      "      - -10\n",
      "      - 10\n",
      "      p: 0.35\n",
      "      type: RandomRotate\n",
      "    - type: PackInputs\n",
      "val_dataloader:\n",
      "  self: null\n",
      "  subset: val\n",
      "  pipeline: null\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  distributed: false\n",
      "  config: null\n",
      "  shuffle: true\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "  sampler: null\n",
      "  persistent_workers: false\n",
      "  unlabeled_batch_size: null\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - type: PackInputs\n",
      "optimizer:\n",
      "  type: SGD\n",
      "  lr: 0.01\n",
      "  momentum: 0.9\n",
      "  weight_decay: 0.0005\n",
      "checkpoint: null\n",
      "max_iters: null\n",
      "max_epochs: 10\n",
      "distributed: null\n",
      "seed: 1234\n",
      "deterministic: false\n",
      "precision: float32\n",
      "val_interval: 1\n",
      "val_evaluator:\n",
      "- type: Accuracy\n",
      "  topk:\n",
      "  - 1\n",
      "  - 5\n",
      "param_scheduler:\n",
      "  type: CosineAnnealingLR\n",
      "default_hooks:\n",
      "  logger:\n",
      "    type: LoggerHook\n",
      "    interval: 100\n",
      "  timer:\n",
      "    type: IterTimerHook\n",
      "  checkpoint:\n",
      "    interval: 1\n",
      "    max_keep_ckpts: 1\n",
      "    save_best: auto\n",
      "    type: CheckpointHook\n",
      "custom_hooks: null\n",
      "visualizer:\n",
      "  type: UniversalVisualizer\n",
      "  vis_backends:\n",
      "  - type: LocalVisBackend\n",
      "  - type: TensorboardVisBackend\n",
      "test_dataloader:\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - type: PackInputs\n",
      "test_evaluator:\n",
      "- type: Accuracy\n",
      "  topk:\n",
      "  - 1\n",
      "  - 5\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --print_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. otx train --[args]\n",
    "- Proceed with the training using the prepared commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-09-08 16:55:48,179 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:48,203 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:48,269 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:48,459 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:48,478 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:48,543 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1/classification\n",
      "2023-09-08 16:55:48,545 | INFO : Try to create a 0 size memory pool.\n",
      "09/08 16:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1234\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 2.0.0+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1234\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "09/08 16:55:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "09/08 16:55:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-09-08 16:55:49,810 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "09/08 16:55:49 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "09/08 16:55:49 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "09/08 16:55:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-demo-1/classification/20230908_165548_train.\n",
      "09/08 16:55:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165548\n",
      "09/08 16:55:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 1.1259  data_time: 0.0668  memory: 3620  loss: 1.1976\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165548\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1/1]  lr: 7.5000e-03  eta: 0:00:00  time: 0.6326  data_time: 0.0757  memory: 3632  loss: 1.0827\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165548\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1/1]  lr: 2.5000e-03  eta: 0:00:00  time: 0.4684  data_time: 0.0790  memory: 3632  loss: 0.9387\n",
      "09/08 16:55:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "[*] OTX Model Weight: /tmp/otx-demo-1/classification/20230908_165548_train/models/weights.pth\n",
      "[*] The OTX configuration used in the training: /tmp/otx-demo-1/classification/20230908_165548_train/models/configs.yaml\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --max_epochs 3 -o /tmp/otx-demo-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. otx test --[args]\n",
    "- Proceed with the testing using the prepared commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2023-09-08 16:55:55,171 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:55,195 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:55,261 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:55,610 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:55:55,629 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:55:55,696 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1-test/classification\n",
      "2023-09-08 16:55:55,698 | INFO : Try to create a 0 size memory pool.\n",
      "09/08 16:55:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1762870122\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 2.0.0+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1762870122\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "09/08 16:55:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "09/08 16:55:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: /tmp/otx-demo-1/classification/latest/weights.pth\n",
      "09/08 16:55:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from /tmp/otx-demo-1/classification/latest/weights.pth\n",
      "2023-09-08 16:55:56,929 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "09/08 16:55:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [1/1]    accuracy/top1: 40.6250  data_time: 0.0659  time: 0.9546\n",
      "{'accuracy/top1': 40.625}\n"
     ]
    }
   ],
   "source": [
    "! otx test --data.test_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --checkpoint /tmp/otx-demo-1/classification/latest/weights.pth -o /tmp/otx-demo-1-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. otx predict --img [img_path] --checkpoint [checkpoint_path]\n",
    "- Use the trained model to make predictions about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2023-09-08 16:56:01,272 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:01,295 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:01,361 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:01,696 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:01,715 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:01,781 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1-predict/classification\n",
      "Loads checkpoint by local backend from path: /tmp/otx-demo-1/classification/latest/weights.pth\n",
      "\u001b[?25lInference \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h[<DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    num_classes: 3\n",
      "    img_shape: (224, 224)\n",
      "    ori_shape: (24, 24)\n",
      "    scale_factor: (9.333333333333334, 9.333333333333334)\n",
      "\n",
      "DATA FIELDS\n",
      "    pred_label: tensor([2])\n",
      "    pred_score: tensor([0.2782, 0.3547, 0.3671])\n",
      "\n",
      ") at 0x7f3d8e3a0eb0>]\n"
     ]
    }
   ],
   "source": [
    "! otx predict --img ../../../../tests/assets/classification_dataset_class_incremental/2/22.jpg --checkpoint /tmp/otx-demo-1/classification/latest/weights.pth -o /tmp/otx-demo-1-predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. otx export --checkpoint [checkpoint_path] --[args]\n",
    "- This can be used to export trained TORCH models to other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2023-09-08 16:56:04,860 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:04,884 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:04,950 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:05,287 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:05,307 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:05,374 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1-export/classification\n",
      "09/08 16:56:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "09/08 16:56:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "2023-09-08 16:56:06,113 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "09/08 16:56:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: /tmp/otx-demo-1-export/classification/20230908_165605_export/onnx/openvino.onnx.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "09/08 16:56:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Args for Model Optimizer: mo --input_model=\"/tmp/otx-demo-1-export/classification/20230908_165605_export/onnx/openvino.onnx\" --output_dir=\"/tmp/otx-demo-1-export/classification/20230908_165605_export/openvino\" --output=\"output\" --input=\"input\" --input_shape=\"[1, 3, 224, 224]\" --mean_values=\"[123.675, 116.28, 103.53]\" --scale_values=\"[58.395, 57.12, 57.375]\" \n",
      "09/08 16:56:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - [ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/otx-demo-1-export/classification/20230908_165605_export/openvino/openvino.xml\n",
      "[ SUCCESS ] BIN file: /tmp/otx-demo-1-export/classification/20230908_165605_export/openvino/openvino.bin\n",
      "\n",
      "09/08 16:56:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Successfully exported OpenVINO model: /tmp/otx-demo-1-export/classification/20230908_165605_export/openvino/openvino.xml\n",
      "[*] Model exporting ended successfully.\n"
     ]
    }
   ],
   "source": [
    "! otx export --checkpoint /tmp/otx-demo-1/classification/latest/weights.pth -o /tmp/otx-demo-1-export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. otx train --config [output_config_file_path]\n",
    "- This gives users the ability to reuse the configs file output from the training to reproduce the same training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2023-09-08 16:56:12,013 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:12,038 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:12,103 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:12,448 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-09-08 16:56:12,467 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-09-08 16:56:12,532 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1-copy/classification\n",
      "2023-09-08 16:56:12,537 | INFO : Try to create a 0 size memory pool.\n",
      "2023-09-08 16:56:12,537 | WARNING : Currently, OTX does not accept val_dataloader as a dict configuration.\n",
      "2023-09-08 16:56:12,538 | WARNING : Currently, OTX does not accept test_dataloader as a dict configuration.\n",
      "2023-09-08 16:56:12,538 | WARNING : In Engine.config, remove ['data', 'optimizer', 'checkpoint', 'max_iters', 'max_epochs', 'distributed', 'precision', 'val_interval', 'framework'] that are unavailable to the Runner.\n",
      "09/08 16:56:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1234\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 2.0.0+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1234\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "09/08 16:56:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "09/08 16:56:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-09-08 16:56:13,704 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "09/08 16:56:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "09/08 16:56:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "09/08 16:56:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-demo-1-copy/classification/20230908_165612_train.\n",
      "09/08 16:56:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165612\n",
      "09/08 16:56:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 1.1195  data_time: 0.0681  memory: 3620  loss: 1.1976\n",
      "09/08 16:56:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165612\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1/1]  lr: 7.5000e-03  eta: 0:00:00  time: 0.6280  data_time: 0.0749  memory: 3632  loss: 1.0826\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230908_165612\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1/1]  lr: 2.5000e-03  eta: 0:00:00  time: 0.4652  data_time: 0.0784  memory: 3632  loss: 0.9387\n",
      "09/08 16:56:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "[*] OTX Model Weight: /tmp/otx-demo-1-copy/classification/20230908_165612_train/models/weights.pth\n",
      "[*] The OTX configuration used in the training: /tmp/otx-demo-1-copy/classification/20230908_165612_train/models/configs.yaml\n"
     ]
    }
   ],
   "source": [
    "! otx train --config /tmp/otx-demo-1/classification/latest/configs.yaml -o /tmp/otx-demo-1-copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
