{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTX CLI\n",
    "1. otx [subcommand] [args] --help\n",
    "- Users can see the options available to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:44:58,996 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:44:59,019 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:44:59,084 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "usage: otx [options] train [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "                           [-o WORK_DIR] [--framework FRAMEWORK]\n",
      "                           [--model CONFIG] [--model.multilabel MULTILABEL]\n",
      "                           [--model.hierarchical HIERARCHICAL]\n",
      "                           [--model.task_adapt TASK_ADAPT]\n",
      "                           [--model.track_loss_dynamics {true,false}]\n",
      "                           [--model.head HEAD] [--model.backbone BACKBONE]\n",
      "                           [--model.neck NECK] [--model.pretrained PRETRAINED]\n",
      "                           [--model.train_cfg TRAIN_CFG]\n",
      "                           [--model.data_preprocessor DATA_PREPROCESSOR]\n",
      "                           [--model.init_cfg INIT_CFG] [--model.name NAME]\n",
      "                           [--data CONFIG] [--data.task TASK]\n",
      "                           [--data.train_type TRAIN_TYPE]\n",
      "                           [--data.train_data_roots TRAIN_DATA_ROOTS]\n",
      "                           [--data.train_ann_files TRAIN_ANN_FILES]\n",
      "                           [--data.val_data_roots VAL_DATA_ROOTS]\n",
      "                           [--data.val_ann_files VAL_ANN_FILES]\n",
      "                           [--data.test_data_roots TEST_DATA_ROOTS]\n",
      "                           [--data.test_ann_files TEST_ANN_FILES]\n",
      "                           [--data.unlabeled_data_roots UNLABELED_DATA_ROOTS]\n",
      "                           [--data.unlabeled_file_list UNLABELED_FILE_LIST]\n",
      "                           [--data.data_format DATA_FORMAT]\n",
      "                           [--val_dataloader CONFIG]\n",
      "                           [--val_dataloader.self SELF]\n",
      "                           [--val_dataloader.subset SUBSET]\n",
      "                           [--val_dataloader.pipeline PIPELINE]\n",
      "                           [--val_dataloader.config CONFIG]\n",
      "                           [--val_dataloader.batch_size BATCH_SIZE]\n",
      "                           [--val_dataloader.num_workers NUM_WORKERS]\n",
      "                           [--val_dataloader.shuffle {true,false}]\n",
      "                           [--val_dataloader.pin_memory {true,false}]\n",
      "                           [--val_dataloader.drop_last {true,false}]\n",
      "                           [--val_dataloader.sampler.help CLASS_PATH_OR_NAME]\n",
      "                           [--val_dataloader.sampler SAMPLER]\n",
      "                           [--val_dataloader.persistent_workers {true,false}]\n",
      "                           [--val_dataloader.distributed {true,false}]\n",
      "                           [--val_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE]\n",
      "                           [--train_dataloader CONFIG]\n",
      "                           [--train_dataloader.self SELF]\n",
      "                           [--train_dataloader.subset SUBSET]\n",
      "                           [--train_dataloader.pipeline PIPELINE]\n",
      "                           [--train_dataloader.config CONFIG]\n",
      "                           [--train_dataloader.batch_size BATCH_SIZE]\n",
      "                           [--train_dataloader.num_workers NUM_WORKERS]\n",
      "                           [--train_dataloader.shuffle {true,false}]\n",
      "                           [--train_dataloader.pin_memory {true,false}]\n",
      "                           [--train_dataloader.drop_last {true,false}]\n",
      "                           [--train_dataloader.sampler.help CLASS_PATH_OR_NAME]\n",
      "                           [--train_dataloader.sampler SAMPLER]\n",
      "                           [--train_dataloader.persistent_workers {true,false}]\n",
      "                           [--train_dataloader.distributed {true,false}]\n",
      "                           [--train_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE]\n",
      "                           [--checkpoint CHECKPOINT]\n",
      "                           [--optimizer.help CLASS_PATH_OR_NAME]\n",
      "                           [--optimizer OPTIMIZER] [--max_iters MAX_ITERS]\n",
      "                           [--max_epochs MAX_EPOCHS]\n",
      "                           [--distributed {true,false,null}] [--seed SEED]\n",
      "                           [--deterministic {true,false,null}]\n",
      "                           [--precision PRECISION]\n",
      "                           [--val_interval VAL_INTERVAL]\n",
      "                           [--val_evaluator.help CLASS_PATH_OR_NAME]\n",
      "                           [--val_evaluator VAL_EVALUATOR]\n",
      "                           [--param_scheduler.help CLASS_PATH_OR_NAME]\n",
      "                           [--param_scheduler PARAM_SCHEDULER]\n",
      "                           [--default_hooks DEFAULT_HOOKS]\n",
      "                           [--custom_hooks.help CLASS_PATH_OR_NAME]\n",
      "                           [--custom_hooks CUSTOM_HOOKS]\n",
      "                           [--visualizer.help CLASS_PATH_OR_NAME]\n",
      "                           [--visualizer VISUALIZER]\n",
      "\n",
      "Training Functions with the MMEngine Framework.\n",
      "\n",
      "default config file locations:\n",
      "  ['/home/harimkan/workspace/repo/otx-\n",
      "  main/src/otx/v2/configs/classification/otx_mmpretrain_cli.yaml',\n",
      "  '/home/harimkan/workspace/repo/otx-\n",
      "  main/src/otx/v2/configs/classification/models/otx_efficientnet_b0.yaml'],\n",
      "  Note: default values below are the ones overridden by the contents of:\n",
      "  ['/home/harimkan/workspace/repo/otx-\n",
      "  main/src/otx/v2/configs/classification/otx_mmpretrain_cli.yaml',\n",
      "  '/home/harimkan/workspace/repo/otx-\n",
      "  main/src/otx/v2/configs/classification/models/otx_efficientnet_b0.yaml']\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a configuration file in yaml format.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "  -o WORK_DIR, --work_dir WORK_DIR\n",
      "                        Path to store logs and outputs related to the command.\n",
      "                        (type: str, default: null)\n",
      "  --framework FRAMEWORK\n",
      "                        Select Framework: {mmpretrain, anomalib}. (type: str,\n",
      "                        default: mmpretrain)\n",
      "  --model.name NAME     Enter the name of model. (default:\n",
      "                        otx_efficientnet_b0)\n",
      "\n",
      "SAM-enabled ImageClassifier:\n",
      "  --model CONFIG        Path to a configuration file.\n",
      "  --model.multilabel MULTILABEL\n",
      "                        (type: Any, default: False)\n",
      "  --model.hierarchical HIERARCHICAL\n",
      "                        (type: Any, default: False)\n",
      "  --model.task_adapt TASK_ADAPT\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --model.track_loss_dynamics {true,false}\n",
      "                        (type: bool, default: False)\n",
      "  --model.head HEAD     The head module to do prediction and calculate loss\n",
      "                        from processed features. See\n",
      "                        :mod:`mmpretrain.models.heads`. Notice that if the\n",
      "                        head is not set, almost all methods cannot be used\n",
      "                        except :meth:`extract_feat`. Defaults to None. (type:\n",
      "                        Optional[Any], default: {'in_channels': -1, 'loss':\n",
      "                        {'loss_weight': 1.0, 'type': 'CrossEntropyLoss'},\n",
      "                        'num_classes': 1000, 'topk': (1, 5), 'type':\n",
      "                        'CustomLinearClsHead'})\n",
      "  --model.backbone BACKBONE\n",
      "                        The backbone module. See\n",
      "                        :mod:`mmpretrain.models.backbones`. (required, type:\n",
      "                        <class 'dict'>, default: {'pretrained': True, 'type':\n",
      "                        'OTXEfficientNet', 'version': 'b0'})\n",
      "  --model.neck NECK     The neck module to process features from backbone. See\n",
      "                        :mod:`mmpretrain.models.necks`. Defaults to None.\n",
      "                        (type: Optional[dict], default: {'type':\n",
      "                        'GlobalAveragePooling'})\n",
      "  --model.pretrained PRETRAINED\n",
      "                        The pretrained checkpoint path, support local path and\n",
      "                        remote path. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --model.train_cfg TRAIN_CFG\n",
      "                        The training setting. The acceptable fields are: -\n",
      "                        augments (List[dict]): The batch augmentation methods\n",
      "                        to use. More details can be found in\n",
      "                        :mod:`mmpretrain.model.utils.augment`. - probs\n",
      "                        (List[float], optional): The probability of every\n",
      "                        batch augmentation methods. If None, choose evenly.\n",
      "                        Defaults to None. Defaults to None. (type:\n",
      "                        Optional[dict], default: null)\n",
      "  --model.data_preprocessor DATA_PREPROCESSOR\n",
      "                        The config for preprocessing input data. If None or no\n",
      "                        specified type, it will use \"ClsDataPreprocessor\" as\n",
      "                        type. See :class:`ClsDataPreprocessor` for more\n",
      "                        details. Defaults to None. (type: Optional[dict],\n",
      "                        default: {'mean': [123.675, 116.28, 103.53], 'std':\n",
      "                        [58.395, 57.12, 57.375], 'to_rgb': True})\n",
      "  --model.init_cfg INIT_CFG\n",
      "                        the config to control the initialization. Defaults to\n",
      "                        None. (type: Optional[dict], default: null)\n",
      "\n",
      "MMPretrain's Dataset class:\n",
      "  --data CONFIG         Path to a configuration file.\n",
      "  --data.task TASK      The task type of the dataset want to load. Defaults to\n",
      "                        None. (type: Union[TaskType, str, null], default:\n",
      "                        Classification)\n",
      "  --data.train_type TRAIN_TYPE\n",
      "                        The train type of the dataset want to load. Defaults\n",
      "                        to None. (type: Union[TrainType, str, null], default:\n",
      "                        TrainType.Incremental)\n",
      "  --data.train_data_roots TRAIN_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        training. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.train_ann_files TRAIN_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for training. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.val_data_roots VAL_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        validation. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.val_ann_files VAL_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for validation. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.test_data_roots TEST_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        testing. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.test_ann_files TEST_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for testing. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.unlabeled_data_roots UNLABELED_DATA_ROOTS\n",
      "                        The root address of the unlabeled dataset to be used\n",
      "                        for training. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.unlabeled_file_list UNLABELED_FILE_LIST\n",
      "                        The file where the list of unlabeled images is\n",
      "                        declared. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.data_format DATA_FORMAT\n",
      "                        The format of the dataset. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "\n",
      "MMPretrain's Dataset.subset_dataloader:\n",
      "  --val_dataloader CONFIG\n",
      "                        Path to a configuration file.\n",
      "  --val_dataloader.self SELF\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --val_dataloader.subset SUBSET\n",
      "                        Enter an available subset of that dataset. (required,\n",
      "                        type: str, default: val)\n",
      "  --val_dataloader.pipeline PIPELINE, --val_dataloader.pipeline+ PIPELINE\n",
      "                        Dataset Pipeline. Defaults to None. (type:\n",
      "                        Union[List[Union[Dict, Any]], Dict[str,\n",
      "                        List[Union[Dict, Any]]], null], default: null)\n",
      "  --val_dataloader.config CONFIG\n",
      "                        Path to configuration file or Config. Defaults to\n",
      "                        None. (type: Union[str, Dict[str, Any], null],\n",
      "                        default: null)\n",
      "  --val_dataloader.batch_size BATCH_SIZE\n",
      "                        How many samples per batch to load. Defaults to None.\n",
      "                        (type: Optional[int], default: 32)\n",
      "  --val_dataloader.num_workers NUM_WORKERS\n",
      "                        How many subprocesses to use for data loading. ``0``\n",
      "                        means that the data will be loaded in the main\n",
      "                        process. Defaults to None. (type: Optional[int],\n",
      "                        default: 2)\n",
      "  --val_dataloader.shuffle {true,false}\n",
      "                        Set to ``True`` to have the data reshuffled at every\n",
      "                        epoch. Defaults to True. (type: bool, default: True)\n",
      "  --val_dataloader.pin_memory {true,false}\n",
      "                        If ``True``, the data loader will copy Tensors into\n",
      "                        device/CUDA pinned memory before returning them. If\n",
      "                        your data elements are a custom type, or your\n",
      "                        :attr:`collate_fn` returns a batch that is a custom\n",
      "                        type, see the example below. Defaults to False. (type:\n",
      "                        bool, default: False)\n",
      "  --val_dataloader.drop_last {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.sampler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Sampler and\n",
      "                        exit.\n",
      "  --val_dataloader.sampler SAMPLER, --val_dataloader.sampler+ SAMPLER\n",
      "                        Defines the strategy to draw samples from the dataset.\n",
      "                        Can be any ``Iterable`` with ``__len__`` implemented.\n",
      "                        If specified, :attr:`shuffle` must not be specified..\n",
      "                        Defaults to None. (type: Union[Sampler, Iterable,\n",
      "                        Dict, null], default: null, known subclasses:\n",
      "                        torch.utils.data.Sampler,\n",
      "                        torch.utils.data.SequentialSampler,\n",
      "                        torch.utils.data.RandomSampler,\n",
      "                        torch.utils.data.SubsetRandomSampler,\n",
      "                        torch.utils.data.WeightedRandomSampler,\n",
      "                        torch.utils.data.BatchSampler,\n",
      "                        torch.utils.data.DistributedSampler,\n",
      "                        mmengine.dataset.DefaultSampler,\n",
      "                        mmpretrain.datasets.SequentialSampler,\n",
      "                        mmengine.dataset.InfiniteSampler,\n",
      "                        mmpretrain.datasets.RepeatAugSampler, otx.v2.adapters.\n",
      "                        torch.modules.dataloaders.samplers.BalancedSampler, ot\n",
      "                        x.v2.adapters.torch.modules.dataloaders.samplers.ClsIn\n",
      "                        crSampler, timm.data.distributed_sampler.OrderedDistri\n",
      "                        butedSampler,\n",
      "                        timm.data.distributed_sampler.RepeatAugSampler)\n",
      "  --val_dataloader.persistent_workers {true,false}\n",
      "                        If ``True``, the data loader will not shutdown the\n",
      "                        worker processes after a dataset has been consumed\n",
      "                        once. This allows to maintain the workers `Dataset`\n",
      "                        instances alive. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.distributed {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE\n",
      "                        (type: Optional[Any], default: null)\n",
      "\n",
      "MMPretrain's Dataset.subset_dataloader:\n",
      "  --train_dataloader CONFIG\n",
      "                        Path to a configuration file.\n",
      "  --train_dataloader.self SELF\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --train_dataloader.subset SUBSET\n",
      "                        Enter an available subset of that dataset. (required,\n",
      "                        type: str, default: train)\n",
      "  --train_dataloader.pipeline PIPELINE, --train_dataloader.pipeline+ PIPELINE\n",
      "                        Dataset Pipeline. Defaults to None. (type:\n",
      "                        Union[List[Union[Dict, Any]], Dict[str,\n",
      "                        List[Union[Dict, Any]]], null], default: null)\n",
      "  --train_dataloader.config CONFIG\n",
      "                        Path to configuration file or Config. Defaults to\n",
      "                        None. (type: Union[str, Dict[str, Any], null],\n",
      "                        default: null)\n",
      "  --train_dataloader.batch_size BATCH_SIZE\n",
      "                        How many samples per batch to load. Defaults to None.\n",
      "                        (type: Optional[int], default: 32)\n",
      "  --train_dataloader.num_workers NUM_WORKERS\n",
      "                        How many subprocesses to use for data loading. ``0``\n",
      "                        means that the data will be loaded in the main\n",
      "                        process. Defaults to None. (type: Optional[int],\n",
      "                        default: 2)\n",
      "  --train_dataloader.shuffle {true,false}\n",
      "                        Set to ``True`` to have the data reshuffled at every\n",
      "                        epoch. Defaults to True. (type: bool, default: True)\n",
      "  --train_dataloader.pin_memory {true,false}\n",
      "                        If ``True``, the data loader will copy Tensors into\n",
      "                        device/CUDA pinned memory before returning them. If\n",
      "                        your data elements are a custom type, or your\n",
      "                        :attr:`collate_fn` returns a batch that is a custom\n",
      "                        type, see the example below. Defaults to False. (type:\n",
      "                        bool, default: False)\n",
      "  --train_dataloader.drop_last {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.sampler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Sampler and\n",
      "                        exit.\n",
      "  --train_dataloader.sampler SAMPLER, --train_dataloader.sampler+ SAMPLER\n",
      "                        Defines the strategy to draw samples from the dataset.\n",
      "                        Can be any ``Iterable`` with ``__len__`` implemented.\n",
      "                        If specified, :attr:`shuffle` must not be specified..\n",
      "                        Defaults to None. (type: Union[Sampler, Iterable,\n",
      "                        Dict, null], default: null, known subclasses:\n",
      "                        torch.utils.data.Sampler,\n",
      "                        torch.utils.data.SequentialSampler,\n",
      "                        torch.utils.data.RandomSampler,\n",
      "                        torch.utils.data.SubsetRandomSampler,\n",
      "                        torch.utils.data.WeightedRandomSampler,\n",
      "                        torch.utils.data.BatchSampler,\n",
      "                        torch.utils.data.DistributedSampler,\n",
      "                        mmengine.dataset.DefaultSampler,\n",
      "                        mmpretrain.datasets.SequentialSampler,\n",
      "                        mmengine.dataset.InfiniteSampler,\n",
      "                        mmpretrain.datasets.RepeatAugSampler, otx.v2.adapters.\n",
      "                        torch.modules.dataloaders.samplers.BalancedSampler, ot\n",
      "                        x.v2.adapters.torch.modules.dataloaders.samplers.ClsIn\n",
      "                        crSampler, timm.data.distributed_sampler.OrderedDistri\n",
      "                        butedSampler,\n",
      "                        timm.data.distributed_sampler.RepeatAugSampler)\n",
      "  --train_dataloader.persistent_workers {true,false}\n",
      "                        If ``True``, the data loader will not shutdown the\n",
      "                        worker processes after a dataset has been consumed\n",
      "                        once. This allows to maintain the workers `Dataset`\n",
      "                        instances alive. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.distributed {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE\n",
      "                        (type: Optional[Any], default: null)\n",
      "\n",
      "Training Functions with the MMEngine Framework:\n",
      "  --checkpoint CHECKPOINT\n",
      "                        (type: Union[str, Path, null], default: null)\n",
      "  --optimizer.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Optimizer and\n",
      "                        exit.\n",
      "  --optimizer OPTIMIZER\n",
      "                        _description_. Defaults to None. (type: Union[dict,\n",
      "                        Optimizer, null], default: {'type': 'SGD', 'lr': 0.01,\n",
      "                        'momentum': 0.9, 'weight_decay': 0.0005}, known\n",
      "                        subclasses: torch.optim.Optimizer,\n",
      "                        torch.optim.Adadelta, torch.optim.Adagrad,\n",
      "                        torch.optim.Adam, torch.optim.AdamW,\n",
      "                        torch.optim.Adamax, torch.optim.ASGD,\n",
      "                        torch.optim.NAdam, torch.optim.RAdam,\n",
      "                        torch.optim.RMSprop, torch.optim.Rprop,\n",
      "                        torch.optim.SGD, torch.optim.SparseAdam,\n",
      "                        torch.optim.LBFGS,\n",
      "                        torch.distributed.optim.PostLocalSGDOptimizer,\n",
      "                        torch.distributed.optim.ZeroRedundancyOptimizer,\n",
      "                        mmengine.optim.ZeroRedundancyOptimizer,\n",
      "                        mmpretrain.engine.Adan, mmpretrain.engine.Lamb,\n",
      "                        mmpretrain.engine.LARS, otx.v2.adapters.torch.mmengine\n",
      "                        .mmpretrain.modules.LARS)\n",
      "  --max_iters MAX_ITERS\n",
      "                        Specifies the maximum iters of training. Defaults to\n",
      "                        None. (type: Optional[int], default: null)\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Specifies the maximum epoch of training. Defaults to\n",
      "                        None. (type: Optional[int], default: 10)\n",
      "  --distributed {true,false,null}\n",
      "                        Whether to use the distributed setting. Defaults to\n",
      "                        None. (type: Optional[bool], default: null)\n",
      "  --seed SEED           The seed to use for training. Defaults to None. (type:\n",
      "                        Optional[int], default: 1234)\n",
      "  --deterministic {true,false,null}\n",
      "                        The deterministic to use for training. Defaults to\n",
      "                        None. (type: Optional[bool], default: False)\n",
      "  --precision PRECISION\n",
      "                        The precision to use for training. Defaults to None.\n",
      "                        (type: Optional[str], default: float32)\n",
      "  --val_interval VAL_INTERVAL\n",
      "                        Specifies the validation Interval. Defaults to None.\n",
      "                        (type: Optional[int], default: 1)\n",
      "  --val_evaluator.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Evaluator and\n",
      "                        exit.\n",
      "  --val_evaluator VAL_EVALUATOR, --val_evaluator+ VAL_EVALUATOR\n",
      "                        A evaluator object used for computing metrics for\n",
      "                        validation. It can be a dict or a list of dict to\n",
      "                        build a evaluator. If specified,\n",
      "                        :attr:`val_dataloader` should also be specified.\n",
      "                        Defaults to None. (type: Union[Evaluator, Dict, List,\n",
      "                        null], default: [{'type': 'Accuracy', 'topk': (1,\n",
      "                        5)}], known subclasses: mmengine.evaluator.Evaluator)\n",
      "  --param_scheduler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of\n",
      "                        _ParamScheduler and exit.\n",
      "  --param_scheduler PARAM_SCHEDULER, --param_scheduler+ PARAM_SCHEDULER\n",
      "                        (type: Union[_ParamScheduler, Dict, List, null],\n",
      "                        default: {'type': 'CosineAnnealingLR'}, known\n",
      "                        subclasses: mmengine.optim.StepParamScheduler,\n",
      "                        mmengine.optim.StepLR, mmengine.optim.StepMomentum,\n",
      "                        mmengine.optim.MultiStepParamScheduler,\n",
      "                        mmengine.optim.MultiStepLR,\n",
      "                        mmengine.optim.MultiStepMomentum,\n",
      "                        mmengine.optim.ConstantParamScheduler,\n",
      "                        mmengine.optim.ConstantLR,\n",
      "                        mmengine.optim.ConstantMomentum,\n",
      "                        mmengine.optim.ExponentialParamScheduler,\n",
      "                        mmengine.optim.ExponentialLR,\n",
      "                        mmengine.optim.ExponentialMomentum,\n",
      "                        mmengine.optim.CosineAnnealingParamScheduler,\n",
      "                        mmengine.optim.CosineAnnealingLR,\n",
      "                        mmengine.optim.CosineAnnealingMomentum,\n",
      "                        mmpretrain.engine.CosineAnnealingWeightDecay,\n",
      "                        mmengine.optim.LinearParamScheduler,\n",
      "                        mmengine.optim.LinearLR,\n",
      "                        mmengine.optim.LinearMomentum,\n",
      "                        mmengine.optim.PolyParamScheduler,\n",
      "                        mmengine.optim.PolyLR, mmengine.optim.PolyMomentum,\n",
      "                        mmengine.optim.OneCycleParamScheduler,\n",
      "                        mmengine.optim.OneCycleLR,\n",
      "                        mmengine.optim.scheduler.CosineRestartParamScheduler,\n",
      "                        mmengine.optim.scheduler.CosineRestartLR,\n",
      "                        mmengine.optim.scheduler.CosineRestartMomentum,\n",
      "                        mmengine.optim.ReduceOnPlateauParamScheduler,\n",
      "                        mmengine.optim.ReduceOnPlateauLR,\n",
      "                        mmengine.optim.ReduceOnPlateauMomentum)\n",
      "  --default_hooks DEFAULT_HOOKS\n",
      "                        Hooks to execute default actions like updating model\n",
      "                        parameters and saving checkpoints. Default hooks are\n",
      "                        ``OptimizerHook``, ``IterTimerHook``, ``LoggerHook``,\n",
      "                        ``ParamSchedulerHook`` and ``CheckpointHook``.\n",
      "                        Defaults to None. See :meth:`register_default_hooks`\n",
      "                        for more details. (type: Optional[Dict[str,\n",
      "                        Union[Hook, Dict[str, Any]]]], default: {'logger':\n",
      "                        {'type': 'LoggerHook', 'interval': 100}, 'timer':\n",
      "                        {'type': 'IterTimerHook'}, 'checkpoint': {'interval':\n",
      "                        1, 'max_keep_ckpts': 1, 'save_best': 'auto', 'type':\n",
      "                        'CheckpointHook'}})\n",
      "  --custom_hooks.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Hook and exit.\n",
      "  --custom_hooks CUSTOM_HOOKS, --custom_hooks+ CUSTOM_HOOKS\n",
      "                        Hooks to execute custom actions like visualizing\n",
      "                        images processed by pipeline. Defaults to None. (type:\n",
      "                        Union[List, Dict, Hook, null], default: null, known\n",
      "                        subclasses: mmengine.hooks.Hook,\n",
      "                        mmengine.hooks.CheckpointHook,\n",
      "                        mmengine.hooks.EarlyStoppingHook, otx.v2.adapters.torc\n",
      "                        h.mmengine.modules.LazyEarlyStoppingHook,\n",
      "                        mmengine.hooks.EMAHook, mmpretrain.engine.EMAHook, otx\n",
      "                        .v2.adapters.torch.mmengine.modules.CustomModelEMAHook\n",
      "                        , mmengine.hooks.EmptyCacheHook,\n",
      "                        mmengine.hooks.IterTimerHook,\n",
      "                        mmengine.hooks.LoggerHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.OTXLoggerHook,\n",
      "                        mmengine.hooks.NaiveVisualizationHook,\n",
      "                        mmengine.hooks.ParamSchedulerHook, otx.v2.adapters.tor\n",
      "                        ch.mmengine.modules.ReduceLROnPlateauLrUpdaterHook,\n",
      "                        mmengine.hooks.ProfilerHook,\n",
      "                        mmengine.hooks.NPUProfilerHook,\n",
      "                        mmengine.hooks.RuntimeInfoHook,\n",
      "                        mmengine.hooks.DistSamplerSeedHook,\n",
      "                        mmengine.hooks.SyncBuffersHook,\n",
      "                        mmengine.hooks.PrepareTTAHook,\n",
      "                        mmpretrain.engine.ClassNumCheckHook,\n",
      "                        mmpretrain.engine.DenseCLHook, mmcv.cnn.RFSearchHook,\n",
      "                        mmpretrain.engine.SetAdaptiveMarginsHook,\n",
      "                        mmpretrain.engine.PreciseBNHook,\n",
      "                        mmpretrain.engine.PrepareProtoBeforeValLoopHook,\n",
      "                        mmpretrain.engine.SimSiamHook,\n",
      "                        mmpretrain.engine.SwAVHook,\n",
      "                        mmpretrain.engine.SwitchRecipeHook,\n",
      "                        mmpretrain.engine.VisualizationHook,\n",
      "                        mmpretrain.engine.WarmupParamHook, otx.v2.adapters.tor\n",
      "                        ch.mmengine.modules.StopLossNanTrainingHook, otx.v2.ad\n",
      "                        apters.torch.mmengine.modules.AdaptiveTrainSchedulingH\n",
      "                        ook, otx.v2.adapters.torch.mmengine.modules.CancelTrai\n",
      "                        ningHook, otx.v2.adapters.torch.mmengine.modules.Cance\n",
      "                        lInterfaceHook, otx.v2.adapters.torch.mmengine.modules\n",
      "                        .CheckpointHookWithValResults, otx.v2.adapters.torch.m\n",
      "                        mengine.modules.EnsureCorrectBestCheckpointHook, otx.v\n",
      "                        2.adapters.torch.mmengine.modules.SaveInitialWeightHoo\n",
      "                        k, otx.v2.adapters.torch.mmengine.modules.ComposedData\n",
      "                        LoadersHook, otx.v2.adapters.torch.mmengine.modules.EM\n",
      "                        AMomentumUpdateHook, otx.v2.adapters.torch.mmengine.mo\n",
      "                        dules.DualModelEMAHook, otx.v2.adapters.torch.mmengine\n",
      "                        .modules.UnbiasedTeacherHook, otx.v2.adapters.torch.mm\n",
      "                        engine.modules.ForceTrainModeHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.IBLossHook, otx\n",
      "                        .v2.adapters.torch.mmengine.modules.LoggerReplaceHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.LossDynamicsTra\n",
      "                        ckingHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.MemCacheHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.ModelEmaV2Hook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.NoBiasDecayHook\n",
      "                        , otx.v2.adapters.torch.mmengine.modules.OTXProgressHo\n",
      "                        ok,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.SemiSLClsHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.TaskAdaptHook, \n",
      "                        otx.v2.adapters.torch.mmengine.modules.TwoCropTransfor\n",
      "                        mHook)\n",
      "  --visualizer.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Visualizer and\n",
      "                        exit.\n",
      "  --visualizer VISUALIZER\n",
      "                        A Visualizer object or a dict build Visualizer object.\n",
      "                        Defaults to None. If not specified, default config\n",
      "                        will be used. (type: Union[Visualizer, Dict, null],\n",
      "                        default: {'type': 'UniversalVisualizer',\n",
      "                        'vis_backends': [{'type': 'LocalVisBackend'}, {'type':\n",
      "                        'TensorboardVisBackend'}]}, known subclasses:\n",
      "                        mmengine.visualization.Visualizer,\n",
      "                        mmpretrain.visualization.UniversalVisualizer)\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. otx [subcommand] [args] --print_config\n",
    "- This allows the user to see the configuration used by the command before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:02,105 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:02,128 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:02,193 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "work_dir: null\n",
      "framework: mmpretrain\n",
      "model:\n",
      "  multilabel: false\n",
      "  hierarchical: false\n",
      "  task_adapt: null\n",
      "  track_loss_dynamics: false\n",
      "  head:\n",
      "    in_channels: -1\n",
      "    loss:\n",
      "      loss_weight: 1.0\n",
      "      type: CrossEntropyLoss\n",
      "    num_classes: 1000\n",
      "    topk:\n",
      "    - 1\n",
      "    - 5\n",
      "    type: CustomLinearClsHead\n",
      "  backbone:\n",
      "    pretrained: true\n",
      "    type: OTXEfficientNet\n",
      "    version: b0\n",
      "  neck:\n",
      "    type: GlobalAveragePooling\n",
      "  pretrained: null\n",
      "  train_cfg: null\n",
      "  data_preprocessor:\n",
      "    mean:\n",
      "    - 123.675\n",
      "    - 116.28\n",
      "    - 103.53\n",
      "    std:\n",
      "    - 58.395\n",
      "    - 57.12\n",
      "    - 57.375\n",
      "    to_rgb: true\n",
      "  init_cfg: null\n",
      "  name: otx_efficientnet_b0\n",
      "  type: CustomImageClassifier\n",
      "data:\n",
      "  task: Classification\n",
      "  train_type: Incremental\n",
      "  train_data_roots: ../../../../tests/assets/classification_dataset_class_incremental\n",
      "  train_ann_files: null\n",
      "  val_data_roots: null\n",
      "  val_ann_files: null\n",
      "  test_data_roots: null\n",
      "  test_ann_files: null\n",
      "  unlabeled_data_roots: null\n",
      "  unlabeled_file_list: null\n",
      "  data_format: null\n",
      "val_dataloader:\n",
      "  self: null\n",
      "  subset: val\n",
      "  pipeline: null\n",
      "  config: null\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  shuffle: true\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "  sampler: null\n",
      "  persistent_workers: false\n",
      "  distributed: false\n",
      "  unlabeled_batch_size: null\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - type: PackInputs\n",
      "train_dataloader:\n",
      "  self: null\n",
      "  subset: train\n",
      "  pipeline: null\n",
      "  config: null\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  shuffle: true\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "  sampler: null\n",
      "  persistent_workers: false\n",
      "  distributed: false\n",
      "  unlabeled_batch_size: null\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - direction: horizontal\n",
      "      prob: 0.5\n",
      "      type: RandomFlip\n",
      "    - config_str: augmix-m5-w3-d1\n",
      "      type: AugMixAugment\n",
      "    - angle:\n",
      "      - -10\n",
      "      - 10\n",
      "      p: 0.35\n",
      "      type: RandomRotate\n",
      "    - type: PackInputs\n",
      "checkpoint: null\n",
      "optimizer:\n",
      "  type: SGD\n",
      "  lr: 0.01\n",
      "  momentum: 0.9\n",
      "  weight_decay: 0.0005\n",
      "max_iters: null\n",
      "max_epochs: 10\n",
      "distributed: null\n",
      "seed: 1234\n",
      "deterministic: false\n",
      "precision: float32\n",
      "val_interval: 1\n",
      "val_evaluator:\n",
      "- type: Accuracy\n",
      "  topk:\n",
      "  - 1\n",
      "  - 5\n",
      "param_scheduler:\n",
      "  type: CosineAnnealingLR\n",
      "default_hooks:\n",
      "  logger:\n",
      "    type: LoggerHook\n",
      "    interval: 100\n",
      "  timer:\n",
      "    type: IterTimerHook\n",
      "  checkpoint:\n",
      "    interval: 1\n",
      "    max_keep_ckpts: 1\n",
      "    save_best: auto\n",
      "    type: CheckpointHook\n",
      "custom_hooks: null\n",
      "visualizer:\n",
      "  type: UniversalVisualizer\n",
      "  vis_backends:\n",
      "  - type: LocalVisBackend\n",
      "  - type: TensorboardVisBackend\n",
      "test_dataloader:\n",
      "  batch_size: 32\n",
      "  num_workers: 2\n",
      "  dataset:\n",
      "    type: OTXClsDataset\n",
      "    pipeline:\n",
      "    - scale: 224\n",
      "      type: Resize\n",
      "    - type: PackInputs\n",
      "test_evaluator:\n",
      "- type: Accuracy\n",
      "  topk:\n",
      "  - 1\n",
      "  - 5\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --print_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. otx train --[args]\n",
    "- Proceed with the training using the prepared commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:05,172 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:05,195 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:05,261 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:05,444 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:05,462 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:05,526 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1\n",
      "2023-08-22 14:45:05,528 | INFO : Try to create a 0 size memory pool.\n",
      "08/22 14:45:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1234\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1234\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 14:45:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 14:45:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 14:45:06,593 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 14:45:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 14:45:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "08/22 14:45:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-demo-1.\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144505\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 1.0686  data_time: 0.0588  memory: 3602  loss: 1.1976\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144505\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1/1]  lr: 7.5000e-03  eta: 0:00:00  time: 0.5956  data_time: 0.0618  memory: 3628  loss: 1.0826\n",
      "08/22 14:45:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "08/22 14:45:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144505\n",
      "08/22 14:45:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1/1]  lr: 2.5000e-03  eta: 0:00:00  time: 0.4377  data_time: 0.0627  memory: 3628  loss: 0.9388\n",
      "08/22 14:45:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "/tmp/otx-demo-1/epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "! otx train --data.train_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --max_epochs 3 -o /tmp/otx-demo-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. otx test --[args]\n",
    "- Proceed with the testing using the prepared commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:11,517 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:11,540 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:11,606 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:11,795 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:11,814 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:11,878 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace\n",
      "2023-08-22 14:45:11,880 | INFO : Try to create a 0 size memory pool.\n",
      "08/22 14:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1028809869\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1028809869\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 14:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 14:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: /tmp/otx-demo-1/epoch_3.pth\n",
      "08/22 14:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from /tmp/otx-demo-1/epoch_3.pth\n",
      "2023-08-22 14:45:12,961 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 14:45:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [1/1]    accuracy/top1: 40.6250  data_time: 0.0534  time: 0.9591\n",
      "{'accuracy/top1': 40.625}\n"
     ]
    }
   ],
   "source": [
    "! otx test --data.test_data_roots  ../../../../tests/assets/classification_dataset_class_incremental --checkpoint /tmp/otx-demo-1/epoch_3.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. otx predict --img [img_path] --checkpoint [checkpoint_path]\n",
    "- Use the trained model to make predictions about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:17,332 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:17,360 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:17,430 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:17,617 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:17,638 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:17,707 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace\n",
      "Loads checkpoint by local backend from path: /tmp/otx-demo-1/epoch_3.pth\n",
      "\u001b[?25lInference \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h[<DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    scale_factor: (9.333333333333334, 9.333333333333334)\n",
      "    num_classes: 3\n",
      "    img_shape: (224, 224)\n",
      "    ori_shape: (24, 24)\n",
      "\n",
      "DATA FIELDS\n",
      "    pred_label: tensor([2])\n",
      "    pred_score: tensor([0.2781, 0.3547, 0.3672])\n",
      "\n",
      ") at 0x7fc508eddd30>]\n"
     ]
    }
   ],
   "source": [
    "! otx predict --img ../../../../tests/assets/classification_dataset_class_incremental/2/22.jpg --checkpoint /tmp/otx-demo-1/epoch_3.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. otx export --checkpoint [checkpoint_path] --[args]\n",
    "- This can be used to export trained TORCH models to other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:20,680 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:20,703 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:20,769 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:20,949 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:20,968 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:21,033 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace\n",
      "08/22 14:45:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "08/22 14:45:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "2023-08-22 14:45:21,726 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "08/22 14:45:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.onnx.\n",
      "08/22 14:45:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Args for Model Optimizer: mo --input_model=\"/home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.onnx\" --output_dir=\"/home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/\" --output=\"output\" --input=\"input\" --input_shape=\"[1, 3, 224, 224]\" --mean_values=\"[123.675, 116.28, 103.53]\" --scale_values=\"[58.395, 57.12, 57.375]\" \n",
      "08/22 14:45:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - [ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.xml\n",
      "[ SUCCESS ] BIN file: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.bin\n",
      "\n",
      "08/22 14:45:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Successfully exported OpenVINO model: /home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.xml\n",
      "{'outputs': {'bin': '/home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.bin', 'xml': '/home/harimkan/workspace/repo/otx-main/src/otx/v2/demo/otx-workspace/openvino.xml'}}\n"
     ]
    }
   ],
   "source": [
    "! otx export --checkpoint /tmp/otx-demo-1/epoch_3.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. otx train --config [output_config_file_path]\n",
    "- This gives users the ability to reuse the configs file output from the training to reproduce the same training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "/home/harimkan/workspace/repo/otx-main/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "2023-08-22 14:45:28,071 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:28,094 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:28,160 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:28,349 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-08-22 14:45:28,367 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-08-22 14:45:28,432 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "[*] Workspace Path: /tmp/otx-demo-1-copy\n",
      "2023-08-22 14:45:28,436 | INFO : Try to create a 0 size memory pool.\n",
      "2023-08-22 14:45:28,436 | WARNING : Currently, OTX does not accept val_dataloader as a dict configuration.\n",
      "2023-08-22 14:45:28,436 | WARNING : Currently, OTX does not accept test_dataloader as a dict configuration.\n",
      "2023-08-22 14:45:28,437 | WARNING : In Engine.config, remove ['data', 'checkpoint', 'optimizer', 'max_iters', 'max_epochs', 'distributed', 'precision', 'val_interval', 'framework'] that are unavailable to the Runner.\n",
      "08/22 14:45:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1234\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1234\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/22 14:45:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/22 14:45:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-08-22 14:45:29,490 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "08/22 14:45:29 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/22 14:45:29 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "08/22 14:45:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-demo-1-copy.\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144528\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 1.0690  data_time: 0.0602  memory: 3602  loss: 1.1976\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144528\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1/1]  lr: 7.5000e-03  eta: 0:00:00  time: 0.5959  data_time: 0.0625  memory: 3628  loss: 1.0827\n",
      "08/22 14:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "08/22 14:45:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: otx_train_20230822_144528\n",
      "08/22 14:45:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1/1]  lr: 2.5000e-03  eta: 0:00:00  time: 0.4376  data_time: 0.0628  memory: 3628  loss: 0.9387\n",
      "08/22 14:45:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "/tmp/otx-demo-1-copy/epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "! otx train --config /tmp/otx-demo-1/configs.yaml -o /tmp/otx-demo-1-copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
