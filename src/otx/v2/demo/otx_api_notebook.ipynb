{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTX API DEMO (MMCLS Example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization Training API\n",
    "\n",
    "Select a framework & import adapter modules.\n",
    "\n",
    "We'll choose MMCLS here, and we'll import the following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harimkan/workspace/repo/otx-fork-3/venv-origin/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/harimkan/workspace/repo/otx-fork-3/venv-origin/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/home/harimkan/workspace/repo/otx-fork-3/venv-origin/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmcv.mmcls import Dataset, build_model_from_config\n",
    "from otx.v2.adapters.torch.mmcv.mmcls.engine import MMCLSEngine  # TODO: Import structure that needs to be changed\n",
    "# or from otx.adapters.torch.mmcv.mmcls import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset & DataLoader\n",
    "1. Prepare a dataset and enter path into Dataset\n",
    "\n",
    "    - Convert to OTX's DatasetEntity and Label Schema by leveraging Datumaro's features through paths (path -> Datumaro -> OTX DatasetEntity & LabelSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    train_data_roots=\"../../../../tests/assets/classification_dataset_class_incremental\",\n",
    "    val_data_roots=\"../../../../tests/assets/classification_dataset_class_incremental\",\n",
    "    test_data_roots=\"../../../../tests/assets/classification_dataset_class_incremental\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. Build Torch Dataset from MMCV config (filepath or dict) -> torch.utils.data.Dataset\n",
    "\n",
    "    - User can build a dataset from a config file or dictionary used by MMCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 32\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = dataset.train_dataloader()\n",
    "print(f\"Dataset type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 16\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Customize batch_size\n",
    "train_dataloader = dataset.train_dataloader(batch_size=2)\n",
    "print(f\"DataLoader type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {train_dataloader.dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 32\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# With Config File\n",
    "data_config = \"/home/harimkan/workspace/repo/otx-fork-3/src/otx/algorithms/classification/configs/efficientnet_b0_cls_incr/data_pipeline.py\"\n",
    "\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    config=data_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(f\"DataLoader type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {train_dataloader.dataset.num_classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2. Or Build a custom dataset using mmcv's Data pipeline API -> torch.utils.data.Dataset\n",
    "\n",
    "    - User can build the data pipeline by configuring it using mmcv's API, without using Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 32\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Customize mmcls pipeline\n",
    "# Build using pipeline Object\n",
    "from mmcls.datasets.pipelines import Resize, Normalize, ImageToTensor, Collect, ToTensor\n",
    "data_pipeline = [\n",
    "    Resize(size=224),\n",
    "    Normalize(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    ImageToTensor(keys=[\"img\"]),\n",
    "    ToTensor(keys=[\"gt_label\"]),\n",
    "    Collect(keys=[\"img\", \"gt_label\"])\n",
    "]\n",
    "train_dataloader_2 = dataset.train_dataloader(\n",
    "    pipeline=data_pipeline\n",
    ")\n",
    "print(f\"DataLoader type: {type(train_dataloader_2)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader_2)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader_2.dataset)}\")\n",
    "print(f\"Number of classes: {train_dataloader_2.dataset.num_classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "Config to build the torch.nn.Module model. -> Provide function for building models so that each framework's config can be used\n",
    "\n",
    "    - Users can build a torch.nn.Module via config as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:26:35,684 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-05 13:26:35,708 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-07-05 13:26:35,767 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'otx.v2.adapters.torch.mmcv.mmcls.modules.models.classifiers.sam_classifier.SAMImageClassifier'>\n",
      "{'loss': tensor(0.8474, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8473533987998962)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1415, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1415321826934814)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8870, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.887002170085907)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1295, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1294714212417603)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8797, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8797056078910828)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9411, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.9410520195960999)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1448, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1448103189468384)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8795, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8795167803764343)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9007, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.9006833434104919)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8884, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8884137868881226)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.3266, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.32662832736969)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.3307, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.3306725025177002)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9228, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.922768235206604)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1522, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1521600484848022)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1421, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.14211905002594)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1122, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.112216830253601)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1426, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1426018476486206)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.0976, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.0975959300994873)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1407, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1406757831573486)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9116, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.911604642868042)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8962, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8961973190307617)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.2801, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.280123233795166)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8799, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.8799305558204651)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.3320, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.3320403099060059)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.2992, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.2992209196090698)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.3247, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.3246738910675049)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.8962, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.89622563123703)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.3394, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.339413046836853)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9095, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.9094846844673157)]), 'num_samples': 1}\n",
      "{'loss': tensor(0.9195, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 0.9194749593734741)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1394, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1393535137176514)]), 'num_samples': 1}\n",
      "{'loss': tensor(1.1849, grad_fn=<AddBackward0>), 'log_vars': OrderedDict([('loss', 1.1849019527435303)]), 'num_samples': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build from config file\n",
    "model = build_model_from_config(\n",
    "    config=\"/home/harimkan/workspace/repo/otx-fork-3/src/otx/v2/configs/classification/mmcls_efficientnet_b0.yaml\",\n",
    "    num_classes=dataset.num_classes\n",
    ")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "for i, data_batch in enumerate(train_dataloader):\n",
    "    output = model.train_step(data_batch)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:26:45,819 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "Model type: <class 'otx.v2.adapters.torch.mmcv.mmcls.modules.models.classifiers.sam_classifier.SAMImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmcv.mmcls.modules.models import SAMImageClassifier\n",
    "model_2 = SAMImageClassifier(\n",
    "    backbone=dict(type='otx.OTXEfficientNet', pretrained=True, version='b0'),\n",
    "    neck=dict(type='GlobalAveragePooling'),\n",
    "    head=dict(\n",
    "        type='CustomLinearClsHead',\n",
    "        num_classes=train_dataloader.dataset.num_classes,\n",
    "        in_channels=1280,\n",
    "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
    "        topk=(1,)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Users can use each framework's training provided by OTX. (Engine)\n",
    "\n",
    "- The engine requires the necessary models and DataLoaders for each framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:26:49,484 | INFO : Start running, host: harimkan@harimkang-desktop, work_dir: /home/harimkan/workspace/repo/otx-fork/otx-workspace-test\n",
      "2023-07-05 13:26:49,485 | INFO : Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "2023-07-05 13:26:49,486 | INFO : workflow: [('train', 1)], max: 2 epochs\n",
      "2023-07-05 13:26:49,486 | INFO : Checkpoints will be saved to /home/harimkan/workspace/repo/otx-fork/otx-workspace-test by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:26:50,207 | INFO : Saving checkpoint at 1 epochs\n",
      "2023-07-05 13:26:50,968 | INFO : Saving checkpoint at 2 epochs\n",
      "/home/harimkan/workspace/repo/otx-fork/otx-workspace-test/latest.pth\n"
     ]
    }
   ],
   "source": [
    "# Use Custom Hook & other things -> mmX Runner\n",
    "mmcls_engine = MMCLSEngine()\n",
    "\n",
    "output_weight_path = mmcls_engine.train(\n",
    "    model=model,  # torch.nn.Module\n",
    "    train_dataloader=train_dataloader,  # torch.utils.data.DataLoader\n",
    "    work_dir=\"/home/harimkan/workspace/repo/otx-fork/otx-workspace-test\",\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "print(output_weight_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTX AutoEngine (Automation Training API)\n",
    "OTX provides a more convenient API called AutoEngine.\n",
    "\n",
    "- It's more convenient for users to use Engine, which provides auto-configuration and the features provided by OTX without having to choose a framework.\n",
    "- Prepare Dataset & DataLoader + Prepare Model + OTX Recipes + Training + ETC.\n",
    "- This will make all of the above steps happen automatically. (Auto: Model Selection & build, Dataset Configuration, Training, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:27:30,807 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-05 13:27:30,826 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-07-05 13:27:30,884 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-05 13:27:30,928 | INFO : Start running, host: harimkan@harimkang-desktop, work_dir: /tmp/OTX-API-test\n",
      "2023-07-05 13:27:30,929 | INFO : Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "2023-07-05 13:27:30,930 | INFO : workflow: [('train', 1)], max: 2 epochs\n",
      "2023-07-05 13:27:30,930 | INFO : Checkpoints will be saved to /tmp/OTX-API-test by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:27:31,607 | INFO : Saving checkpoint at 1 epochs\n",
      "2023-07-05 13:27:32,308 | INFO : Saving checkpoint at 2 epochs\n",
      "/tmp/OTX-API-test/latest.pth\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.api.core.engine import AutoEngine\n",
    "\n",
    "output_dir = \"/tmp/OTX-API-test\"\n",
    "data_roots = \"/home/harimkan/workspace/repo/otx-fork-3/tests/assets/classification_dataset_class_incremental\"\n",
    "default_config_path = \"/home/harimkan/workspace/repo/otx-fork-3/src/otx/v2/configs/classification/mmcls_efficientnet_b0.yaml\"\n",
    "\n",
    "engine = AutoEngine(\n",
    "    work_dir=output_dir,\n",
    "    train_data_roots=data_roots,\n",
    "    config=default_config_path,\n",
    ")\n",
    "\n",
    "# Auto-mation training\n",
    "# output_weight_path = engine.train()\n",
    "# Customization training\n",
    "output_weight_path = engine.train(batch_size=2, max_epochs=2)\n",
    "print(output_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:27:35,532 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-05 13:27:35,551 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-07-05 13:27:35,609 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-05 13:27:35,660 | INFO : Start running, host: harimkan@harimkang-desktop, work_dir: /tmp/OTX-API-test\n",
      "2023-07-05 13:27:35,661 | INFO : Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(ABOVE_NORMAL) CustomEvalHook                     \n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(ABOVE_NORMAL) CustomEvalHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(ABOVE_NORMAL) CustomEvalHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) CustomEvalHook                     \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(ABOVE_NORMAL) CustomEvalHook                     \n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "2023-07-05 13:27:35,661 | INFO : workflow: [('train', 1)], max: 2 epochs\n",
      "2023-07-05 13:27:35,662 | INFO : Checkpoints will be saved to /tmp/OTX-API-test by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harimkan/workspace/repo/otx-fork-3/venv-origin/lib/python3.9/site-packages/mmcv/runner/hooks/evaluation.py:226: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 32/32, 82.2 task/s, elapsed: 0s, ETA:     0s\n",
      "2023-07-05 13:27:36,724 | INFO : Saving checkpoint at 1 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 32/32, 83.2 task/s, elapsed: 0s, ETA:     0s\n",
      "2023-07-05 13:27:37,819 | INFO : Saving checkpoint at 2 epochs\n",
      "/tmp/OTX-API-test/latest.pth\n"
     ]
    }
   ],
   "source": [
    "# Add validation step\n",
    "engine = AutoEngine(\n",
    "    work_dir=output_dir,\n",
    "    train_data_roots=data_roots,\n",
    "    val_data_roots=data_roots,\n",
    "    config=default_config_path,\n",
    ")\n",
    "\n",
    "output_weight_path = engine.train(batch_size=2, max_epochs=2)\n",
    "print(output_weight_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-SL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otx.v2.adapters.torch.mmcv.mmcls import Dataset\n",
    "dataset = Dataset(\n",
    "    train_data_roots=\"/home/harimkan/workspace/datasets/otx_cls_dataset/cifar10@4_0/train_data\",\n",
    "    val_data_roots=\"/home/harimkan/workspace/datasets/otx_cls_dataset/cifar10@4_0/val_data\",\n",
    "    test_data_roots=\"/home/harimkan/workspace/datasets/otx_cls_dataset/cifar10@4_0/val_data\",\n",
    "    unlabeled_data_roots=\"/home/harimkan/workspace/datasets/otx_cls_dataset/cifar10@4_0/unlabel_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Semisupervised training type detected with unlabeled data: /home/harimkan/workspace/datasets/otx_cls_dataset/cifar10@4_0/unlabel_data\n",
      "2023-07-05 13:27:48,744 | INFO : possible max iterations = 40\n"
     ]
    }
   ],
   "source": [
    "config = \"/home/harimkan/workspace/repo/otx-fork/otx-workspace-test/otx-workspace-CLASSIFICATION/semisl/data_pipeline.py\"\n",
    "\n",
    "train_dataloader = dataset.train_dataloader(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TrainType.Semisupervised: 'Semisupervised'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:27:57,514 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmcv.mmcls import build_model_from_config\n",
    "semi_sl_model = dict(\n",
    "    model=dict(\n",
    "        type='SemiSLClassifier',\n",
    "        backbone=dict(\n",
    "            type='otx.OTXEfficientNet',\n",
    "            pretrained=True,\n",
    "            version='b0'),\n",
    "        neck=dict(\n",
    "            type='GlobalAveragePooling'),\n",
    "        head=dict(\n",
    "            type='SemiLinearClsHead',\n",
    "            num_classes=1000,\n",
    "            in_channels=1280,\n",
    "            loss=dict(\n",
    "                type='CrossEntropyLoss',\n",
    "                loss_weight=1.0),\n",
    "            topk=(1, 5)),\n",
    "        task='classification',\n",
    "        pretrained=None)\n",
    "    )\n",
    "\n",
    "model = build_model_from_config(\n",
    "    config=semi_sl_model,\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:28:04,797 | INFO : Start running, host: harimkan@harimkang-desktop, work_dir: /home/harimkan/workspace/repo/otx-fork/otx-workspace-test\n",
      "2023-07-05 13:28:04,798 | INFO : Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "2023-07-05 13:28:04,799 | INFO : workflow: [('train', 1)], max: 3 epochs\n",
      "2023-07-05 13:28:04,799 | INFO : Checkpoints will be saved to /home/harimkan/workspace/repo/otx-fork/otx-workspace-test by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:28:07,346 | INFO : Saving checkpoint at 1 epochs\n",
      "2023-07-05 13:28:09,891 | INFO : Saving checkpoint at 2 epochs\n",
      "2023-07-05 13:28:12,419 | INFO : Saving checkpoint at 3 epochs\n",
      "/home/harimkan/workspace/repo/otx-fork/otx-workspace-test/latest.pth\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmcv.mmcls.engine import MMCLSEngine\n",
    "mmcls_engine = MMCLSEngine()\n",
    "\n",
    "output = mmcls_engine.train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    work_dir=\"/home/harimkan/workspace/repo/otx-fork/otx-workspace-test\",\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:28:17,538 | INFO : Start running, host: harimkan@harimkang-desktop, work_dir: /home/harimkan/workspace/repo/otx-fork/otx-workspace-test\n",
      "2023-07-05 13:28:17,539 | INFO : Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(ABOVE_NORMAL) ModelEmaV2Hook                     \n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(ABOVE_NORMAL) ModelEmaV2Hook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(NORMAL      ) SemiSLClsHook                      \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) ModelEmaV2Hook                     \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) SemiSLClsHook                      \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) SemiSLClsHook                      \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(NORMAL      ) SemiSLClsHook                      \n",
      " -------------------- \n",
      "2023-07-05 13:28:17,539 | INFO : workflow: [('train', 1)], max: 3 epochs\n",
      "2023-07-05 13:28:17,539 | INFO : \t* EMA V2 Enable\n",
      "2023-07-05 13:28:17,539 | INFO : Checkpoints will be saved to /home/harimkan/workspace/repo/otx-fork/otx-workspace-test by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:28:20,064 | INFO : Saving checkpoint at 1 epochs\n",
      "2023-07-05 13:28:22,631 | INFO : Saving checkpoint at 2 epochs\n",
      "2023-07-05 13:28:25,178 | INFO : Saving checkpoint at 3 epochs\n",
      "/home/harimkan/workspace/repo/otx-fork/otx-workspace-test/latest.pth\n"
     ]
    }
   ],
   "source": [
    "custom_hooks=[\n",
    "    dict(\n",
    "        type='ModelEmaV2Hook',\n",
    "        priority='ABOVE_NORMAL'\n",
    "    ),\n",
    "    dict(type='SemiSLClsHook')\n",
    "]\n",
    "\n",
    "\n",
    "output = mmcls_engine.train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    custom_hooks=custom_hooks,\n",
    "    work_dir=\"/home/harimkan/workspace/repo/otx-fork/otx-workspace-test\",\n",
    "    max_epochs=3,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
