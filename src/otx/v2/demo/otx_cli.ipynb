{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "/home/harimkan/workspace/repo/otx-fork-3/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "usage: otx [options] train [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "                           [-o WORK_DIR] [--model CONFIG]\n",
      "                           [--model.multilabel MULTILABEL]\n",
      "                           [--model.hierarchical HIERARCHICAL]\n",
      "                           [--model.task_adapt TASK_ADAPT]\n",
      "                           [--model.track_loss_dynamics {true,false}]\n",
      "                           [--model.head HEAD] --model.backbone BACKBONE\n",
      "                           [--model.neck NECK] [--model.pretrained PRETRAINED]\n",
      "                           [--model.train_cfg TRAIN_CFG]\n",
      "                           [--model.data_preprocessor DATA_PREPROCESSOR]\n",
      "                           [--model.init_cfg INIT_CFG] [--model.type TYPE]\n",
      "                           [--data CONFIG] [--data.task TASK]\n",
      "                           [--data.train_type TRAIN_TYPE]\n",
      "                           [--data.train_data_roots TRAIN_DATA_ROOTS]\n",
      "                           [--data.train_ann_files TRAIN_ANN_FILES]\n",
      "                           [--data.val_data_roots VAL_DATA_ROOTS]\n",
      "                           [--data.val_ann_files VAL_ANN_FILES]\n",
      "                           [--data.test_data_roots TEST_DATA_ROOTS]\n",
      "                           [--data.test_ann_files TEST_ANN_FILES]\n",
      "                           [--data.unlabeled_data_roots UNLABELED_DATA_ROOTS]\n",
      "                           [--data.unlabeled_file_list UNLABELED_FILE_LIST]\n",
      "                           [--data.data_format DATA_FORMAT]\n",
      "                           [--val_dataloader CONFIG]\n",
      "                           [--val_dataloader.self SELF]\n",
      "                           [--val_dataloader.subset SUBSET]\n",
      "                           [--val_dataloader.pipeline PIPELINE]\n",
      "                           [--val_dataloader.config CONFIG]\n",
      "                           [--val_dataloader.batch_size BATCH_SIZE]\n",
      "                           [--val_dataloader.num_workers NUM_WORKERS]\n",
      "                           [--val_dataloader.shuffle {true,false}]\n",
      "                           [--val_dataloader.pin_memory {true,false}]\n",
      "                           [--val_dataloader.drop_last {true,false}]\n",
      "                           [--val_dataloader.sampler.help CLASS_PATH_OR_NAME]\n",
      "                           [--val_dataloader.sampler SAMPLER]\n",
      "                           [--val_dataloader.persistent_workers {true,false}]\n",
      "                           [--val_dataloader.distributed {true,false}]\n",
      "                           [--val_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE]\n",
      "                           [--train_dataloader CONFIG]\n",
      "                           [--train_dataloader.self SELF]\n",
      "                           [--train_dataloader.subset SUBSET]\n",
      "                           [--train_dataloader.pipeline PIPELINE]\n",
      "                           [--train_dataloader.config CONFIG]\n",
      "                           [--train_dataloader.batch_size BATCH_SIZE]\n",
      "                           [--train_dataloader.num_workers NUM_WORKERS]\n",
      "                           [--train_dataloader.shuffle {true,false}]\n",
      "                           [--train_dataloader.pin_memory {true,false}]\n",
      "                           [--train_dataloader.drop_last {true,false}]\n",
      "                           [--train_dataloader.sampler.help CLASS_PATH_OR_NAME]\n",
      "                           [--train_dataloader.sampler SAMPLER]\n",
      "                           [--train_dataloader.persistent_workers {true,false}]\n",
      "                           [--train_dataloader.distributed {true,false}]\n",
      "                           [--train_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE]\n",
      "                           [--optimizer.help CLASS_PATH_OR_NAME]\n",
      "                           [--optimizer OPTIMIZER] [--max_iters MAX_ITERS]\n",
      "                           [--max_epochs MAX_EPOCHS]\n",
      "                           [--distributed {true,false,null}] [--seed SEED]\n",
      "                           [--deterministic {true,false,null}]\n",
      "                           [--precision PRECISION]\n",
      "                           [--eval_interval EVAL_INTERVAL]\n",
      "                           [--custom_hooks.help CLASS_PATH_OR_NAME]\n",
      "                           [--custom_hooks CUSTOM_HOOKS]\n",
      "\n",
      "Training Functions with the MMEngine Framework.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a configuration file in yaml format.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: comments,\n",
      "                        skip_default, skip_null.\n",
      "  -o WORK_DIR, --work_dir WORK_DIR\n",
      "                        Path to store logs and outputs related to the command.\n",
      "                        (type: str, default: null)\n",
      "  --model.type TYPE     Enter the class name of model. (default: <class 'otx.v\n",
      "                        2.adapters.torch.mmengine.mmpretrain.modules.models.cl\n",
      "                        assifiers.sam_classifier.SAMImageClassifier'>)\n",
      "\n",
      "SAM-enabled ImageClassifier:\n",
      "  --model CONFIG        Path to a configuration file.\n",
      "  --model.multilabel MULTILABEL\n",
      "                        (type: Any, default: False)\n",
      "  --model.hierarchical HIERARCHICAL\n",
      "                        (type: Any, default: False)\n",
      "  --model.task_adapt TASK_ADAPT\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --model.track_loss_dynamics {true,false}\n",
      "                        (type: bool, default: False)\n",
      "  --model.head HEAD     The head module to do prediction and calculate loss\n",
      "                        from processed features. See\n",
      "                        :mod:`mmpretrain.models.heads`. Notice that if the\n",
      "                        head is not set, almost all methods cannot be used\n",
      "                        except :meth:`extract_feat`. Defaults to None. (type:\n",
      "                        Optional[Any], default: null)\n",
      "  --model.backbone BACKBONE\n",
      "                        The backbone module. See\n",
      "                        :mod:`mmpretrain.models.backbones`. (required, type:\n",
      "                        <class 'dict'>)\n",
      "  --model.neck NECK     The neck module to process features from backbone. See\n",
      "                        :mod:`mmpretrain.models.necks`. Defaults to None.\n",
      "                        (type: Optional[dict], default: null)\n",
      "  --model.pretrained PRETRAINED\n",
      "                        The pretrained checkpoint path, support local path and\n",
      "                        remote path. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --model.train_cfg TRAIN_CFG\n",
      "                        The training setting. The acceptable fields are: -\n",
      "                        augments (List[dict]): The batch augmentation methods\n",
      "                        to use. More details can be found in\n",
      "                        :mod:`mmpretrain.model.utils.augment`. - probs\n",
      "                        (List[float], optional): The probability of every\n",
      "                        batch augmentation methods. If None, choose evenly.\n",
      "                        Defaults to None. Defaults to None. (type:\n",
      "                        Optional[dict], default: null)\n",
      "  --model.data_preprocessor DATA_PREPROCESSOR\n",
      "                        The config for preprocessing input data. If None or no\n",
      "                        specified type, it will use \"ClsDataPreprocessor\" as\n",
      "                        type. See :class:`ClsDataPreprocessor` for more\n",
      "                        details. Defaults to None. (type: Optional[dict],\n",
      "                        default: null)\n",
      "  --model.init_cfg INIT_CFG\n",
      "                        the config to control the initialization. Defaults to\n",
      "                        None. (type: Optional[dict], default: null)\n",
      "\n",
      "MMPretrain's Dataset class:\n",
      "  --data CONFIG         Path to a configuration file.\n",
      "  --data.task TASK      The task type of the dataset want to load. Defaults to\n",
      "                        None. (type: Union[TaskType, str, null], default:\n",
      "                        null)\n",
      "  --data.train_type TRAIN_TYPE\n",
      "                        The train type of the dataset want to load. Defaults\n",
      "                        to None. (type: Union[TrainType, str, null], default:\n",
      "                        null)\n",
      "  --data.train_data_roots TRAIN_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        training. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.train_ann_files TRAIN_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for training. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.val_data_roots VAL_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        validation. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.val_ann_files VAL_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for validation. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.test_data_roots TEST_DATA_ROOTS\n",
      "                        The root address of the dataset to be used for\n",
      "                        testing. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.test_ann_files TEST_ANN_FILES\n",
      "                        Location of the annotation file for the dataset to be\n",
      "                        used for testing. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "  --data.unlabeled_data_roots UNLABELED_DATA_ROOTS\n",
      "                        The root address of the unlabeled dataset to be used\n",
      "                        for training. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.unlabeled_file_list UNLABELED_FILE_LIST\n",
      "                        The file where the list of unlabeled images is\n",
      "                        declared. Defaults to None. (type: Optional[str],\n",
      "                        default: null)\n",
      "  --data.data_format DATA_FORMAT\n",
      "                        The format of the dataset. Defaults to None. (type:\n",
      "                        Optional[str], default: null)\n",
      "\n",
      "MMPretrain's Dataset.subset_dataloader:\n",
      "  --val_dataloader CONFIG\n",
      "                        Path to a configuration file.\n",
      "  --val_dataloader.self SELF\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --val_dataloader.subset SUBSET\n",
      "                        Enter an available subset of that dataset. (required,\n",
      "                        type: str, default: val)\n",
      "  --val_dataloader.pipeline PIPELINE, --val_dataloader.pipeline+ PIPELINE\n",
      "                        Dataset Pipeline. Defaults to None. (type:\n",
      "                        Union[List[Union[Dict, Any]], Dict[str,\n",
      "                        List[Union[Dict, Any]]], null], default: null)\n",
      "  --val_dataloader.config CONFIG\n",
      "                        Path to configuration file or Config. Defaults to\n",
      "                        None. (type: Union[str, Dict[str, Any], null],\n",
      "                        default: null)\n",
      "  --val_dataloader.batch_size BATCH_SIZE\n",
      "                        How many samples per batch to load. Defaults to None.\n",
      "                        (type: Optional[int], default: null)\n",
      "  --val_dataloader.num_workers NUM_WORKERS\n",
      "                        How many subprocesses to use for data loading. ``0``\n",
      "                        means that the data will be loaded in the main\n",
      "                        process. Defaults to None. (type: Optional[int],\n",
      "                        default: null)\n",
      "  --val_dataloader.shuffle {true,false}\n",
      "                        Set to ``True`` to have the data reshuffled at every\n",
      "                        epoch. Defaults to True. (type: bool, default: True)\n",
      "  --val_dataloader.pin_memory {true,false}\n",
      "                        If ``True``, the data loader will copy Tensors into\n",
      "                        device/CUDA pinned memory before returning them. If\n",
      "                        your data elements are a custom type, or your\n",
      "                        :attr:`collate_fn` returns a batch that is a custom\n",
      "                        type, see the example below. Defaults to False. (type:\n",
      "                        bool, default: False)\n",
      "  --val_dataloader.drop_last {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.sampler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Sampler and\n",
      "                        exit.\n",
      "  --val_dataloader.sampler SAMPLER, --val_dataloader.sampler+ SAMPLER\n",
      "                        Defines the strategy to draw samples from the dataset.\n",
      "                        Can be any ``Iterable`` with ``__len__`` implemented.\n",
      "                        If specified, :attr:`shuffle` must not be specified..\n",
      "                        Defaults to None. (type: Union[Sampler, Iterable,\n",
      "                        Dict, null], default: null, known subclasses:\n",
      "                        torch.utils.data.Sampler,\n",
      "                        torch.utils.data.SequentialSampler,\n",
      "                        torch.utils.data.RandomSampler,\n",
      "                        torch.utils.data.SubsetRandomSampler,\n",
      "                        torch.utils.data.WeightedRandomSampler,\n",
      "                        torch.utils.data.BatchSampler,\n",
      "                        torch.utils.data.DistributedSampler,\n",
      "                        mmengine.dataset.DefaultSampler,\n",
      "                        mmpretrain.datasets.SequentialSampler,\n",
      "                        mmengine.dataset.InfiniteSampler,\n",
      "                        mmpretrain.datasets.RepeatAugSampler, otx.v2.adapters.\n",
      "                        torch.modules.dataloaders.samplers.BalancedSampler, ot\n",
      "                        x.v2.adapters.torch.modules.dataloaders.samplers.ClsIn\n",
      "                        crSampler, timm.data.distributed_sampler.OrderedDistri\n",
      "                        butedSampler,\n",
      "                        timm.data.distributed_sampler.RepeatAugSampler)\n",
      "  --val_dataloader.persistent_workers {true,false}\n",
      "                        If ``True``, the data loader will not shutdown the\n",
      "                        worker processes after a dataset has been consumed\n",
      "                        once. This allows to maintain the workers `Dataset`\n",
      "                        instances alive. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.distributed {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --val_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE\n",
      "                        (type: Optional[Any], default: null)\n",
      "\n",
      "MMPretrain's Dataset.subset_dataloader:\n",
      "  --train_dataloader CONFIG\n",
      "                        Path to a configuration file.\n",
      "  --train_dataloader.self SELF\n",
      "                        (type: Optional[Any], default: null)\n",
      "  --train_dataloader.subset SUBSET\n",
      "                        Enter an available subset of that dataset. (required,\n",
      "                        type: str, default: train)\n",
      "  --train_dataloader.pipeline PIPELINE, --train_dataloader.pipeline+ PIPELINE\n",
      "                        Dataset Pipeline. Defaults to None. (type:\n",
      "                        Union[List[Union[Dict, Any]], Dict[str,\n",
      "                        List[Union[Dict, Any]]], null], default: null)\n",
      "  --train_dataloader.config CONFIG\n",
      "                        Path to configuration file or Config. Defaults to\n",
      "                        None. (type: Union[str, Dict[str, Any], null],\n",
      "                        default: null)\n",
      "  --train_dataloader.batch_size BATCH_SIZE\n",
      "                        How many samples per batch to load. Defaults to None.\n",
      "                        (type: Optional[int], default: null)\n",
      "  --train_dataloader.num_workers NUM_WORKERS\n",
      "                        How many subprocesses to use for data loading. ``0``\n",
      "                        means that the data will be loaded in the main\n",
      "                        process. Defaults to None. (type: Optional[int],\n",
      "                        default: null)\n",
      "  --train_dataloader.shuffle {true,false}\n",
      "                        Set to ``True`` to have the data reshuffled at every\n",
      "                        epoch. Defaults to True. (type: bool, default: True)\n",
      "  --train_dataloader.pin_memory {true,false}\n",
      "                        If ``True``, the data loader will copy Tensors into\n",
      "                        device/CUDA pinned memory before returning them. If\n",
      "                        your data elements are a custom type, or your\n",
      "                        :attr:`collate_fn` returns a batch that is a custom\n",
      "                        type, see the example below. Defaults to False. (type:\n",
      "                        bool, default: False)\n",
      "  --train_dataloader.drop_last {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.sampler.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Sampler and\n",
      "                        exit.\n",
      "  --train_dataloader.sampler SAMPLER, --train_dataloader.sampler+ SAMPLER\n",
      "                        Defines the strategy to draw samples from the dataset.\n",
      "                        Can be any ``Iterable`` with ``__len__`` implemented.\n",
      "                        If specified, :attr:`shuffle` must not be specified..\n",
      "                        Defaults to None. (type: Union[Sampler, Iterable,\n",
      "                        Dict, null], default: null, known subclasses:\n",
      "                        torch.utils.data.Sampler,\n",
      "                        torch.utils.data.SequentialSampler,\n",
      "                        torch.utils.data.RandomSampler,\n",
      "                        torch.utils.data.SubsetRandomSampler,\n",
      "                        torch.utils.data.WeightedRandomSampler,\n",
      "                        torch.utils.data.BatchSampler,\n",
      "                        torch.utils.data.DistributedSampler,\n",
      "                        mmengine.dataset.DefaultSampler,\n",
      "                        mmpretrain.datasets.SequentialSampler,\n",
      "                        mmengine.dataset.InfiniteSampler,\n",
      "                        mmpretrain.datasets.RepeatAugSampler, otx.v2.adapters.\n",
      "                        torch.modules.dataloaders.samplers.BalancedSampler, ot\n",
      "                        x.v2.adapters.torch.modules.dataloaders.samplers.ClsIn\n",
      "                        crSampler, timm.data.distributed_sampler.OrderedDistri\n",
      "                        butedSampler,\n",
      "                        timm.data.distributed_sampler.RepeatAugSampler)\n",
      "  --train_dataloader.persistent_workers {true,false}\n",
      "                        If ``True``, the data loader will not shutdown the\n",
      "                        worker processes after a dataset has been consumed\n",
      "                        once. This allows to maintain the workers `Dataset`\n",
      "                        instances alive. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.distributed {true,false}\n",
      "                        _description_. Defaults to False. (type: bool,\n",
      "                        default: False)\n",
      "  --train_dataloader.unlabeled_batch_size UNLABELED_BATCH_SIZE\n",
      "                        (type: Optional[Any], default: null)\n",
      "\n",
      "Training Functions with the MMEngine Framework:\n",
      "  --optimizer.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Optimizer and\n",
      "                        exit.\n",
      "  --optimizer OPTIMIZER\n",
      "                        _description_. Defaults to None. (type: Union[dict,\n",
      "                        Optimizer, null], default: null, known subclasses:\n",
      "                        torch.optim.Optimizer, torch.optim.Adadelta,\n",
      "                        torch.optim.Adagrad, torch.optim.Adam,\n",
      "                        torch.optim.AdamW, torch.optim.Adamax,\n",
      "                        torch.optim.ASGD, torch.optim.NAdam,\n",
      "                        torch.optim.RAdam, torch.optim.RMSprop,\n",
      "                        torch.optim.Rprop, torch.optim.SGD,\n",
      "                        torch.optim.SparseAdam, torch.optim.LBFGS,\n",
      "                        torch.distributed.optim.PostLocalSGDOptimizer,\n",
      "                        torch.distributed.optim.ZeroRedundancyOptimizer,\n",
      "                        mmengine.optim.ZeroRedundancyOptimizer,\n",
      "                        mmpretrain.engine.Adan, mmpretrain.engine.Lamb,\n",
      "                        mmpretrain.engine.LARS, otx.v2.adapters.torch.mmengine\n",
      "                        .mmpretrain.modules.LARS)\n",
      "  --max_iters MAX_ITERS\n",
      "                        Specifies the maximum iters of training. Defaults to\n",
      "                        None. (type: Optional[int], default: null)\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Specifies the maximum epoch of training. Defaults to\n",
      "                        None. (type: Optional[int], default: null)\n",
      "  --distributed {true,false,null}\n",
      "                        Whether to use the distributed setting. Defaults to\n",
      "                        None. (type: Optional[bool], default: null)\n",
      "  --seed SEED           The seed to use for training. Defaults to None. (type:\n",
      "                        Optional[int], default: null)\n",
      "  --deterministic {true,false,null}\n",
      "                        The deterministic to use for training. Defaults to\n",
      "                        None. (type: Optional[bool], default: null)\n",
      "  --precision PRECISION\n",
      "                        The precision to use for training. Defaults to None.\n",
      "                        (type: Optional[str], default: null)\n",
      "  --eval_interval EVAL_INTERVAL\n",
      "                        Specifies the validation Interval. Defaults to None.\n",
      "                        (type: Optional[int], default: null)\n",
      "  --custom_hooks.help CLASS_PATH_OR_NAME\n",
      "                        Show the help for the given subclass of Hook and exit.\n",
      "  --custom_hooks CUSTOM_HOOKS, --custom_hooks+ CUSTOM_HOOKS\n",
      "                        Custom Hooks for mmengine.Runner. Defaults to None.\n",
      "                        (type: Union[List, Dict, Hook, null], default: null,\n",
      "                        known subclasses: mmengine.hooks.Hook,\n",
      "                        mmengine.hooks.CheckpointHook,\n",
      "                        mmengine.hooks.EarlyStoppingHook, otx.v2.adapters.torc\n",
      "                        h.mmengine.modules.LazyEarlyStoppingHook,\n",
      "                        mmengine.hooks.EMAHook, mmpretrain.engine.EMAHook, otx\n",
      "                        .v2.adapters.torch.mmengine.modules.CustomModelEMAHook\n",
      "                        , mmengine.hooks.EmptyCacheHook,\n",
      "                        mmengine.hooks.IterTimerHook,\n",
      "                        mmengine.hooks.LoggerHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.OTXLoggerHook,\n",
      "                        mmengine.hooks.NaiveVisualizationHook,\n",
      "                        mmengine.hooks.ParamSchedulerHook, otx.v2.adapters.tor\n",
      "                        ch.mmengine.modules.ReduceLROnPlateauLrUpdaterHook,\n",
      "                        mmengine.hooks.ProfilerHook,\n",
      "                        mmengine.hooks.NPUProfilerHook,\n",
      "                        mmengine.hooks.RuntimeInfoHook,\n",
      "                        mmengine.hooks.DistSamplerSeedHook,\n",
      "                        mmengine.hooks.SyncBuffersHook,\n",
      "                        mmengine.hooks.PrepareTTAHook,\n",
      "                        mmpretrain.engine.ClassNumCheckHook,\n",
      "                        mmpretrain.engine.DenseCLHook, mmcv.cnn.RFSearchHook,\n",
      "                        mmpretrain.engine.SetAdaptiveMarginsHook,\n",
      "                        mmpretrain.engine.PreciseBNHook,\n",
      "                        mmpretrain.engine.PrepareProtoBeforeValLoopHook,\n",
      "                        mmpretrain.engine.SimSiamHook,\n",
      "                        mmpretrain.engine.SwAVHook,\n",
      "                        mmpretrain.engine.SwitchRecipeHook,\n",
      "                        mmpretrain.engine.VisualizationHook,\n",
      "                        mmpretrain.engine.WarmupParamHook, otx.v2.adapters.tor\n",
      "                        ch.mmengine.modules.StopLossNanTrainingHook, otx.v2.ad\n",
      "                        apters.torch.mmengine.modules.AdaptiveTrainSchedulingH\n",
      "                        ook, otx.v2.adapters.torch.mmengine.modules.CancelTrai\n",
      "                        ningHook, otx.v2.adapters.torch.mmengine.modules.Cance\n",
      "                        lInterfaceHook, otx.v2.adapters.torch.mmengine.modules\n",
      "                        .CheckpointHookWithValResults, otx.v2.adapters.torch.m\n",
      "                        mengine.modules.EnsureCorrectBestCheckpointHook, otx.v\n",
      "                        2.adapters.torch.mmengine.modules.SaveInitialWeightHoo\n",
      "                        k, otx.v2.adapters.torch.mmengine.modules.ComposedData\n",
      "                        LoadersHook, otx.v2.adapters.torch.mmengine.modules.EM\n",
      "                        AMomentumUpdateHook, otx.v2.adapters.torch.mmengine.mo\n",
      "                        dules.DualModelEMAHook, otx.v2.adapters.torch.mmengine\n",
      "                        .modules.UnbiasedTeacherHook, otx.v2.adapters.torch.mm\n",
      "                        engine.modules.ForceTrainModeHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.IBLossHook, otx\n",
      "                        .v2.adapters.torch.mmengine.modules.LoggerReplaceHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.LossDynamicsTra\n",
      "                        ckingHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.MemCacheHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.ModelEmaV2Hook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.NoBiasDecayHook\n",
      "                        , otx.v2.adapters.torch.mmengine.modules.OTXProgressHo\n",
      "                        ok,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.SemiSLClsHook,\n",
      "                        otx.v2.adapters.torch.mmengine.modules.TaskAdaptHook, \n",
      "                        otx.v2.adapters.torch.mmengine.modules.TwoCropTransfor\n",
      "                        mHook)\n"
     ]
    }
   ],
   "source": [
    "! otx train -o ./v2_outputs/otx-test-1 --data.train_data_roots ../../../../tests/assets/classification_dataset_class_incremental --config ../configs/classification/otx_mmpretrain_cli.yaml --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;35m\n",
      "\n",
      " ██████╗     ████████╗    ██╗  ██╗\n",
      "██╔═══██╗    ╚══██╔══╝    ╚██╗██╔╝\n",
      "██║   ██║       ██║        ╚███╔╝\n",
      "██║   ██║       ██║        ██╔██╗\n",
      "╚██████╔╝       ██║       ██╔╝ ██╗\n",
      " ╚═════╝        ╚═╝       ╚═╝  ╚═╝\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "/home/harimkan/workspace/repo/otx-fork-3/venv/lib/python3.9/site-packages/openvino/pyopenvino/__init__.py:10: FutureWarning: The module is private and following namespace `pyopenvino` will be removed in the future\n",
      "  warnings.warn(message=\"The module is private and following namespace \" \"`pyopenvino` will be removed in the future\", category=FutureWarning)\n",
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-07-18 18:07:10,846 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-18 18:07:10,870 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-07-18 18:07:10,935 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-18 18:07:10,949 | INFO : Try to create a 0 size memory pool.\n",
      "2023-07-18 18:07:10,950 | WARNING : Currently, OTX does not accept val_dataloader as a dict configuration.\n",
      "2023-07-18 18:07:10,950 | WARNING : Currently, OTX does not accept test_dataloader as a dict configuration.\n",
      "2023-07-18 18:07:10,950 | WARNING : In Engine.config, remove ['framework', 'task', 'train_type', 'max_epochs', 'max_iters', 'precision'] that are unavailable to the Runner.\n",
      "07/18 18:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1234\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.7.4\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    dist_cfg: {'backend': 'nccl', 'linear_scale_lr': True}\n",
      "    seed: 1234\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "07/18 18:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "07/18 18:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "2023-07-18 18:07:12,011 | WARNING : The config used in the build isstored as an object in the configurationfile because the object doesn't have it.This can result in a non-reusable configs.py.\n",
      "07/18 18:07:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "07/18 18:07:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "07/18 18:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/harimkan/workspace/repo/otx-fork-3/src/otx/v2/demo/v2_outputs/otx-test-1.\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][1/1]  lr: 1.0000e-02  eta: 0:00:12  time: 1.3850  data_time: 0.1142  memory: 4487  loss: 6.9613\n",
      "/home/harimkan/workspace/repo/otx-fork-3/venv/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "07/18 18:07:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][1/1]  lr: 1.0000e-02  eta: 0:00:06  time: 0.7864  data_time: 0.1229  memory: 3645  loss: 6.6483\n",
      "07/18 18:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][1/1]  lr: 1.0000e-02  eta: 0:00:04  time: 0.5853  data_time: 0.1244  memory: 3641  loss: 6.1276\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 0.4850  data_time: 0.1253  memory: 3641  loss: 5.4260\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][1/1]  lr: 1.0000e-02  eta: 0:00:02  time: 0.4264  data_time: 0.1275  memory: 3641  loss: 4.7290\n",
      "07/18 18:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][1/1]  lr: 1.0000e-02  eta: 0:00:01  time: 0.3865  data_time: 0.1281  memory: 3641  loss: 4.0935\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 6 epochs\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][1/1]  lr: 1.0000e-02  eta: 0:00:01  time: 0.3584  data_time: 0.1287  memory: 3641  loss: 3.5638\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 7 epochs\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][1/1]  lr: 1.0000e-02  eta: 0:00:00  time: 0.3373  data_time: 0.1291  memory: 3641  loss: 3.1297\n",
      "07/18 18:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 8 epochs\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][1/1]  lr: 1.0000e-02  eta: 0:00:00  time: 0.3207  data_time: 0.1292  memory: 3641  loss: 2.7842\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 9 epochs\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230718_180710\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][1/1]  lr: 1.0000e-02  eta: 0:00:00  time: 0.3080  data_time: 0.1299  memory: 3641  loss: 2.5091\n",
      "07/18 18:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n"
     ]
    }
   ],
   "source": [
    "! otx train -o ./v2_outputs/otx-test-1 --data.train_data_roots ../../../../tests/assets/classification_dataset_class_incremental --config ../configs/classification/otx_mmpretrain_cli.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
