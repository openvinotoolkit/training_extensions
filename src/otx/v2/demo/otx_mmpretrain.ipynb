{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTX API DEMO (MMPretrain Example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization Training API\n",
    "\n",
    "Select a framework & import adapter modules.\n",
    "\n",
    "We'll choose MMpretrain here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Environment:\n",
    "- mmpretrain-1.0.0rc8\n",
    "- mmcv-2.0.1\n",
    "- mmengine-0.7.4\n",
    "- mmdeploy-1.2.0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset & DataLoader\n",
    "1. Prepare a dataset and enter path into Dataset\n",
    "\n",
    "    - Convert to OTX's DatasetEntity and Label Schema by leveraging Datumaro's features through paths (path -> Datumaro -> OTX DatasetEntity & LabelSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain import Dataset\n",
    "dataset = Dataset(\n",
    "    train_data_roots=\"/home/harimkan/workspace/repo/otx-fork-3/tests/assets/classification_dataset_class_incremental\",\n",
    "    val_data_roots=\"/home/harimkan/workspace/repo/otx-fork-3/tests/assets/classification_dataset_class_incremental\",\n",
    "    test_data_roots=\"/home/harimkan/workspace/repo/otx-fork-3/tests/assets/classification_dataset_class_incremental\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. Build Torch Dataset from MMCV config (filepath or dict) -> torch.utils.data.Dataset\n",
    "\n",
    "    - User can build a dataset from a config file or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detected dataset format: imagenet\n",
      "[*] Detected task type: CLASSIFICATION\n",
      "2023-07-06 14:53:00,567 | INFO : Try to create a 0 size memory pool.\n",
      "Dataset type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 32\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = dataset.train_dataloader()\n",
    "print(f\"Dataset type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Length of DataLoader: 16\n",
      "Dataset size: 32\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Customize batch_size\n",
    "train_dataloader = dataset.train_dataloader(batch_size=2)\n",
    "print(f\"DataLoader type: {type(train_dataloader)}\")\n",
    "print(f\"Length of DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Number of classes: {train_dataloader.dataset.num_classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "### Model provided by OTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-06 14:53:17,450 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "2023-07-06 14:53:17,474 | INFO : 'in_channels' config in model.head is updated from -1 to 1280\n",
      "2023-07-06 14:53:17,538 | INFO : init weight - https://github.com/osmr/imgclsmob/releases/download/v0.0.364/efficientnet_b0-0752-0e386130.pth.zip\n",
      "Model type: <class 'otx.v2.adapters.torch.mmengine.mmpretrain.modules.models.classifiers.sam_classifier.SAMImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "## OTX Custom Model\n",
    "from otx.v2.adapters.torch.mmengine.mmpretrain import build_model_from_config\n",
    "otx_model = build_model_from_config(\n",
    "    config=\"/home/harimkan/workspace/repo/otx-fork-3/src/otx/v2/configs/classification/otx_efficientnet_b0.yaml\",\n",
    "    num_classes=dataset.num_classes\n",
    ")\n",
    "print(f\"Model type: {type(otx_model)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model provided by mmpretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'mmpretrain.models.classifiers.image.ImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "# mmpretrain's pre-defined model\n",
    "from mmpretrain import get_model\n",
    "mmpretrain_model = get_model(\"resnet18_8xb32_in1k\")\n",
    "print(f\"Model type: {type(mmpretrain_model)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Users can use each framework's training provided by OTX. (Engine)\n",
    "\n",
    "- The engine requires the necessary models and DataLoaders for each framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 14:53:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 901436827\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.7.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 901436827\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "07/06 14:53:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "07/06 14:53:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "07/06 14:53:31 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "07/06 14:53:31 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "07/06 14:53:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "07/06 14:53:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][10/16]  lr: 1.0000e-02  eta: 0:00:08  time: 0.0238  data_time: 0.0007  memory: 391  loss: 6.8048\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][16/16]  lr: 1.0000e-02  eta: 0:00:05  time: 0.0244  data_time: 0.0007  memory: 391  loss: 6.4421\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "07/06 14:53:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][10/16]  lr: 1.0000e-02  eta: 0:00:03  time: 0.0225  data_time: 0.0007  memory: 391  loss: 5.9806\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][16/16]  lr: 1.0000e-02  eta: 0:00:02  time: 0.0236  data_time: 0.0007  memory: 391  loss: 5.7458\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "07/06 14:53:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][10/16]  lr: 1.0000e-02  eta: 0:00:01  time: 0.0226  data_time: 0.0007  memory: 391  loss: 4.8316\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][16/16]  lr: 1.0000e-02  eta: 0:00:01  time: 0.0234  data_time: 0.0007  memory: 391  loss: 5.2934\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "07/06 14:53:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0238  data_time: 0.0007  memory: 391  loss: 4.7307\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0274  data_time: 0.0008  memory: 391  loss: 4.7292\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0239  data_time: 0.0007  memory: 391  loss: 3.6009\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145330\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0242  data_time: 0.0007  memory: 391  loss: 3.9167\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "07/06 14:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "Output type: <class 'otx.v2.adapters.torch.mmengine.mmpretrain.modules.models.classifiers.sam_classifier.SAMImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "from otx.v2.adapters.torch.mmengine.mmpretrain.engine import MMPTEngine\n",
    "\n",
    "# OTX Model Training\n",
    "engine = MMPTEngine()\n",
    "output = engine.train(\n",
    "    model=otx_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    work_dir=\"/tmp/otx-test\",\n",
    "    max_epochs=5\n",
    ")\n",
    "\n",
    "print(f\"Output type: {type(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.13 (main, Aug 25 2022, 23:26:10) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 50978344\n",
      "    GPU 0,1: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "    GCC: gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.7.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 50978344\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "07/06 14:54:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /tmp/otx-test.\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0095  data_time: 0.0005  memory: 244  loss: 2.2699\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0096  data_time: 0.0006  memory: 244  loss: 1.2215\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "07/06 14:54:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "07/06 14:54:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0096  data_time: 0.0006  memory: 244  loss: 1.7348\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0095  data_time: 0.0006  memory: 244  loss: 1.4209\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0095  data_time: 0.0006  memory: 244  loss: 2.3609\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0097  data_time: 0.0006  memory: 244  loss: 3.4297\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "07/06 14:54:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0096  data_time: 0.0005  memory: 244  loss: 1.9009\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0097  data_time: 0.0006  memory: 244  loss: 1.8892\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][10/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0096  data_time: 0.0006  memory: 244  loss: 0.2875\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230706_145400\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][16/16]  lr: 1.0000e-02  eta: 0:00:00  time: 0.0095  data_time: 0.0006  memory: 244  loss: 1.1198\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "07/06 14:54:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "Output type: <class 'mmpretrain.models.classifiers.image.ImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "# MMPretrain Model Training\n",
    "engine = MMPTEngine()\n",
    "output = engine.train(\n",
    "    model=mmpretrain_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    work_dir=\"/tmp/otx-test\",\n",
    "    max_epochs=5\n",
    ")\n",
    "\n",
    "print(f\"Output type: {type(output)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
