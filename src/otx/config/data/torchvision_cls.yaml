defaults:
  - default

data_format: imagenet_with_subset_dirs

mem_cache_img_max_size: ${as_int_tuple:500,500}

train_subset:
  transform_lib_type: TORCHVISION
  transforms:
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      size: [224, 224]
      antialias: True
    - _target_: torchvision.transforms.v2.RandomHorizontalFlip
      p: 0.5
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: ${as_torch_dtype:torch.float32}
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
val_subset:
  transform_lib_type: TORCHVISION
  transforms:
    - _target_: torchvision.transforms.v2.Resize
      size: [256, 256]
      antialias: True
    - _target_: torchvision.transforms.v2.CenterCrop
      size: [224, 224]
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: ${as_torch_dtype:torch.float32}
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
test_subset:
  transform_lib_type: TORCHVISION
  transforms:
    - _target_: torchvision.transforms.v2.Resize
      size: [256, 256]
      antialias: True
    - _target_: torchvision.transforms.v2.CenterCrop
      size: [224, 224]
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: ${as_torch_dtype:torch.float32}
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
