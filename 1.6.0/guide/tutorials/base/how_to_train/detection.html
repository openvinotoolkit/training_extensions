

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Object Detection model &#8212; OpenVINO™ Training Extensions 1.6.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'guide/tutorials/base/how_to_train/detection';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Instance Segmentation model" href="instance_segmentation.html" />
    <link rel="prev" title="Classification model" href="classification.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/otx-logo.png" class="logo__image only-light" alt="OpenVINO™ Training Extensions 1.6.0 documentation - Home"/>
    <script>document.write(`<img src="../../../../_static/otx-logo.png" class="logo__image only-dark" alt="OpenVINO™ Training Extensions 1.6.0 documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/openvinotoolkit/training_extensions" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="../../../../_static/logos/github_icon.png" class="icon-link-image" alt="GitHub"/></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/cli_commands.html">OpenVINO™ Training Extensions CLI commands</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Base Tutorials</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">How to train, validate, export and optimize the model</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="classification.html">Classification  model</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Object Detection model</a></li>
<li class="toctree-l3"><a class="reference internal" href="instance_segmentation.html">Instance Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="semantic_segmentation.html">Semantic Segmentation model</a></li>
<li class="toctree-l3"><a class="reference internal" href="anomaly_detection.html">Anomaly Detection Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="action_classification.html">Action Classification model</a></li>
<li class="toctree-l3"><a class="reference internal" href="action_detection.html">Action Detection model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../demo.html">How to run the demonstration mode with OpenVINO™ Training Extensions CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deploy.html">How to deploy the model and use demo in exportable code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../explain.html">How to explain the model behavior</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../advanced/index.html">Advanced Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/semi_sl.html">Use Semi-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/self_sl.html">Use Self-Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/backbones.html">Backbone Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/api_tutorial.html">Utilize OpenVINO™ Training Extensions APIs in your project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/hpo_tutorial.html">Simple HPO Tutorial</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Explanation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../explanation/algorithms/index.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../explanation/algorithms/classification/index.html">Classification</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/classification/multi_class_classification.html">Multi-class Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/classification/multi_label_classification.html">Multi-label Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/classification/hierarhical_classification.html">Hierarchical Classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../explanation/algorithms/object_detection/index.html">Object Detection</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/object_detection/object_detection.html">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../explanation/algorithms/segmentation/index.html">Segmentation</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/segmentation/semantic_segmentation.html">Semantic Segmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/segmentation/instance_segmentation.html">Instance Segmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/algorithms/anomaly/index.html">Anomaly Detection</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../explanation/algorithms/action/index.html">Action Recognition</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/action/action_classification.html">Action Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/action/action_detection.html">Action Detection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../explanation/algorithms/visual_prompting/index.html">Visual Prompting</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/visual_prompting/fine_tuning.html">Visual Prompting (Fine-tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../explanation/algorithms/visual_prompting/zero_shot.html">Visual Prompting (Zero-shot learning)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../explanation/additional_features/index.html">Additional Features</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/models_optimization.html">Models Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/hpo.html">Hyperparameters Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/auto_configuration.html">Auto-configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/adaptive_training.html">Adaptive Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/xai.html">Explainable AI (XAI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/noisy_label_detection.html">Noisy Label Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/fast_data_loading.html">Fast Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/tiling.html">Improve Small Object Detection with Image Tiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../explanation/additional_features/config_input_size.html">Configurable Input Size</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../reference/index.html">API reference</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.html">otx</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.html">otx.algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.action.html">otx.algorithms.action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.anomaly.html">otx.algorithms.anomaly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.classification.html">otx.algorithms.classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.common.html">otx.algorithms.common</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.detection.html">otx.algorithms.detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.segmentation.html">otx.algorithms.segmentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.algorithms.visual_prompting.html">otx.algorithms.visual_prompting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.api.html">otx.api</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.configuration.html">otx.api.configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.entities.html">otx.api.entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.serialization.html">otx.api.serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.usecases.html">otx.api.usecases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.api.utils.html">otx.api.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.html">otx.cli</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.builder.html">otx.cli.builder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.manager.html">otx.cli.manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.registry.html">otx.cli.registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.tools.html">otx.cli.tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.cli.utils.html">otx.cli.utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.core.html">otx.core</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.data.html">otx.core.data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.file.html">otx.core.file</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.ov.html">otx.core.ov</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.core.patcher.html">otx.core.patcher</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../reference/_autosummary/otx.hpo.html">otx.hpo</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.html">otx.recipes</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.recipes.stages.html">otx.recipes.stages</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.html">otx.utils</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.logger.html">otx.utils.logger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../reference/_autosummary/otx.utils.utils.html">otx.utils.utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../release_notes/index.html">Releases</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">





<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Guide</a></li>
    
    
    <li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">How to train, validate, export and optimize the model</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Object...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="object-detection-model">
<h1>Object Detection model<a class="headerlink" href="#object-detection-model" title="Permalink to this heading">#</a></h1>
<p>This tutorial reveals end-to-end solution from installation to model export and optimization for object detection task on a specific example.</p>
<p>To learn more about Object Detection task, refer to <a class="reference internal" href="../../../explanation/algorithms/object_detection/object_detection.html"><span class="doc">Object Detection</span></a>.</p>
<p>On this page, we show how to train, validate, export and optimize ATSS model on WGISD public dataset.</p>
<p>To have a specific example in this tutorial, all commands will be run on the ATSS model. It’s a medium model, that achieves relatively high accuracy while keeping the inference fast.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To learn how to deploy the trained model and run the exported demo, refer to <a class="reference internal" href="../deploy.html"><span class="doc">How to deploy the model and use demo in exportable code</span></a>.</p>
<p>To learn how to run the demo in CLI and visualize results, refer to <a class="reference internal" href="../demo.html"><span class="doc">How to run the demonstration mode with OpenVINO™ Training Extensions CLI</span></a>.</p>
</div>
<p>The process has been tested on the following configuration.</p>
<ul class="simple">
<li><p>Ubuntu 20.04</p></li>
<li><p>NVIDIA GeForce RTX 3090</p></li>
<li><p>Intel(R) Core(TM) i9-10980XE CPU</p></li>
<li><p>CUDA Toolkit 11.1</p></li>
</ul>
<section id="setup-virtual-environment">
<h2>Setup virtual environment<a class="headerlink" href="#setup-virtual-environment" title="Permalink to this heading">#</a></h2>
<p>1. You can follow the installation process from a <a class="reference internal" href="../../../get_started/installation.html"><span class="doc">quick start guide</span></a>
to create a universal virtual environment for OpenVINO™ Training Extensions.</p>
<p>2. Activate your virtual
environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">.</span><span class="n">otx</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
<span class="c1"># or by this line, if you created an environment, using tox</span>
<span class="o">.</span> <span class="n">venv</span><span class="o">/</span><span class="n">otx</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
</pre></div>
</div>
</section>
<section id="dataset-preparation">
<span id="wgisd-dataset-descpiption"></span><h2>Dataset preparation<a class="headerlink" href="#dataset-preparation" title="Permalink to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, we support the following object detection dataset formats:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cocodataset.org/#format-data">COCO</a></p></li>
<li><p><a class="reference external" href="https://openvinotoolkit.github.io/datumaro/stable/docs/data-formats/formats/pascal_voc.html">Pascal-VOC</a></p></li>
<li><p><a class="reference external" href="https://openvinotoolkit.github.io/datumaro/stable/docs/data-formats/formats/yolo.html">YOLO</a></p></li>
</ul>
</div>
<p>1. Clone a repository with
<a class="reference external" href="https://github.com/thsant/wgisd">WGISD dataset</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="n">data</span> <span class="p">;</span> <span class="n">cd</span> <span class="n">data</span>
<span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">thsant</span><span class="o">/</span><span class="n">wgisd</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">wgisd</span>
<span class="n">git</span> <span class="n">checkout</span> <span class="mi">6910</span><span class="n">edc5ae3aae8c20062941b1641821f0c30127</span>
</pre></div>
</div>
<p>This dataset contains images of grapevines with the annotation for different varieties of grapes.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CDY</span></code> - Chardonnay</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CFR</span></code> - Cabernet Franc</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CSV</span></code> - Cabernet Sauvignon</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SVB</span></code> - Sauvignon Blanc</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SYH</span></code> - Syrah</p></li>
</ul>
<p>It’s a great example to start with. The model achieves high accuracy right from the beginning of the training due to relatively large and focused objects. Also, these objects are distinguished by a person, so we can check inference results just by looking at images.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<a class="reference internal image-reference" href="../../../../_images/wgisd_gt_sample.jpg"><img alt="this image uploaded from this `source &lt;https://github.com/thsant/wgisd/blob/master/data/CDY_2015.jpg&gt;`_" src="../../../../_images/wgisd_gt_sample.jpg" style="width: 600px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>2. To run the training using <a class="reference internal" href="../../../explanation/additional_features/auto_configuration.html"><span class="doc">auto-configuration feature</span></a>,
we need to reformat the dataset according to this structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>wgisd
├── annotations/
    ├── instances_train.json
    ├── instances_val.json
    (Optional)
    └── instances_test.json
├──images/
    (The split on folders is optional)
    ├── train
    ├── val
    └── test
(There may be more extra unrelated folders)
</pre></div>
</div>
<p>We can do that by running these commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># format images folder</span>
<span class="n">mv</span> <span class="n">data</span> <span class="n">images</span>

<span class="c1"># format annotations folder</span>
<span class="n">mv</span> <span class="n">coco_annotations</span> <span class="n">annotations</span>

<span class="c1"># rename annotations to meet *_train.json pattern</span>
<span class="n">mv</span> <span class="n">annotations</span><span class="o">/</span><span class="n">train_bbox_instances</span><span class="o">.</span><span class="n">json</span> <span class="n">annotations</span><span class="o">/</span><span class="n">instances_train</span><span class="o">.</span><span class="n">json</span>
<span class="n">mv</span> <span class="n">annotations</span><span class="o">/</span><span class="n">test_bbox_instances</span><span class="o">.</span><span class="n">json</span> <span class="n">annotations</span><span class="o">/</span><span class="n">instances_val</span><span class="o">.</span><span class="n">json</span>

<span class="n">cd</span> <span class="o">../..</span>
</pre></div>
</div>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h2>
<p>1. First of all, you need to choose which object detection model you want to train.
The list of supported templates for object detection is available with the command line below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The characteristics and detailed comparison of the models could be found in <a class="reference internal" href="../../../explanation/algorithms/object_detection/object_detection.html"><span class="doc">Explanation section</span></a>.</p>
<p>To modify the architecture of supported models with various backbones, please refer to the <a class="reference internal" href="../../advanced/backbones.html"><span class="doc">advanced tutorial for backbone replacement</span></a>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx find --template --task DETECTION
+-----------+-----------------------------------+------------------+------------------------------------------------------------------------------------+
|    TASK   |                 ID                |       NAME       |                                     BASE PATH                                      |
+-----------+-----------------------------------+------------------+------------------------------------------------------------------------------------+
| DETECTION | Custom_Object_Detection_Gen3_ATSS | MobileNetV2-ATSS |   src/otx/algorithms/detection/configs/detection/mobilenetv2_atss/template.yaml    |
| DETECTION |  Object_Detection_ResNeXt101_ATSS | ResNeXt101-ATSS  |    src/otx/algorithms/detection/configs/detection/resnext101_atss/template.yaml    |
| DETECTION |  Custom_Object_Detection_Gen3_SSD |       SSD        |    src/otx/algorithms/detection/configs/detection/mobilenetv2_ssd/template.yaml    |
| DETECTION |      Object_Detection_YOLOX_L     |     YOLOX-L      |  src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_l/template.yaml   |
| DETECTION |      Object_Detection_YOLOX_S     |     YOLOX-S      |  src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_s/template.yaml   |
| DETECTION |   Custom_Object_Detection_YOLOX   |    YOLOX-TINY    | src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_tiny/template.yaml |
| DETECTION |      Object_Detection_YOLOX_X     |     YOLOX-X      |  src/otx/algorithms/detection/configs/detection/cspdarknet_yolox_x/template.yaml   |
+-----------+-----------------------------------+------------------+------------------------------------------------------------------------------------+
</pre></div>
</div>
<p id="detection-workspace">2. On this step we will create <strong>otx-workspace-Detection</strong>
with:</p>
<ul class="simple">
<li><p>all necessary configs for Custom_Object_Detection_Gen3_ATSS</p></li>
<li><p>prepared <code class="docutils literal notranslate"><span class="pre">data.yaml</span></code> to simplify CLI commands launch</p></li>
<li><p>train/validation sets, based on provided annotation.</p></li>
</ul>
<p>It may be counterintuitive, but for <code class="docutils literal notranslate"><span class="pre">--train-data-roots</span></code> we need to pass the path to the dataset folder root (in our case it’s <code class="docutils literal notranslate"><span class="pre">data/wgisd</span></code>) instead of the folder with validation images.
This is because the function automatically detects annotations and images according to the expected folder structure we achieved above.
So, if you’d like to add <code class="docutils literal notranslate"><span class="pre">--val-data-roots</span></code>, please note, that it should also be a path to a dataset folder root.</p>
<p>On contrary, if we omit adding <code class="docutils literal notranslate"><span class="pre">--val-data-roots</span></code>, the function will find images for validation according to validation annotation and create <code class="docutils literal notranslate"><span class="pre">splitted_dataset</span></code> folder inside the workplace with the desired split.</p>
<p>Let’s prepare the object detection workspace running the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># we can specify the template by its ID
(otx) ...$ otx build Custom_Object_Detection_Gen3_ATSS --train-data-roots data/wgisd

# or its name
(otx) ...$ otx build MobileNetV2-ATSS --train-data-roots data/wgisd

# or its path
(otx) ...$ otx build src/otx/algorithms/detection/configs/detection/mobilenetv2_atss/template.yaml --train-data-roots data/wgisd

...
[*] Workspace Path: otx-workspace-DETECTION
[*] Load Model Template ID: Custom_Object_Detection_Gen3_ATSS
[*] Load Model Name: MobileNetV2-ATSS
[*]     - Updated: otx-workspace-DETECTION/model.py
[*]     - Updated: otx-workspace-DETECTION/data_pipeline.py
[*]     - Updated: otx-workspace-DETECTION/tile_pipeline.py
[*]     - Updated: otx-workspace-DETECTION/deployment.py
[*]     - Updated: otx-workspace-DETECTION/hpo_config.yaml
[*]     - Updated: otx-workspace-DETECTION/compression_config.json
[*] Found validation data in your dataset. It&#39;ll be used as validation data.
[*] Update data configuration file to: otx-workspace-DETECTION/data.yaml
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you want to rebuild your current workspace by running <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">build</span></code> with other parameters, it’s better to delete the original workplace before that to prevent mistakes.</p>
</div>
<p>Check <code class="docutils literal notranslate"><span class="pre">otx-workspace-DETECTION/data.yaml</span></code> to ensure, which data subsets will be used for training and validation, and update it if necessary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">:</span>
<span class="n">train</span><span class="p">:</span>
  <span class="n">ann</span><span class="o">-</span><span class="n">files</span><span class="p">:</span> <span class="n">null</span>
  <span class="n">data</span><span class="o">-</span><span class="n">roots</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">training_extensions_path</span><span class="o">&gt;/</span><span class="n">otx</span><span class="o">-</span><span class="n">workspace</span><span class="o">-</span><span class="n">DETECTION</span><span class="o">/</span><span class="n">splitted_dataset</span><span class="o">/</span><span class="n">train</span>
<span class="n">val</span><span class="p">:</span>
  <span class="n">ann</span><span class="o">-</span><span class="n">files</span><span class="p">:</span> <span class="n">null</span>
  <span class="n">data</span><span class="o">-</span><span class="n">roots</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">training_extensions_path</span><span class="o">&gt;/</span><span class="n">otx</span><span class="o">-</span><span class="n">workspace</span><span class="o">-</span><span class="n">DETECTION</span><span class="o">/</span><span class="n">splitted_dataset</span><span class="o">/</span><span class="n">val</span>
<span class="n">test</span><span class="p">:</span>
  <span class="n">ann</span><span class="o">-</span><span class="n">files</span><span class="p">:</span> <span class="n">null</span>
  <span class="n">data</span><span class="o">-</span><span class="n">roots</span><span class="p">:</span> <span class="n">null</span>
<span class="n">unlabeled</span><span class="p">:</span>
  <span class="n">file</span><span class="o">-</span><span class="nb">list</span><span class="p">:</span> <span class="n">null</span>
  <span class="n">data</span><span class="o">-</span><span class="n">roots</span><span class="p">:</span> <span class="n">null</span>
</pre></div>
</div>
<p>We also can modify the backbone of the model, by adding <code class="docutils literal notranslate"><span class="pre">--backbone</span></code> parameter.
We can find the available backbone by running <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">find</span></code> with the framework parameter.
Learn more about modified backbones in <a class="reference internal" href="../../advanced/backbones.html"><span class="doc">advanced tutorial for backbone replacement</span></a>.</p>
<p>3. <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">train</span></code> trains a model (a particular model template)
on a dataset and results in two files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weights.pth</span></code> - a model snapshot</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code> - a label schema used in training, created from a dataset</p></li>
</ul>
<p>These are needed as inputs for the further commands: <code class="docutils literal notranslate"><span class="pre">export</span></code>, <code class="docutils literal notranslate"><span class="pre">eval</span></code>,  <code class="docutils literal notranslate"><span class="pre">optimize</span></code>,  <code class="docutils literal notranslate"><span class="pre">deploy</span></code> and <code class="docutils literal notranslate"><span class="pre">demo</span></code>.</p>
<p>4. The following command line starts training of the medium object
detection model on the first GPU on WGISD dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ cd otx-workspace-DETECTION/
(otx) ...$ otx train  --output ../outputs --workspace ../outputs/logs --gpus 1
</pre></div>
</div>
<p>To start multi-gpu training, list the indexes of GPUs you want to train on or omit <cite>gpus</cite> parameter, so training will run on all available GPUs.</p>
<p>4. <code class="docutils literal notranslate"><span class="pre">(Optional)</span></code> Additionally, we can tune training parameters such as batch size, learning rate, patience epochs or warm-up iterations.
Learn more about template-specific parameters using <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">train</span> <span class="pre">params</span> <span class="pre">--help</span></code>.</p>
<p>It can be done by manually updating parameters in the <code class="docutils literal notranslate"><span class="pre">template.yaml</span></code> file in your workplace or via the command line.</p>
<p>For example, to decrease the batch size to 4, fix the number of epochs to 100 and disable early stopping, extend the command line above with the following line.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">--</span><span class="n">learning_parameters</span><span class="o">.</span><span class="n">batch_size</span> <span class="mi">4</span>
       <span class="o">--</span><span class="n">learning_parameters</span><span class="o">.</span><span class="n">num_iters</span> <span class="mi">100</span> \
       <span class="o">--</span><span class="n">learning_parameters</span><span class="o">.</span><span class="n">enable_early_stopping</span> <span class="n">false</span>
</pre></div>
</div>
<p>5. The training results are <code class="docutils literal notranslate"><span class="pre">weights.pth</span></code> and <code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code> files that located in <code class="docutils literal notranslate"><span class="pre">outputs</span></code> folder,
while training logs can be found in the <code class="docutils literal notranslate"><span class="pre">outputs/logs</span></code> dir.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We also can visualize the training using <code class="docutils literal notranslate"><span class="pre">Tensorboard</span></code> as these logs are located in <code class="docutils literal notranslate"><span class="pre">outputs/logs/tf_logs</span></code>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">21</span><span class="p">,</span><span class="mi">520</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Update</span> <span class="n">Lr</span> <span class="n">patience</span><span class="p">:</span> <span class="mi">3</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">21</span><span class="p">,</span><span class="mi">520</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Update</span> <span class="n">Validation</span> <span class="n">Interval</span><span class="p">:</span> <span class="mi">2</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">21</span><span class="p">,</span><span class="mi">520</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Update</span> <span class="n">Early</span><span class="o">-</span><span class="n">Stop</span> <span class="n">patience</span><span class="p">:</span> <span class="mi">5</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">40</span><span class="p">:</span><span class="mi">23</span><span class="p">,</span><span class="mi">140</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Epoch</span> <span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="o">/</span><span class="mi">31</span><span class="p">]</span>        <span class="n">lr</span><span class="p">:</span> <span class="mf">1.333e-03</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span> <span class="mi">11</span> <span class="n">days</span><span class="p">,</span> <span class="mi">14</span><span class="p">:</span><span class="mi">44</span><span class="p">:</span><span class="mi">47</span><span class="p">,</span> <span class="n">time</span><span class="p">:</span> <span class="mf">1.619</span><span class="p">,</span> <span class="n">data_time</span><span class="p">:</span> <span class="mf">0.961</span><span class="p">,</span> <span class="n">memory</span><span class="p">:</span> <span class="mi">4673</span><span class="p">,</span> <span class="n">current_iters</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss_cls</span><span class="p">:</span> <span class="mf">1.1261</span><span class="p">,</span> <span class="n">loss_bbox</span><span class="p">:</span> <span class="mf">0.6514</span><span class="p">,</span> <span class="n">loss_centerness</span><span class="p">:</span> <span class="mf">0.6337</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">2.4112</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">:</span> <span class="mf">18.5789</span>

<span class="o">...</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">52</span><span class="p">:</span><span class="mi">33</span><span class="p">,</span><span class="mi">985</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">run</span> <span class="n">task</span> <span class="n">done</span><span class="o">.</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">52</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">682</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Inference</span> <span class="n">completed</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">52</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">683</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">called</span> <span class="n">evaluate</span><span class="p">()</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">52</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">907</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">F</span><span class="o">-</span><span class="n">measure</span> <span class="n">after</span> <span class="n">evaluation</span><span class="p">:</span> <span class="mf">0.5487693710118504</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">10</span> <span class="mi">05</span><span class="p">:</span><span class="mi">52</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span><span class="mi">907</span> <span class="o">|</span> <span class="n">INFO</span> <span class="p">:</span> <span class="n">Evaluation</span> <span class="n">completed</span>
<span class="n">Performance</span><span class="p">(</span><span class="n">score</span><span class="p">:</span> <span class="mf">0.5487693710118504</span><span class="p">,</span> <span class="n">dashboard</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span> <span class="n">metric</span> <span class="n">groups</span><span class="p">))</span>
</pre></div>
</div>
<p>The training time highly relies on the hardware characteristics, for example on 1 NVIDIA GeForce RTX 3090 the training took about 15 minutes.</p>
<p>After that, we have the PyTorch object detection model trained with OpenVINO™ Training Extensions, which we can use for evaliation, export, optimization and deployment.</p>
</section>
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Permalink to this heading">#</a></h2>
<p>1. <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> runs evaluation of a
trained model on a particular dataset.</p>
<p>Eval function receives test annotation information and model snapshot, trained in previous step.
Please note, <code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code> file contains meta information about the dataset and it should be located in the same folder as the model snapshot.</p>
<p>The default metric is F1 measure.</p>
<p>2. That’s how we can evaluate the snapshot in <code class="docutils literal notranslate"><span class="pre">outputs</span></code>
folder on WGISD dataset and save results to <code class="docutils literal notranslate"><span class="pre">outputs/performance</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx eval --test-data-roots splitted_dataset/val \
                    --load-weights ../outputs/weights.pth \
                    --output ../outputs/
</pre></div>
</div>
<p>3. The output of <code class="docutils literal notranslate"><span class="pre">../outputs/performance.json</span></code> consists of
a dict with target metric name and its value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;f-measure&quot;</span><span class="p">:</span> <span class="mf">0.5487693710118504</span><span class="p">}</span>
</pre></div>
</div>
<p>4. <code class="docutils literal notranslate"><span class="pre">Optional</span></code> Additionally, we can tune evaluation parameters such as confidence threshold via the command line.
Learn more about template-specific parameters using <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span> <span class="pre">params</span> <span class="pre">--help</span></code>.</p>
<p>For example, if there are too many False-Positive predictions (there we have a prediction, but don’t have annotated object for it), we can suppress its number by increasing the confidence threshold as it is shown below.</p>
<p>Please note, by default, the optimal confidence threshold is detected based on validation results to maximize the final F1 metric. To set a custom confidence threshold, please disable <code class="docutils literal notranslate"><span class="pre">result_based_confidence_threshold</span></code> option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx eval --test-data-roots splitted_dataset/val \
                    --load-weights ../outputs/weights.pth \
                    --output ../outputs
                    params \
                    --postprocessing.confidence_threshold 0.5 \
                    --postprocessing.result_based_confidence_threshold false

...

2023-01-10 06:21:04,254 | INFO : F-measure after evaluation: 0.514346439957492
</pre></div>
</div>
</section>
<section id="export">
<h2>Export<a class="headerlink" href="#export" title="Permalink to this heading">#</a></h2>
<p>1. <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">export</span></code> exports a trained Pytorch <cite>.pth</cite> model to the OpenVINO™ Intermediate Representation (IR) format.
It allows to efficiently run it on Intel hardware, especially on CPU, using OpenVINO™ runtime.
Also, the resulting IR model is required to run PTQ optimization in the section below. IR model contains 2 files: <code class="docutils literal notranslate"><span class="pre">openvino.xml</span></code> for weights and <code class="docutils literal notranslate"><span class="pre">openvino.bin</span></code> for architecture.</p>
<p>2. That’s how we can export the trained model <code class="docutils literal notranslate"><span class="pre">../outputs/weights.pth</span></code>
from the previous section and save the exported model to the <code class="docutils literal notranslate"><span class="pre">../outputs/openvino/</span></code> folder.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx export --load-weights ../outputs/weights.pth \
                      --output ../outputs/openvino/

...

2023-01-10 06:23:41,621 | INFO : run task done.
2023-01-10 06:23:41,630 | INFO : Exporting completed
</pre></div>
</div>
<p>3. We can check the accuracy of the IR model and the consistency between the exported model and the PyTorch model,
using <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> and passing the IR model path to the <code class="docutils literal notranslate"><span class="pre">--load-weights</span></code> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx eval --test-data-roots splitted_dataset/val \
                    --load-weights ../outputs/openvino/openvino.xml \
                    --output ../outputs

...
2023-01-10 06:24:50,382 | INFO : Start OpenVINO inference
2023-01-10 06:24:54,943 | INFO : OpenVINO inference completed
2023-01-10 06:24:54,944 | INFO : Start OpenVINO metric evaluation
2023-01-10 06:24:55,117 | INFO : OpenVINO metric evaluation completed
Performance(score: 0.5487693710118504, dashboard: (1 metric groups))
</pre></div>
</div>
<p>4. <code class="docutils literal notranslate"><span class="pre">Optional</span></code> Additionally, we can tune confidence threshold via the command line.
Learn more about template-specific parameters using <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">export</span> <span class="pre">params</span> <span class="pre">--help</span></code>.</p>
<p>For example, if there are too many False-Positive predictions (there we have a prediction, but don’t have annotated object for it), we can suppress its number by increasing the confidence threshold as it is shown below.</p>
<p>Please note, by default, the optimal confidence threshold is detected based on validation results to maximize the final F1 metric. To set a custom confidence threshold, please disable <code class="docutils literal notranslate"><span class="pre">result_based_confidence_threshold</span></code> option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx export --load-weights ../outputs/weights.pth \
                    --output ../outputs \
                    params \
                    --postprocessing.confidence_threshold 0.5 \
                    --postprocessing.result_based_confidence_threshold false
</pre></div>
</div>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h2>
<p>1. We can further optimize the model with <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">optimize</span></code>.
It uses NNCF or PTQ depending on the model and transforms it to <code class="docutils literal notranslate"><span class="pre">INT8</span></code> format.</p>
<p><code class="docutils literal notranslate"><span class="pre">NNCF</span></code> optimization is used for trained snapshots in a framework-specific format such as checkpoint (.pth) file from Pytorch. It starts accuracy-aware quantization based on the obtained weights from the training stage. Generally, we will see the same output as during training.</p>
<p><code class="docutils literal notranslate"><span class="pre">PTQ</span></code> optimization is used for models exported in the OpenVINO™ IR format. It decreases the floating-point precision to integer precision of the exported model by performing the post-training optimization.</p>
<p>The function results with the following files, which could be used to run <a class="reference internal" href="../demo.html"><span class="doc">otx demo</span></a> as well with PyTorch (<cite>.pth</cite>) and IR model (<cite>.xml</cite>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">confidence_threshold</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config.json</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">label_schema.json</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">openvino.bin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">openvino.xml</span></code></p></li>
</ul>
<p>To learn more about optimization, refer to <a class="reference external" href="https://github.com/openvinotoolkit/nncf">NNCF repository</a>.</p>
<p>2. Command example for optimizing a PyTorch model (<cite>.pth</cite>)
with OpenVINO NNCF.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx optimize  --load-weights ../outputs/weights.pth \
                         --output ../outputs/nncf \
                         --output ../outputs/nncf

...

2023-01-17 06:46:08,208 | INFO : run task done.
2023-01-17 06:46:08,618 | INFO : Inference completed
2023-01-17 06:46:08,618 | INFO : called evaluate()
2023-01-17 06:46:08,829 | INFO : F-measure after evaluation: 0.5446735395189003
2023-01-17 06:46:08,829 | INFO : Evaluation completed
Performance(score: 0.5446735395189003, dashboard: (1 metric groups))
</pre></div>
</div>
<p>3.  Command example for optimizing OpenVINO™ model (.xml)
with OpenVINO™ PTQ.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(otx) ...$ otx optimize  --load-weights ../outputs/openvino/openvino.xml \
                         --output ../outputs/ptq \
                         --output ../outputs/ptq

...

2023-01-10 06:29:46,751 | INFO : Loading OpenVINO OTXDetectionTask
2023-01-10 06:29:47,685 | INFO : OpenVINO task initialization completed
2023-01-10 06:29:47,685 | INFO : Start PTQ optimization
2023-01-10 06:34:29,304 | INFO : PTQ optimization completed
2023-01-10 06:34:29,419 | INFO : Start OpenVINO inference
2023-01-10 06:34:33,275 | INFO : OpenVINO inference completed
2023-01-10 06:34:33,275 | INFO : Start OpenVINO metric evaluation
2023-01-10 06:34:33,451 | INFO : OpenVINO metric evaluation completed
Performance(score: 0.5389435989256938, dashboard: (1 metric groups))
</pre></div>
</div>
<p>The optimization time highly relies on the hardware characteristics, for example on 1 NVIDIA GeForce RTX 3090 it took about 10 minutes.
Please note, that PTQ will take some time without logging to optimize the model.</p>
<p>4. Finally, we can also evaluate the optimized model by passing
it to the <code class="docutils literal notranslate"><span class="pre">otx</span> <span class="pre">eval</span></code> function.</p>
<p>Now we have fully trained, optimized and exported an efficient model representation ready-to-use object detection model.</p>
<p>The following tutorials provide further steps how to <a class="reference internal" href="../deploy.html"><span class="doc">deploy</span></a> and use your model in the <a class="reference internal" href="../demo.html"><span class="doc">demonstration mode</span></a> and visualize results.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Classification  model</p>
      </div>
    </a>
    <a class="right-next"
       href="instance_segmentation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Instance Segmentation model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-virtual-environment">Setup virtual environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation">Dataset preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation">Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#export">Export</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../../../../_sources/guide/tutorials/base/how_to_train/detection.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2023, OpenVINO™ Training Extensions Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.2.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>