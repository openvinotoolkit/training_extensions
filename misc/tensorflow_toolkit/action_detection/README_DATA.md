# Data Preparation

Assume the following structure of the data:
```
    |-- data_dir
         |-- images
            |-- video_1
                frame_000000.png
                frame_000001.png
            |-- video_2
                frame_000000.png
                frame_000001.png
            |-- video_3
                frame_000000.png
                frame_000001.png
         |-- annotation
            annotation_file_1.xml
            annotation_file_2.xml
            annotation_file_3.xml
         train_tasks.txt
         test_tasks.txt
```

Each [annotation file](#annotation-file-format) describes a single source of [images](#image-file-format).

## Annotation File Format

For annotating, it is better to use the [CVAT](https://github.com/opencv/cvat) utility. We assume
that the annotation file is stored in the appropriate `.xml`
[format](https://github.com/opencv/cvat/blob/develop/cvat/apps/documentation/xml_format.md). In the
annotation file, we have a single independent track for each person on video that includes a bounding
box description on each frame. General structure of an annotation file:
```
    |-- root
         |-- track_0
              bounding_box_0
              bounding_box_1
         |-- track_1
              bounding_box_0
              bounding_box_1
```

Toy example of an annotation file:
```xml
<?xml version="1.0" encoding="utf-8"?>
<annotations count="1">
    <track id="0" label="person">
        <box frame="0" xtl="1.0" ytl="1.0" xbr="0.0" ybr="0.0" occluded="0">
            <attribute name="action">action_name</attribute>
        </box>
    </track>
</annotations>
```
**Fields Description**:
 - `count` - number of tracks
 - `id` - unique ID of track in file
 - `label` - label of track (data loader skips all other labels except `person`)
 - `frame` - unique ID of frame in track
 - `xtl`, `ytl`, `xbr`, `ybr` - bounding box coordinates of top-left and bottom-right corners
 - `occluded` - marker to highlight heavy occluded bounding boxes (can be skipped during training)
 - `name` - name of bounding box attribute (data loader is sensitive to the `action` class only)
 - `action_name` - valid name of action. You can define your own list of actions.

## Image File Format

Our implementation of data loader works with independent images stored on the drive. Each image
should be named in the `frame_xxxxxx.png` or `frame_xxxxxx.jpg` format (where `xxxxxx` is unique
image number).

> **NOTE**: To extract images from the video, use `tools/data/dump_frames.py`.

## Tasks File Format

For more robust control of image sources, we have created a separate file, where each row represents
a single source in the following format: `annotation_file_path.xml image_height,image_width images_directory_path`.
We assume that all images from the same source are resized to the `image_height,image_width` sizes
(it needs to properly decode annotations).

Example of a `train_tasks.txt` file:
```
annotations/annotation_file_1.xml 1920,1080 images/video1
annotations/annotation_file_2.xml 1920,1080 images/video2
```

Example of a `test_tasks.txt` file:
```
annotations/annotation_file_3.xml 1920,1080 images/video3
```

## Train/Evaluation Data File Generation

To generate the final data file (train or test), run the command:
```shell
python2 tools/data/prepare_pedestrian_db.py -t <PATH_TO_TASKS> \      # path to file with tasks
                                            -o <PATH_TO_OUTPUT_DIR> \ # output directory
```

The output directory structure (see an example of script output in the `./dataset` folder):
```
    |-- root
         |-- annotation
              |-- video_1
                sample_000000.json
                sample_000000.json
              |-- video_2
                sample_000000.json
                sample_000000.json
         data.txt
         class_map.yml
```

Generated files:
 - `data.txt` - input for the train/eval scripts
 - `class_map.txt` - mapping from class names onto class IDs

>**NOTE 1**: To specify class IDs directly, set the `-i` key: `-i <PATH_TO_CLASS_MAP>` (see example
> `tools/data/pedestriandb_class_map.yml`). If you specify your own class mapping, the `class_map.txt`
> file is not generated.

>**NOTE 2**: To generate a valid class mapping for a testing purpose, set `-i <PATH_TO_CLASS_MAP>`,
> where `<PATH_TO_CLASS_MAP>` is generated by the `class_map.txt`script file or your own class
> mapping file. Otherwise, the order of class IDs is different.

>**NOTE 3**: Use a prepared toy dataset (`./dataset` folder) to start your model training. You only
> need to specify the full path to images (`./dataset/images` folder) in the `data.txt` file.

## Config Specification

For the generated dataset, set the correct field values in the appropriate configurations file:
 - `IMAGE_SIZE` - target image size in the following format: `[height, width, num_channels]`
 - `TRAIN_DATA_SIZE` - number of training samples
 - `VAL_DATA_SIZE` - number of testing samples
 - `MAX_NUM_DETECTIONS_PER_IMAGE` - maximum number of objects on a single image. If it is higher, a subset of objects is used.
